{"searchDocs":[{"title":"Happy New Year 2025","type":0,"sectionRef":"#","url":"/blog/happy-new-year-2025","content":"Hi everyone, I hope you and your family are all doing well! üéÜüéÜüéÜ A new year has arrived, many goals from the previous year remain unfinished. But don't worry, we can carry them into this year and accomplish them now. Sometimes, I can not complete everything on my to-do list. However, I have made progress on each task, and that is still growth. Just chill and get better every day. Recently, I have been stuck with some study projects in my graduate school. It is a big change ü•≤ when I was working, I could just type a few keywords into Google and find some documents to solve my problems. But in an academic environment, the information is not just in technical documents; it is also in published papers and their citations. I have spent so much time just trying to understand half of a paper! LOL üòÇ To be honest, after work and studying, I feel a bit lazy. So I have not been writing new posts lately, even though I have a lot of ideas for them. I think I should add a new task to my to-do list: writing new blog posts.","keywords":"","version":null},{"title":"My Perspectives on AI Agents: Beyond the 'Vibe-Coding' Hype","type":0,"sectionRef":"#","url":"/blog/my-perspectives-on-ai-agents-beyond-the-vibe-coding-hype","content":"","keywords":"","version":null},{"title":"My perspective‚Äã","type":1,"pageTitle":"My Perspectives on AI Agents: Beyond the 'Vibe-Coding' Hype","url":"/blog/my-perspectives-on-ai-agents-beyond-the-vibe-coding-hype#my-perspective","content":" AI Agents bring a lot of benefits to my work. But please keep in mind that AI agents are not perfect. They can make mistakes, and they require human oversight to ensure accuracy and reliability. However, when used correctly, they can be powerful allies in our work.  One thing I really like about AI assistants is that they can efficiently complete my tasks, as long as I know how to complete them üòÖ.  A tutor at my school said: &quot;AI only accounts for 30% of the work; the rest depends on human creativity and critical thinking&quot;. That means your question/prompt is taking up to 70% of the success of your task.  ","version":null,"tagName":"h2"},{"title":"The \"Wow\" Factor vs. Technical Reality‚Äã","type":1,"pageTitle":"My Perspectives on AI Agents: Beyond the 'Vibe-Coding' Hype","url":"/blog/my-perspectives-on-ai-agents-beyond-the-vibe-coding-hype#the-wow-factor-vs-technical-reality","content":" Sometimes, AI can subtly point out where your weaknesses lie. You might look at a generated line of Python code and think, &quot;Wow, that's clean/well-architected!&quot;, but a Python developer might look at the same code and point out that it's unoptimized, introduces too many changes to the codebase, or simply doesn't follow best practices.    For non-tech individuals, &quot;vibe-coding&quot; an app can be empowering. They can build something that solves a small problem or addresses a specific aspect they care about, leading to a satisfying &quot;Wow, it works!&quot; moment.  However, for technical people, the perspective is different. When we &quot;vibe-code&quot; or prototype with AI, we can't just stop at &quot;it works.&quot; We have to look at the codebase, the structure, optimization, and product requirements. Everything needs to be clearly defined so that the generated code is stable, consistent with the existing codebase, and maintainable in the future.  Some survey data from Stack Overflow in 2025 shows the reality of AI-generated code in the industry. Most respondents (72%) stated they do not practice &quot;vibe coding&quot;, with an additional 5% emphatically rejecting it as part of their workflow [ref]. Among over 31,000 survey participants, 66% of developers chose &quot;AI solutions that are almost right, but not quite,&quot; and 45% of developers said &quot;Debugging AI-generated code is more time-consuming&quot; [ref].  ","version":null,"tagName":"h3"},{"title":"AI as Workflow Optimization‚Äã","type":1,"pageTitle":"My Perspectives on AI Agents: Beyond the 'Vibe-Coding' Hype","url":"/blog/my-perspectives-on-ai-agents-beyond-the-vibe-coding-hype#ai-as-workflow-optimization","content":" Fundamentally, working with AI is like optimizing your workflow. But optimization isn't always straightforward.  AI tends to increase pressure on specific points in your workflow. It can generate output effectively, but it can also generate &quot;garbage&quot; or create additional downstream work.    For example, when using AI to generate a blog post, you might get a draft very quickly. But this shifts the workload: now you have to read, edit, verify, and sometimes even rewrite significant portions. You've increased the pressure on the &quot;editing&quot; phase. If you simply add AI without removing redundant steps elsewhere in your process, you create an imbalance. You end up creating more work for one position instead of streamlining the whole process.    In software development, reviewers may need to spend more time examining the &quot;source code&quot; of team members, which can be more time-consuming than writing the code. Or, just send a text message &quot;LGTM&quot; üòÇ, no risk involved, trust me bro!  More developers actively distrust the accuracy of AI tools (46%) than trust it (33%), and only a fraction (3%) report &quot;highly trusting&quot; the output. Experienced developers are the most cautious, with the lowest &quot;highly trust&quot; rate (2.6%) and the highest &quot;highly distrust&quot; rate (20%), indicating a widespread need for human verification for those in roles with accountability. ‚Äî Stack Overflow 2025 Developer Survey ‚Äî  Balancing this workflow is key to actually saving time and energy.  I saw an interesting point on Reddit recently regarding OpenClaw. A guy noted that instead of doing less work, he ended up with just another thing to manage.  ","version":null,"tagName":"h3"},{"title":"Personal Use-Cases‚Äã","type":1,"pageTitle":"My Perspectives on AI Agents: Beyond the 'Vibe-Coding' Hype","url":"/blog/my-perspectives-on-ai-agents-beyond-the-vibe-coding-hype#personal-use-cases","content":" Currently, I primarily use AI apps that streamline my workflow and provide results with citations, allowing me to trace sources and verify the accuracy of the information.  Some use-cases I can share:  Studying and learning: NotebookLM and Google AI Studio are my go-to tools for learning new topics and keeping updated with the latest research. What I prepare is a good prompt/form to guide the AI to give me the right information I need. Guiding AI to generate content following summarization frameworks like the Feynman Technique, PACES, or SQ3R effectively improves the quality of the output.    Coding tasks: GitHub Copilot is my main assistant for coding. It helps me write code faster and more efficiently, but I always review and optimize the generated code to ensure it meets my standards and fits well with the project's goals. Fortunately, Copilot has significantly improved its utility tools and integrates with various LLMs.  I also use Antigravity to access more LLM resources üòÖ I like the way AI generates implementation plans that I can review and comment on to adjust. I think that IDE has a lot of work to do to surpass the experience offered by VS Code.  At this point, why GitHub Copilot? Nah, this isn't a recommendation, it's just me sharing my preference. My company provides the GitHub Copilot Enterprise plan, so I just want to use a &quot;tool&quot; to serve my workflow (work and personal tasks).  Automation tasks: Grok Tasks is a great tool for researching and automating repetitive tasks. I have some tasks about searching for newest papers and financial news. Hmmm, maybe, I can create a task to write a blog about trending topics in Technology and post to this blog???? Haha, nvm üòÇ I only want to push something that I really think about to this place.    ","version":null,"tagName":"h2"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"My Perspectives on AI Agents: Beyond the 'Vibe-Coding' Hype","url":"/blog/my-perspectives-on-ai-agents-beyond-the-vibe-coding-hype#conclusion","content":" Honestly, this post was generated with the help of AI. I took some keynotes and ideas that I wanted to share, and then I used AI to help me structure and write the content. In addition, before generating the content, I prompted the Agent to read all of my blogs and understand my writing style.  But don't worry, this blog still conveys my ideas and perspectives, which form &quot;the soul&quot; of the content. AI is just a tool to help me express those ideas more efficiently. I hope this post gives you some insights and maybe inspires you to explore how they can fit into your own work!  Happy new year, and I hope you have a great 2026!  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"My Perspectives on AI Agents: Beyond the 'Vibe-Coding' Hype","url":"/blog/my-perspectives-on-ai-agents-beyond-the-vibe-coding-hype#references","content":" AI 2027AI | 2025 Stack Overflow Developer Survey ","version":null,"tagName":"h2"},{"title":"Essential modules for developing applications with FastAPI (P1 - Migration)","type":0,"sectionRef":"#","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration","content":"","keywords":"","version":null},{"title":"Framework/Library version‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P1 - Migration)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration#frameworklibrary-version","content":" This project uses Python 3.10 as the environment and Poetry as the package manager.  The code and examples in this post will use frameworks/libraries with the following versions.  ./pyproject.toml [tool.poetry.dependencies] python = &quot;^3.10&quot; uvicorn = {extras = [&quot;standard&quot;], version = &quot;^0.24.0.post1&quot;} fastapi = &quot;^0.109.1&quot; python-multipart = &quot;^0.0.7&quot; email-validator = &quot;^2.1.0.post1&quot; passlib = {extras = [&quot;bcrypt&quot;], version = &quot;^1.7.4&quot;} tenacity = &quot;^8.2.3&quot; pydantic = &quot;&gt;2.0&quot; emails = &quot;^0.6&quot; gunicorn = &quot;^21.2.0&quot; jinja2 = &quot;^3.1.2&quot; alembic = &quot;^1.12.1&quot; python-jose = {extras = [&quot;cryptography&quot;], version = &quot;^3.3.0&quot;} httpx = &quot;^0.25.1&quot; psycopg = {extras = [&quot;binary&quot;], version = &quot;^3.1.13&quot;} sqlmodel = &quot;^0.0.16&quot; # Pin bcrypt until passlib supports the latest bcrypt = &quot;4.0.1&quot; pydantic-settings = &quot;^2.2.1&quot; sentry-sdk = {extras = [&quot;fastapi&quot;], version = &quot;^1.40.6&quot;} psycopg2 = &quot;^2.9.9&quot; asyncpg = &quot;^0.29.0&quot;   ","version":null,"tagName":"h2"},{"title":"Migration‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P1 - Migration)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration#migration","content":" If you work with SQL databases, these are important components you need to set up. What I want to talk about is Alembic.  Alembic¬†is a lightweight database migration tool for usage with the¬†SQLAlchemy¬†Database Toolkit for Python.  To add or initialize a module, you can use the following commands:  poetry add alembic # pip install alembic alembic init migrations   Note that ‚Äúmigrations‚Äù is the name of the directory where you want to store your migration versions.  ","version":null,"tagName":"h2"},{"title":"Directory structure‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P1 - Migration)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration#directory-structure","content":" Project/ ‚îú‚îÄ‚îÄ app/ ‚îÇ ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ migrations/ ‚îÇ ‚îú‚îÄ‚îÄ versions/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ... ‚îÇ ‚îú‚îÄ‚îÄ README ‚îÇ ‚îú‚îÄ‚îÄ env.py ‚îÇ ‚îî‚îÄ‚îÄ script.py.mako ‚îú‚îÄ‚îÄ alembic.ini ‚îú‚îÄ‚îÄ pyproject.toml ‚îú‚îÄ‚îÄ poetry.lock ‚îú‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ ...   ","version":null,"tagName":"h3"},{"title":"alembic.ini‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P1 - Migration)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration#alembicini","content":" I will use its default configuration. Or go to this link https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file to get more details.  ./alembic.ini # A generic, single database configuration. [alembic] # path to migration scripts script_location = migrations # template used to generate migration files # file_template = %%(rev)s_%%(slug)s # sys.path path, will be prepended to sys.path if present. # defaults to the current working directory. prepend_sys_path = . ...   ","version":null,"tagName":"h3"},{"title":"env.py‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P1 - Migration)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration#envpy","content":" In this version of FastAPI, SQLModel appears as an integrated version of SQLAlchemy and Pydantic and is recommended for use in the official documentation.  So instead of using DeclarativeMeta, we will use SQLModel.metadata in Alembic‚Äôs env.  After completing the steps above, the env.py will look like this.  ./migrations/env.py from logging.config import fileConfig from sqlalchemy import engine_from_config from sqlalchemy import pool from alembic import context from app.core.db import SQLModel # &lt;--- CHANGE HERE from app.core.config import settings # &lt;--- CHANGE HERE # this is the Alembic Config object, which provides # access to the values within the .ini file in use. config = context.config # Interpret the config file for Python logging. # This line sets up loggers basically. if config.config_file_name is not None: fileConfig(config.config_file_name) # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata target_metadata = SQLModel.metadata # &lt;--- CHANGE HERE # other values from the config, defined by the needs of env.py, # can be acquired: # my_important_option = config.get_main_option(&quot;my_important_option&quot;) # ... etc. config.set_section_option(&quot;alembic&quot;, &quot;sqlalchemy.url&quot;, str(settings.SQLALCHEMY_DATABASE_URI)) # &lt;--- CHANGE HERE def run_migrations_offline() -&gt; None: ...   One note is that to achieve the Alembic ‚Äîautogenerate option, you must import the entity representing your table where you import SQLModel into env. For example app/core/db.py:  ./app/core/db.py from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker from sqlmodel import SQLModel, create_engine from app.core.config import settings async_engine = create_async_engine(settings.SQLALCHEMY_DATABASE_URI_ASYNC) async_session_maker = sessionmaker( async_engine, class_=AsyncSession, expire_on_commit=False, autocommit=False, autoflush=False, ) # We still have a second old style sync SQLAlchemy engine for shell and alembic engine = create_engine(settings.SQLALCHEMY_DATABASE_URI, future=True) SessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False) Base = declarative_base() SQLModel.metadata = Base.metadata # import all models to automatically create migration through Alembic from app.modules.users.model import UserInDb ...   Alternatively, if you organize your database models within a directory, you can leverage the __init__.py file to gather all the models you want Alembic to create migrations automatically. For instance app/models/__init__.py:  ./app/models/__init__.py from app.core.db import SQLModel from .user import UserInDb from .item import ItemInDb ...   And migrations/env.py:  ./migrations/env.py from logging.config import fileConfig from sqlalchemy import engine_from_config from sqlalchemy import pool from alembic import context from app.models import SQLModel # &lt;--- CHANGE HERE ...   ","version":null,"tagName":"h3"},{"title":"script.py.mako‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P1 - Migration)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration#scriptpymako","content":" To make things go smoothly, don't forget to import the sqlmodel into this file.  ./migrations/script.py.mako &quot;&quot;&quot;${message} Revision ID: ${up_revision} Revises: ${down_revision | comma,n} Create Date: ${create_date} &quot;&quot;&quot; from typing import Sequence, Union from alembic import op import sqlalchemy as sa import sqlmodel # &lt;--- CHANGE HERE ${imports if imports else &quot;&quot;} ...   ","version":null,"tagName":"h3"},{"title":"How to use?‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P1 - Migration)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration#how-to-use","content":"   To create a new revision: alembic revision --autogenerate -m &quot;message&quot; To migrate: alembic upgrade head To undo the latest migration: alembic downgrade -1 To roll back to a specific revision: alembic downgrade &lt;&lt;revision_id&gt;&gt;   ","version":null,"tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P1 - Migration)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration#conclusion","content":" We discussed an essential module in developing applications with FastAPI: Migration. By carefully and effectively using this module, we can build and maintain powerful and flexible FastAPI applications. Hopefully, this article has given you the overview and knowledge needed to use this important module in your projects.  If you need a project to run a demo on your environment, here are my¬†Git repository.  See you again!  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P1 - Migration)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration#references","content":" How to get Alembic to recognise SQLModel database model?Tutorial ‚Äî Alembic 1.13.2 documentation (sqlalchemy.org) ","version":null,"tagName":"h2"},{"title":"Asynchronous Request Batching Design Pattern in Node.js","type":0,"sectionRef":"#","url":"/blog/asynchronous-request-batching-design-pattern-in-nodejs","content":"","keywords":"","version":null},{"title":"Define the Problem‚Äã","type":1,"pageTitle":"Asynchronous Request Batching Design Pattern in Node.js","url":"/blog/asynchronous-request-batching-design-pattern-in-nodejs#define-the-problem","content":" In reality, a large number of systems are expected to face issues with high throughput and large workloads. And the projects I have been assigned to are no exception üòÅ. So‚Ä¶ what is the solution? At that time, I imagined a lot of solutions like caching, scaling, partitioning,‚Ä¶  Caching seems quite efficient. However, it comes with the challenge of invalidating cached data. Is there a similar approach to caching that doesn't involve worrying about data invalidation? Is there any simpler way?  The Asynchronous Request Batching Design Pattern appears. This design pattern is really appealing to me at the moment.  ","version":null,"tagName":"h2"},{"title":"Introduction to Asynchronous Request Batching‚Äã","type":1,"pageTitle":"Asynchronous Request Batching Design Pattern in Node.js","url":"/blog/asynchronous-request-batching-design-pattern-in-nodejs#introduction-to-asynchronous-request-batching","content":" Let's say I export an API and the Client makes multiple requests to that API at the same time. The server receives those requests and processes them concurrently. Please look at the image below.    In the example above, both Client 1 and Client 2 make requests to the server. Each request is considered an async operation. As the number of requests increases, the number of asynchronous operations that need to be executed also grows.  Now, let's talk about batching.    Still using the example with Client 1 and Client 2, but this time, their requests are batched together and processed in just one async operation. By doing this, when the operation completes, both clients are notified, even though the¬†async operation is actually executed only once.  This approach offers a remarkably simple yet powerful way to optimize the load of an application. It avoids the complexity of caching mechanisms, which often require robust memory management and invalidation strategies.  ","version":null,"tagName":"h2"},{"title":"Normal API server‚Äã","type":1,"pageTitle":"Asynchronous Request Batching Design Pattern in Node.js","url":"/blog/asynchronous-request-batching-design-pattern-in-nodejs#normal-api-server","content":" Let's consider an API server that manages the sales of an e-commerce company. Its task is to retrieve quantity information for hundreds of thousands of products from the database.  The schema used for this example is a simple table with three fields.  CREATE TABLE sales ( id INTEGER PRIMARY KEY AUTOINCREMENT, product varchar(255), quantity bigint )   The data processing operation is straightforward. It involves retrieving all records with the corresponding product and calculating their total quantity. The algorithm is intentionally slow as we want to highlight the effect of batching and caching later on. The routine would look as follows (file¬†totalQuantity.js):  ./totalQuantity.js import db from './database.js'; export async function totalQuantity(product) { const now = Date.now(); let sql = `SELECT product, quantity FROM sales WHERE product=?`; let total = 0; return new Promise(function (resolve, reject) { db.all(sql, [product], (err, rows) =&gt; { if (err) return reject(); rows.forEach((row) =&gt; { total += row.quantity; }) console.log(`totalQuantity() took: ${Date.now() - now}ms`); resolve(total); }) }) }   Now, I will expose the totalQuantity API through a simple Express.js server (the server.js file):  ./server.js import express from 'express'; import { totalQuantity } from './totalQuantity.js'; const app = express(); const PORT = 8000; app.get('/', (req, res) =&gt; { res.send('Hello World!'); }) app.get('/total-quantity', async (req, res) =&gt; { const { product } = req.query; const total = await totalQuantity(product); res.status(200).json({ product, total }) }) app.listen(PORT, () =&gt; { console.log(`Example app listening on PORT ${PORT}`) })   Before starting the server, we will generate some sample data. Leveraging SQLite's lightweight and efficient nature, I've chosen it as the database for this example. Furthermore, I have prepared a script to populate the sales table with 200,000 rows (the Github repository can be found in the conclusion section).  npm run populate   After seeding data:    Let's start the server now.  npm start   ","version":null,"tagName":"h2"},{"title":"Test scenario‚Äã","type":1,"pageTitle":"Asynchronous Request Batching Design Pattern in Node.js","url":"/blog/asynchronous-request-batching-design-pattern-in-nodejs#test-scenario","content":" To clearly illustrate the difference between a server without batching and one with a batching pattern, we will create a scenario with more than one request. So, we will use a script named loadTest.js, which sends 20 requests at intervals of 20ms. The request sending interval should be lower than the totalQuantity function's processing duration.  To run the test, just execute the following command:  node loadTest.js   Note the total time taken for the test.    The results of 3 test runs are 994ms, 954ms, 957ms ~ avg 968ms.  Let's move on to the exciting part of today's discussion: optimizing asynchronous request processing using the batching pattern.  ","version":null,"tagName":"h2"},{"title":"Async request batching server‚Äã","type":1,"pageTitle":"Asynchronous Request Batching Design Pattern in Node.js","url":"/blog/asynchronous-request-batching-design-pattern-in-nodejs#async-request-batching-server","content":" Now, we need to introduce an additional processing layer on top of the totalQuantity function. This is where we implement the mechanism of the batching pattern.  Now, imagine working with caching without worrying about cache invalidation. That makes it much easier to understand.  We will use a memory space to store the promises that need to be processed (it could be an array, map, dict, etc.). And we will differentiate them based on the input of the request. New requests with the same input will reference the promises stored in the memory space. When the promises complete, they will return results to all the requests referencing them, and then we will remove them from the memory space.  Now it's time to code.  ./totalQuantityBatch.js import { totalQuantity as totalQuantityRaw } from &quot;./totalQuantity.js&quot;; const requestsQueue = new Map(); export function totalQuantity(product) { if (requestsQueue.has(product)) { console.log(&quot;Batching...&quot;) return requestsQueue.get(product); } const promise = totalQuantityRaw(product); requestsQueue.set(product, promise); promise.finally(() =&gt; { requestsQueue.delete(product) }) return promise; }   The totalQuantity function of totalQuantityBatch module can be considered a proxy for totalQuantityRaw function of totalQuantity module, let's see how it works:  In line 3, I define the variable requestsQueue as a Map to serve as temporary memory.From line 6-8, we check if a request with the input ‚Äúproduct‚Äù already exists in the temporary memory. If it does, we return the stored promises.From line 11-14, If the input ‚Äúproduct‚Äù does not exist, we start executing the totalQuantityRaw function and store it in the temporary memory. One nice thing is that we can leverage .finally to attach a callback that removes the promise from the temporary memory.In line 16, return running promise.  After completing the logic in the totalQuantityBatch module, we need to update the server.js to incorporate the new logic.  ./server.js import express from 'express'; // import { totalQuantity } from './totalQuantity.js'; import { totalQuantity } from './totalQuantityBatch.js'; ...   Restart the server and let's see how the results change.    Let‚Äôs run the loadTest.js script:    The results of 3 test runs are 624ms, 648ms, 592ms ~ avg 621ms.  We can clearly see that the processing time for all requests has been significantly reduced. To observe even greater efficiency, you can increase the number of records in the database to extend the processing time of the totalQuantity function.  ","version":null,"tagName":"h2"},{"title":"Notes‚Äã","type":1,"pageTitle":"Asynchronous Request Batching Design Pattern in Node.js","url":"/blog/asynchronous-request-batching-design-pattern-in-nodejs#notes","content":" When implementing in a real-world project, we will use more advanced techniques to ensure the application operates reliably and smoothly.  We will need a more optimized temporary memory space. A large number of requests to the server with different inputs later could cause the memory to expand significantly. You might consider using LRU (Least Recently Used) or FIFO (First In, First Out) methods for data management.You can also apply caching to enhance the effectiveness of this technique and standardize the stored data for easier sharing. Of course, you will need to address the issue of cache invalidation.When the application is distributed across multiple processes and instances, storing data in memory in different locations can lead to inconsistent results and become redundant. The solution is to use a shared storage. Common caching solutions include Redis (Redis - The Real-time Data Platform) and Memcached (memcached - a distributed memory object caching system).  ","version":null,"tagName":"h2"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Asynchronous Request Batching Design Pattern in Node.js","url":"/blog/asynchronous-request-batching-design-pattern-in-nodejs#conclusion","content":" I hope this post was useful. If you need a project to run a demo on your environment, here is my¬†Git repository.  See you again!  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Asynchronous Request Batching Design Pattern in Node.js","url":"/blog/asynchronous-request-batching-design-pattern-in-nodejs#references","content":" Asynchronous Request Batching &amp; Caching in Node.js | by Maharshi Shah | GoGroup Tech BlogNode.js Design Patterns Third Edition by Mario Casciaro and Luciano Mammino (nodejsdesignpatterns.com) ","version":null,"tagName":"h2"},{"title":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","type":0,"sectionRef":"#","url":"/blog/dockerizing-medusajs-for-optimized-deployment","content":"","keywords":"","version":null},{"title":"Introduction‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#introduction","content":" ","version":null,"tagName":"h2"},{"title":"What is MedusaJS?‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#what-is-medusajs","content":" MedusaJS is a set of commerce modules and tools that allow you to build rich, reliable, and performant commerce applications without reinventing core commerce logic. The modules can be customized and used to build advanced e-commerce stores, marketplaces, or any product that needs foundational commerce primitives. All modules are open-source and freely available on npm.  Learn more about¬†Medusa‚Äôs architecture¬†and¬†commerce modules¬†in the Docs.  ","version":null,"tagName":"h3"},{"title":"Why containerize Medusa applications?‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#why-containerize-medusa-applications","content":" The benefits of dockerizing an application, specifically Medusa, are manifold:  Set up development environments swiftly: With Docker Compose, you can set up your development environment with just a few commands. For instance, cloning source code, creating a .env file, and running docker-compose up -d.Deploy easily in the cloud or on-premises: Once you build your Docker images, you can run them consistently on any server environment. It also helps you automate your application deployment and version management.Ensure scalability and flexibility: Dockerized applications are easier to scale and expand. By using Docker Swarm or Kubernetes, you can automatically scale the number of application containers to meet user demand without the need to reconfigure infrastructure.  ","version":null,"tagName":"h3"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#prerequisites","content":" Some packages we need to install before proceeding further in this post.  ","version":null,"tagName":"h2"},{"title":"Docker & Docker Compose‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#docker--docker-compose","content":" Visit the official link below and download the version that corresponds to your environment.  Docker Desktop: The #1 Containerization Tool for Developers | Docker  ","version":null,"tagName":"h3"},{"title":"Node or NVM‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#node-or-nvm","content":" Visit the official link below and download the version that corresponds to your environment.  Node.js ‚Äî Download Node.js¬Æ (nodejs.org)  I suggest you install version 18 or greater because:  Backend Admin needs Node v16+Storefront needs Node v18+  Or install nvm instead to install any version of NodeJS  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash # Or wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash   If your environment is Windows, you may like this one  coreybutler/nvm-windows: A node.js version management utility for Windows. Ironically written in Go. (github.com)  ","version":null,"tagName":"h3"},{"title":"Git‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#git","content":" Visit the official link below and download the version that corresponds to your environment.  Git - Downloads (git-scm.com)  ","version":null,"tagName":"h3"},{"title":"Medusa CLI‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#medusa-cli","content":" To install the Medusa backend, you need Medusa's CLI tool.  npm install -g @medusajs/medusa-cli   Once done, check the version using this command:  medusa --version     ","version":null,"tagName":"h3"},{"title":"Create Medusa Backend‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#create-medusa-backend","content":" We can easily create the backend with the following command:  npx create-medusa-app --skip-db   I use the ‚Äîskip-db option because I will have a different setup later.  Now wait a moment‚Ä¶    ","version":null,"tagName":"h3"},{"title":"Create Medusa Storefront‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#create-medusa-storefront","content":" Create a new Next.js project using the¬†Medusa starter Storefront:  npx create-next-app -e https://github.com/medusajs/nextjs-starter-medusa storefront   ","version":null,"tagName":"h3"},{"title":"Containerize Medusa application‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#containerize-medusa-application","content":" ","version":null,"tagName":"h2"},{"title":"Backend‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#backend","content":" First, create the develop.sh file with this content and place it in the /backend folder.  ./backend/develop.sh #!/bin/bash medusa migrations run medusa $1   Next, create a Dockerfile with this content in the same directory as in the previous step.  ./backend/Dockerfile ### Dependencies ### FROM node:20-alpine as deps WORKDIR /app/ # Copy backend package package.json and yarn.lock from /backend COPY ./package*.json . COPY ./yarn.lock? . # Install deps RUN yarn install --frozen-lockfile ### Build ### FROM node:20-alpine as builder WORKDIR /app/ # Copy cached node_modules from deps COPY --from=deps /app/node_modules /app/node_modules # Install python and medusa-cli RUN apk update RUN apk add python3 RUN yarn global add @medusajs/medusa-cli@latest # Copy app source code COPY . /app # Build the app RUN yarn build # Start the image with a shell script ENTRYPOINT [&quot;/bin/sh&quot;, &quot;./develop.sh&quot;, &quot;start&quot;]   The result:    ","version":null,"tagName":"h3"},{"title":"Admin UI‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#admin-ui","content":" According to the announcement, the admin UI source code has been moved into the core repository since version 1.8. So, we won't have the source code to build the Docker image anymore.  Wait, hold on‚Ä¶  When you run the command medusa develop, the admin UI will build static javascript and HTML files into the /backend/build directory, so we need to copy it and serve it with nginx.  A small note is that if you deploy it in production, you can configure it not to build the admin UI. Splitting it into two separate Docker images is the approach I'm currently taking. Check this one.  First, create the admin.nginx.conf file with this content and place it in the /backend folder.  ./backend/admin.nginx.conf server { listen 80 default_server; server_name localhost; charset utf-8; root /usr/share/nginx/html; location / { try_files $uri $uri/ /index.html =404; } }   Next, create a Dockerfile with this content in the same directory as in the previous step.  ./backend/Dockerfile.admin ### Build ### FROM node:20-alpine as builder WORKDIR /app ENV NODE_ENV=production ENV NODE_OPTIONS=--openssl-legacy-provider # Copy medusa package.json and yarn.lock from /backend COPY ./package*.json . COPY ./yarn.lock? . # Install deps RUN yarn install --frozen-lockfile # Copy app source code COPY . /app RUN yarn build:admin:prod ### Runner ### FROM nginx:1.16.0-alpine as runner # Copy nginx.conf from /backend COPY admin.nginx.conf /etc/nginx/conf.d/default.conf RUN apk add --no-cache bash EXPOSE 80 # Copy static files from Build stage COPY --from=builder /app/build /usr/share/nginx/html # Image entrypoint ENTRYPOINT [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]   The result:    ","version":null,"tagName":"h3"},{"title":"Storefront‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#storefront","content":" Similar to the components above, we will have the following Dockerfile.  ./storefront/Dockerfile ### Dependencies ### FROM node:20-alpine as deps WORKDIR /app # Copy storefront package.json and yarn.lock from /storefront COPY ./package*.json . COPY ./yarn.lock? . # Install deps and launch patch-package RUN yarn install --frozen-lockfile ### Build ### FROM node:20-alpine as builder WORKDIR /app # Copy cached root and package node_modules from deps COPY --from=deps /app/node_modules /app/node_modules # Copy app source code COPY . /app # Build the app RUN yarn build # Run the builded app ENTRYPOINT [ &quot;yarn&quot;, &quot;start&quot; ]   ","version":null,"tagName":"h3"},{"title":"Manage containers‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#manage-containers","content":" With the above sections, we are ready to build and deploy the Docker image. Next, I will introduce how I manage the containers and configure them for deployment using Docker Compose.  ","version":null,"tagName":"h2"},{"title":"Manage environment variables‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#manage-environment-variables","content":" Firstly, we should provide some environment variables for the app. In /backend, create a .env file and paste this content:  JWT_SECRET=something COOKIE_SECRET=something POSTGRES_USER=postgres POSTGRES_PASSWORD=CHANGE_ME POSTGRES_DB=medusa DATABASE_TYPE=&quot;postgres&quot; REDIS_URL=redis://redis DATABASE_URL=postgres://postgres:CHANGE_ME@postgres:5432/medusa STORE_CORS=/http://.+/ ADMIN_CORS=/http://.+/ AUTH_CORS=/http://.+/   Check out the documentation https://docs.medusajs.com/development/backend/configurations#environment-variables.  Next, in /storefront, make a copy of .env.template to .env.local. Then change the value of NEXT_PUBLIC_MEDUSA_BACKEND_URL variable to http://host.docker.internal:9000, if you use Windows.  ","version":null,"tagName":"h3"},{"title":"Docker Compose configuration‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#docker-compose-configuration","content":" Let‚Äôs create a docker-compose.yml file at the same level as the /backend and /storefront directories.  I declare configurations for the database and Redis as follows:  ./docker-compose.yml version: &quot;3.8&quot; services: postgres: image: postgres:12 ports: - &quot;54322:5432&quot; env_file: ./backend/.env volumes: - app-db-data:/var/lib/postgresql/data:cached redis: image: redis expose: - 6379 volumes: app-db-data:   These are the services that must be ready before the application container is initialized and connected to them.  Now I will define the backend and admin UI services.  ./docker-compose.yml ... backend: build: context: ./backend dockerfile: Dockerfile image: pxuanbach/medusa-backend depends_on: - postgres - redis env_file: ./backend/.env ports: - &quot;9000:9000&quot; admin: build: context: ./backend dockerfile: Dockerfile.admin image: pxuanbach/medusa-backend-admin depends_on: - postgres - redis ports: - &quot;7001:80&quot;   The last service is storefront, it needs to be initialized after the backend service. So, here is its configuration:  ./docker-compose.yml ... storefront: build: context: ./storefront dockerfile: Dockerfile.prod image: pxuanbach/medusa-storefront depends_on: - backend ports: - &quot;8000:8000&quot;   ","version":null,"tagName":"h3"},{"title":"Launch all services‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#launch-all-services","content":" After configuring the docker-compose.yml file, we will run it to see how it operates.  docker-compose up -d --build postgres redis backend admin # waiting... # init seed data (Optional) docker-compose exec backend yarn seed # wait... docker-compose up -d --build storefront   In the above command, I run backend before storefront. Because the storefront needs to fetch the region from the backend to build successfully.  Once done, check the results:  Admin Panel: http://localhost:7001 Backend: http://localhost:9000/store/products Storefront: http://localhost:8000  Notes:  If you cannot connect to the Admin Panel or Storefront via Docker container, try changing the default host of each to 0.0.0.0.Storefront needs to fetch the regions from the Backend. So, you should initialize your Backend data first.  ","version":null,"tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#conclusion","content":" I hope this post was useful. If you need a project to run a demo on your environment, here is my¬†Git repository.  In this repository, I have separated into 2 compose configurations. The configuration of this post is equivalent to prod configuration in the repos. Additionally, I created a script called start-up.sh in the demo repository. It will help us to start all services automatically.  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","url":"/blog/dockerizing-medusajs-for-optimized-deployment#references","content":" feat: update backend and storefront dockerfiles by mickeiik ¬∑ Pull Request #13 ¬∑ medusajs/docker-medusa (github.com)Install Medusa Backend | Medusa (medusajs.com) ","version":null,"tagName":"h2"},{"title":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","type":0,"sectionRef":"#","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring","content":"","keywords":"","version":null},{"title":"Framework/Library version‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring#frameworklibrary-version","content":" This project uses Python 3.10 as the environment and Poetry as the package manager.  The code and examples in this post will use frameworks/libraries with the following versions.  [tool.poetry.dependencies] python = &quot;^3.10&quot; uvicorn = {extras = [&quot;standard&quot;], version = &quot;^0.24.0.post1&quot;} fastapi = &quot;^0.109.1&quot; python-multipart = &quot;^0.0.7&quot; email-validator = &quot;^2.1.0.post1&quot; passlib = {extras = [&quot;bcrypt&quot;], version = &quot;^1.7.4&quot;} tenacity = &quot;^8.2.3&quot; pydantic = &quot;&gt;2.0&quot; emails = &quot;^0.6&quot; gunicorn = &quot;^21.2.0&quot; jinja2 = &quot;^3.1.2&quot; alembic = &quot;^1.12.1&quot; python-jose = {extras = [&quot;cryptography&quot;], version = &quot;^3.3.0&quot;} httpx = &quot;^0.25.1&quot; psycopg = {extras = [&quot;binary&quot;], version = &quot;^3.1.13&quot;} sqlmodel = &quot;^0.0.16&quot; # Pin bcrypt until passlib supports the latest bcrypt = &quot;4.0.1&quot; pydantic-settings = &quot;^2.2.1&quot; sentry-sdk = {extras = [&quot;fastapi&quot;], version = &quot;^1.40.6&quot;} psycopg2 = &quot;^2.9.9&quot; asyncpg = &quot;^0.29.0&quot; redis = {extras = [&quot;hiredis&quot;], version = &quot;^5.0.3&quot;} orjson = &quot;^3.10.0&quot; apscheduler = &quot;^3.10.4&quot; prometheus-fastapi-instrumentator = &quot;^7.0.0&quot;   ","version":null,"tagName":"h2"},{"title":"Monitoring‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring#monitoring","content":" I am confident that this is one of the most important modules when you want to deploy a product to a production environment.  Currently, many packages can help you monitor your FastAPI app. The package I like to use the most is Prometheus FastAPI Instrumentator (919 stars, 83 forks on 03/09/2024). I often use it along with Prometheus and Grafana, creating visual dashboards from those metrics helps me easily monitor the application. I will discuss this stack in another article.    How it‚Äôs work?  The API server exports an endpoint /metrics, which tracks the HTTP request count, request/response size in bytes, request duration, and more.We register the /metrics endpoint and set the crawl job duration for the Prometheus service. Next, this service automatically ingests metrics data from the API server, storing the collected data for analysis.Grafana acts as an admin dashboard, retrieving data from Prometheus and visualizing it in specialized graphs, such as time-series, line charts.  Let's integrate this package with FastAPI and deploy Prometheus and Grafana to visualize its metrics.  ","version":null,"tagName":"h2"},{"title":"Integrating with FastAPI‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring#integrating-with-fastapi","content":" We need to initialize an instance of the instrumentator and export it when the application starts.  ./app/main.py from fastapi import FastAPI from contextlib import asynccontextmanager from prometheus_fastapi_instrumentator import Instrumentator @asynccontextmanager async def lifespan(app: FastAPI): # start up instrumentator.expose(app) yield # shut down pass app = FastAPI( title=settings.PROJECT_NAME, openapi_url=f&quot;{settings.API_STR}{settings.API_VERSION_STR}/openapi.json&quot;, generate_unique_id_function=custom_generate_unique_id, lifespan=lifespan ) instrumentator = Instrumentator( should_group_status_codes=False, should_ignore_untemplated=True, excluded_handlers=[&quot;.*admin.*&quot;, &quot;/metrics&quot;], ).instrument(app)   After initializing and running the application, you can easily find monitoring metrics at the /metrics endpoint.    ","version":null,"tagName":"h3"},{"title":"Setting up Prometheus and Grafana services‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring#setting-up-prometheus-and-grafana-services","content":" To quickly see the results, I recommend that you simply copy and paste as instructed in this section.  First, create a docker-compose.yaml file like this:  ./monitoring/docker-compose.yaml services: prometheus: image: prom/prometheus:v2.54.0 volumes: - ./prometheus.yaml:/etc/prometheus/prometheus.yml - prometheus-data:/prometheus command: - '--config.file=/etc/prometheus/prometheus.yml' restart: unless-stopped grafana: image: grafana/grafana:10.4.7 ports: - &quot;3001:3000&quot; volumes: - grafana-data:/var/lib/grafana volumes: prometheus-data: grafana-data:   Next, create a prometheus.yaml file in the same location.  ./monitoring/prometheus.yaml global: scrape_interval: 15s scrape_configs: - job_name: fastapi scrape_interval: 15s scrape_timeout: 10s metrics_path: '/metrics' static_configs: - targets: ['host.docker.internal:8000']   You might be curious about the host.docker.internal address. What is it?  The reason for this is that Docker network works differently on Linux, Windows, and macOS.On Linux, Docker uses the system's built-in networking features.But on Windows and macOS, Docker runs inside a virtual machine. Because of this, Windows and macOS need a special way for containers to talk to the host machine, which is why we use host.docker.internal.  Since my OS is Windows, I'm using it as the host for the Prometheus service so that it can call the FastAPI server, which is not containerized.    Now, docker compose up!  docker compose -f ./monitoring/docker-compose.yaml up -d     ","version":null,"tagName":"h3"},{"title":"Configuring Grafana Dashboard‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring#configuring-grafana-dashboard","content":" Once the services are up and running, you will see logs for requests to the /metrics endpoint being generated every 15 seconds (the scrape_interval of Prometheus).    Next, open your browser and go to 127.0.0.1:3001; the Grafana login page will appear. The default username and password are admin.  After logging in, add Prometheus as a Data source with the connection URL set to http://prometheus:9090.  Why is prometheus:9090?  Containers with a shared Docker Compose configuration can communicate with each other using the service name as the domain.9090 is the default port of the Prometheus service.    Back to Home, click ‚ÄúCreate your first dashboard‚Äù &gt; &quot;Import a dashboard‚Äù. You will see the following screen.    You can easily find the dashboard.json file in my github repository. Import that file and see the result.    Niceee!!!  ","version":null,"tagName":"h3"},{"title":"Notes‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring#notes","content":" These are not all the statistics that need to be tracked. In the real world, we need to track many more statistics, such as logs, RAM usage, CPU usage, total connections used, etc. However, for the scope of this article, this is enough to show you how we can monitor a FastAPI application.  Additionally, I would like to mention the use of packages that help monitor the application. These packages also consume resources to monitor the system, and they can become a performance bottleneck for the application. Always use it carefully, so you don't get into a mess.  Furthermore, you can also refer to a few other libraries such as‚Ä¶  stephenhillier/starlette_exporter (310 stars, 35 forks on 03/09/2024)perdy/starlette-prometheus (272 stars, 31 forks on 03/09/2024)acidjunk/starlette-opentracing (66 stars, 6 forks on 03/09/2024)open-telemetry/opentelemetry-instrumentation-fastapi (###)  ","version":null,"tagName":"h2"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring#conclusion","content":" I hope this post was useful. If you need a project to run a demo on your environment, here is my¬†Git repository.  See you again!  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring#references","content":" Kludex/fastapi-prometheus-grafana: FasAPI + Prometheus + Grafana! üéâ (github.com) ","version":null,"tagName":"h2"},{"title":"Destructuring in Python","type":0,"sectionRef":"#","url":"/blog/destructuring-in-python","content":"","keywords":"","version":null},{"title":"What is Destructuring in Python?‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#what-is-destructuring-in-python","content":" Destructuring syntax is an extremely useful feature in Python that breaks down values from lists, tuples, or dictionary attributes into individual variables. It helps us write clean and readable code.    Now, let‚Äôs explore it!  ","version":null,"tagName":"h2"},{"title":"Destructuring with Lists and Tuples‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#destructuring-with-lists-and-tuples","content":" ","version":null,"tagName":"h2"},{"title":"Standard concept‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#standard-concept","content":" We can easily unpack Lists or Tuples using the following syntax:  # list first, second, third = [3, 6, 8] print(first) # 3 print(second) # 6 print(third) # 8 # tuple one, two, three = (1, 2, 3) print(one) # 1 print(two) # 2 print(three) # 3   These are just the standard concepts for destructuring lists or tuples. At this point, we should be concerned with the order and number of elements of a list or a tuple.  Using the wrong order will lead to incorrect data flow in the system. This can become a major issue for your system, where high data consistency is essential.If the number of variables to be unpacked is not equal to the length of the object. It will raise a ValueError exception. For example:  try: first, second, third, four = [3, 6, 8] except ValueError as e: print(&quot;ValueError -&quot;, str(e)) # ValueError - not enough values to unpack (expected 4, got 3)   ","version":null,"tagName":"h3"},{"title":"Ignoring values‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#ignoring-values","content":" In practice, there are many cases where we just want to unpack some values in a list or a tuple. How can we do that? Luckily, we have some syntax to make this more convenient.  With an _ character In place of a variable name. We can skip the unused element and move on to the next element in the list or tuple.  Code example:  one, _, three, _, _ = [1, 2, 3, 4, 5] print(one, three) # 1 3   ","version":null,"tagName":"h3"},{"title":"Assign the remaining values‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#assign-the-remaining-values","content":" In some cases, we still want to use the remaining values. The * operator will help us do it. In Python, we can use the¬†*¬†operator to collect leftover values when performing a destructuring assignment.  a, b, *re = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;] print(a) # a print(b) # b print(re) # ['c', 'd', 'e'] *start, end = (&quot;dog&quot;, &quot;cat&quot;, &quot;frog&quot;, &quot;crab&quot;) print(start) # ['dog', 'cat', 'frog'] print(end) # crab   We can use the _ character and the * operator together to ignore the remaining values.  a, *_ = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;] print(a) # a start, *_, end = (&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) print(start) # a print(end) # e   ","version":null,"tagName":"h3"},{"title":"Destructuring in loops‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#destructuring-in-loops","content":" We are familiar with the syntax of for loops. We can access each element in a list directly instead of using an index like in some other languages. This makes our code more Pythonic.  users = [ ( 1, &quot;Bach&quot;, &quot;HCM&quot; ), ( 2, &quot;Nam&quot;, &quot;HN&quot; ), ( 3, &quot;Trung&quot;, &quot;NT&quot; ) ] for user in users: print(user) # (1, 'Bach', 'HCM') # (2, 'Nam', 'HN') # (3, 'Trung', 'NT')   With destructuring syntax, we can access individual attributes inside an element in for loops. We can write clearer and more readable code. For example:  users = [ ( 1, &quot;Bach&quot;, &quot;HCM&quot; ), ( 2, &quot;Nam&quot;, &quot;HN&quot; ), ( 3, &quot;Trung&quot;, &quot;PR-TC&quot; ) ] for id, name, city in users: print(&quot;Id:&quot;, id, &quot;- Name:&quot;, name, &quot;- City:&quot;, city) # Id: 1 - Name: Bach - City: HCM # Id: 2 - Name: Nam - City: HN # Id: 3 - Name: Trung - City: PR-TC   Or you can even get the index of the element with the enumerate object.  The enumerate object yields pairs containing a count (from start, which defaults to zero) and a value yielded by the iterable argument. ‚Äî Python Docs ‚Äî  users = [ ( 1, &quot;Bach&quot;, &quot;HCM&quot; ), ( 2, &quot;Nam&quot;, &quot;HN&quot; ), ( 3, &quot;Trung&quot;, &quot;PR-TC&quot; ) ] for index, (id, name, city) in enumerate(users): print(&quot;Index:&quot;, index, &quot;- Id:&quot;, id, &quot;- Name:&quot;, name, &quot;- City:&quot;, city) # Index: 0 - Id: 1 - Name: Bach - City: HCM # Index: 1 - Id: 2 - Name: Nam - City: HN # Index: 2 - Id: 3 - Name: Trung - City: PR-TC   Furthermore, we can combine the use of the ignore values syntax and collect the remaining values.  users = [ [ 1, &quot;Bach&quot;, &quot;HCM&quot;, &quot;Python&quot; ], [ 2, &quot;Nam&quot;, &quot;HN&quot;, &quot;JavaScript&quot; ], [ 3, &quot;Trung&quot;, &quot;PR-TC&quot;, &quot;TypeScript&quot; ] ] for id, _, *values in users: print(&quot;Id:&quot;, id, &quot;- Value:&quot;, values) # Id: 1 - Value: ['HCM', 'Python'] # Id: 2 - Value: ['HN', 'JavaScript'] # Id: 3 - Value: ['PR-TC', 'TypeScript']   ","version":null,"tagName":"h2"},{"title":"Destructuring dictionaries‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#destructuring-dictionaries","content":" In my work, I often encounter situations where I need to handle objects/dictionaries with many key-value pairs.  ","version":null,"tagName":"h2"},{"title":"Standard concept‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#standard-concept-1","content":" Let's evaluate the example below.  customer = { &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Cena&quot;, &quot;age&quot;: 23, &quot;email&quot;: &quot;johncena@gmail.com&quot; } one, two, three, four = customer print(f&quot;One '{one}', two '{two}', three '{three}', four '{four}'&quot;) # One 'first_name', two 'last_name', three 'age', four 'email' one, two, three, four = customer.values() print(f&quot;One '{one}', two '{two}', three '{three}', four '{four}'&quot;) # One 'John', two 'Cena', three '23', four 'johncena@gmail.com'   In this example, when we try to get the variables one, two, three, and four, these variables will receive the corresponding values of the keys from the customer dictionary. Or we can get the list of values of the keys in that order by using the .values() method of the dictionary.  Instead of using the above approach, we can directly get the values from the dictionary using their keys.  print(f&quot;Customer email {customer['email']}, age {customer['age']}&quot;) # Customer email johncena@gmail.com, age 23   If you access an unknown key, it will throw a KeyError exception.  ","version":null,"tagName":"h3"},{"title":"Advanced techniques‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#advanced-techniques","content":" The above approaches are completely fine, and you can get the job done quickly without much effort.  But maintaining or reading that code is a nightmare. Imagine you have a dictionary with hundreds of keys (or even more), each part where you access a key of the dictionary and perform logic with it. After a few weeks or months, you get a task related to that code. That's where the nightmare begins.  To solve that problem, we should group the declarations of the variables we need to use together. And I found a way to do that while keeping our code clean and readable.  We can use¬†operator¬†module from the standard library as follows:  from operator import itemgetter current_user = { &quot;id&quot;: 1, &quot;username&quot;: &quot;pxuanbach&quot;, &quot;email&quot;: &quot;pxuanbach@gmail.com&quot;, &quot;phone&quot;: &quot;832819201&quot;, &quot;full_name&quot;: &quot;Bach Pham&quot;, &quot;gender&quot;: &quot;Male&quot;, &quot;website&quot;: &quot;immersedincode.io.vn&quot; } id, email, gender, username = itemgetter( 'id', 'email', 'gender', 'username' )(current_user) print(&quot;Id:&quot;, id, &quot;- Email:&quot;, email, &quot;- Gender:&quot;, gender, &quot;- Username:&quot;, username) # Id: 1 - Email: pxuanbach@gmail.com - Gender: Male - Username: pxuanbach   In the example above, the value of each variable will correspond to the order of keys in the itemgetter function. Additionally, If you access an unknown key, the function will throw a KeyError exception.  ","version":null,"tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#conclusion","content":" So we've covered destructuring lists, tuples, for loops, and dictionaries. I hope this article is helpful to you.  If you need a project to run a demo on your environment, here is my¬†Git repository.  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Destructuring in Python","url":"/blog/destructuring-in-python#references","content":" Destructuring in Python (teclado.com)Destructuring dicts and objects in Python (stackoverflow.com) ","version":null,"tagName":"h2"},{"title":"Welcome","type":0,"sectionRef":"#","url":"/blog/welcome","content":"","keywords":"","version":null},{"title":"What will this blog be about?‚Äã","type":1,"pageTitle":"Welcome","url":"/blog/welcome#what-will-this-blog-be-about","content":" Currently, my primary focus is on back-end development. I also manage the product deployment process and front-end web maintenance.  Therefore, the main content of this blog will revolve around back-end and DevOps topics üòÖ. ","version":null,"tagName":"h2"},{"title":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","type":0,"sectionRef":"#","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting","content":"","keywords":"","version":null},{"title":"Framework/Library version‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#frameworklibrary-version","content":" This project uses Python 3.10 as the environment and Poetry as the package manager.  The code and examples in this post will use frameworks/libraries with the following versions.  [tool.poetry.dependencies] python = &quot;^3.10&quot; uvicorn = {extras = [&quot;standard&quot;], version = &quot;^0.24.0.post1&quot;} fastapi = &quot;^0.109.1&quot; python-multipart = &quot;^0.0.7&quot; email-validator = &quot;^2.1.0.post1&quot; passlib = {extras = [&quot;bcrypt&quot;], version = &quot;^1.7.4&quot;} tenacity = &quot;^8.2.3&quot; pydantic = &quot;&gt;2.0&quot; emails = &quot;^0.6&quot; gunicorn = &quot;^21.2.0&quot; jinja2 = &quot;^3.1.2&quot; alembic = &quot;^1.12.1&quot; python-jose = {extras = [&quot;cryptography&quot;], version = &quot;^3.3.0&quot;} httpx = &quot;^0.25.1&quot; psycopg = {extras = [&quot;binary&quot;], version = &quot;^3.1.13&quot;} sqlmodel = &quot;^0.0.16&quot; # Pin bcrypt until passlib supports the latest bcrypt = &quot;4.0.1&quot; pydantic-settings = &quot;^2.2.1&quot; sentry-sdk = {extras = [&quot;fastapi&quot;], version = &quot;^1.40.6&quot;} psycopg2 = &quot;^2.9.9&quot; asyncpg = &quot;^0.29.0&quot; redis = {extras = [&quot;hiredis&quot;], version = &quot;^5.0.3&quot;} orjson = &quot;^3.10.0&quot; apscheduler = &quot;^3.10.4&quot;   ","version":null,"tagName":"h2"},{"title":"Rate-limiting Overview‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#rate-limiting-overview","content":" ","version":null,"tagName":"h2"},{"title":"What exactly is rate-limiting?‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#what-exactly-is-rate-limiting","content":" Rate limiting is a strategy for limiting network traffic. It puts a cap on how often someone can repeat an action within a certain timeframe ‚Äì for instance, trying to log in to an account. Rate limiting can help stop certain kinds of malicious¬†bot activity. It can also reduce strain on web servers.‚Äî By Cloud Flare ‚Äî  Besides, it also offers other benefits, such as monitoring API usage, enforcing API usage policies, and even implementing some business logic to differentiate customer permission levels.  There are many types of rate-limiting to meet various needs, including Fixed Window Counter, Sliding Window Counter, Token Bucket, and more. Additionally, rule limiting is also a topic of concern. I will introduce Fixed Window Counter rate-limiting in this article with an IP-based limiting mechanism.  ","version":null,"tagName":"h3"},{"title":"How does it work?‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#how-does-it-work","content":" I'll give a few simple examples to describe how Rate-limit works; take a look at the image below:    Suppose we have a Client sending requests to a Server, and we set a rule on the Server to only accept 2 requests within 1 minute. Therefore, if more than 2 requests are sent from the Client, starting from the 3rd request, an error will occur and it will be responded to immediately.  Perhaps you are curious as to why the 1-minute time frame is not calculated from point (1) but rather from point (2). To understand this, you'll need to learn about Rate-Limit algorithms. If the time were calculated from point (1) and I could successfully make 2 requests after 1 minute, that would be the Fixed Window Counter algorithm. I calculate the time from point (2) because I am applying the Sliding Window Counter algorithm.  Fixed Window Counter algorithm can be described as follows:    We still use the 2 requests per minute rule.R1 and R2 belong to Frame 1, and both responded successfully. At this point, Server will count R1 and R2, with count = 2.R3 failed because Frame 1 reached limit (count = 2 = limit).R5 and R6 belong to Frame 2, and do the same with R1, R2.  Sliding Window Counter algorithm can be described as follows:    R1 was successful because Frame 1 had no previous requests. We store R1 with time = 60.R2 also executes successfully. Because in Frame 2, there is only 1 request, which is R1 (count = 1). At this stage, we adds R2 with time = 75 to cache.R3 failed because Frame 3 reached limit. We will check how many requests have been stored from around 30 to 90. Oh, we have R1 and R2 with corresponding times of 60 and 75.R4 was successful because Frame 4 only contains R2 (count = 1). At this point, we will cache R2 and R4.  ","version":null,"tagName":"h3"},{"title":"Prepare before coding‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#prepare-before-coding","content":" In part 3 - caching, we integrated Redis into our system. So I'm going to reuse it to store some stuff.  Now, let's find out what we are going to do.  In the API Server, I usually create a rate-limit module and treat it as middleware. First, the request will pass through the Rate-limit middleware, which will check whether this request is allowed to proceed to the next middleware or the execution layer.    When working with FastAPI, we have many ways to implement this module. I can list a few methods as follows:  Take advantage of the Dependencies feature in FastAPI.Use Python's decorator feature.Build a middleware in the FastAPI application, similar in concept to Global Dependencies.  In this article, I choose option 1 for simplicity and efficiency.  ‚ÄúSimplicity is the soul of efficiency.‚Äù üòÅ  Next, I will create a rate-limit class. It will be responsible for connecting to Redis and executing the algorithm to filter requests. Additionally, we'll need some helper functions to process the input data. Now, let's start coding!  ","version":null,"tagName":"h3"},{"title":"Implement Rate-limit module‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#implement-rate-limit-module","content":" ","version":null,"tagName":"h2"},{"title":"Initialize RateLimiter class‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#initialize-ratelimiter-class","content":" This is the core of this module. The RateLimiter class has the following constructor parameters:  ./app/core/rate_limiter.py class RateLimiter(): def __init__( self, rule: str, exception_message: str = &quot;Too many Request!&quot;, exception_status: int = status.HTTP_429_TOO_MANY_REQUESTS, redis: RedisClient = redis_client, lua_script: str = SLIDING_WINDOW_COUNTER ) -&gt; None: ( self.limit, # count requests in duration time self.duration_in_second ) = retrieve_rule(rule) self.exp_message = exception_message self.exp_status = exception_status if redis_client: self.redis_client = redis self.lua_script = lua_script self.lua_sha = &quot;&quot;   rule (str): This is the parameter that takes the filtering rule for the API. For example, 5/15s .exception_message (str): An optional parameter to customize the error message sent when a request violates the rule. Default, Too many Request!.exception_status (int): An optional parameter to customize the error status code sent when a request violates the rule. Default, 429.redis (RedisClient): An optional parameter to change the Redis instance used.lua_script (str): An optional parameter to change the script running the algorithm on the Redis server. You can find it on Sliding Window Rate Limiting app using ASP.NET (redis.io).  There are some functions and information you might be curious about. Don‚Äôt worry, I will explain them in the next section.  ","version":null,"tagName":"h3"},{"title":"RateLimiter execution function‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#ratelimiter-execution-function","content":" Since we are defining a class, it operates within Dependencies based on the __call__ method.  ./app/core/rate_limiter.py class RateLimiter(): ... async def __call__(self, request: Request) -&gt; Any: if not await self.redis_client.ping(): raise RedisUnavailableException key = self.req_key_builder(request) try: is_valid = await self.check(key) except NoScriptError: self.lua_sha = await self.redis_client.load_script(self.lua_script) is_valid = await self.check(key) if is_valid == 0: return True raise HTTPException(status_code=self.exp_status, detail=self.exp_message)   In this function, it performs 4 steps:  In line 4, ensure the server is connected to Redis. In line 7, create a key based on the incoming request. We will identify incoming requests based on request method, path and client IP. Example: get:127.0.0.1:/api/v1/utils/limiting ./app/core/rate_limiter.py class RateLimiter(): ... @staticmethod def req_key_builder(req: Request, **kwargs): return &quot;:&quot;.join([req.method.lower(), req.client.host, req.url.path]) From line 9-13, call check method with key parameter created in the previous step. If the Lua script hasn't been loaded onto Redis, load it and then call check method again. ./app/core/rate_limiter.py class RateLimiter(): ... async def check(self, key: str, **kwargs): return await self.redis_client.evaluate_sha( self.lua_sha, 1, [key, str(self.duration_in_second), str(self.limit)] ) It will make a call to the Redis server and get the result from executing the Lua script we loaded earlier into Redis. From line 15-17, get the result from the check method and perform the corresponding action.  ","version":null,"tagName":"h3"},{"title":"retrieve_rule function‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#retrieve_rule-function","content":" This function simply takes a valid string and extracts the corresponding values. The desired outcome is to determine the number of requests allowed within a given time frame (in seconds).  ./app/core/rate_limiter.py PATTERN: Final[str] = &quot;(\\d+)\\/((\\d+)(s|m|h))+&quot; def retrieve_rule(rule: str): try: limit = re.search(PATTERN, rule).group(1) duration = re.search(PATTERN, rule).group(3) period = re.search(PATTERN, rule).group(4) limit = int(limit) duration = int(duration) except (re.error, AttributeError, ValueError): raise RetrieveRuleException if limit &lt; 1 or duration &lt; 0: raise LimitRuleException duration_in_s = duration # second if period == &quot;m&quot;: duration_in_s = duration * 60 elif period == &quot;h&quot;: duration_in_s = duration * 60 * 60 return limit, duration_in_s   ","version":null,"tagName":"h3"},{"title":"Why use Lua scripts?‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#why-use-lua-scripts","content":" Basically, the execution of the algorithm will be handled by Redis, which reduces the load on the API Server when there is a large number of incoming requests. Additionally, it is faster than running the algorithm on the API Server.  Lua scripts block everything. You can't run two Lua scripts in parallel. So it ensures atomicity for our operations.  The trick here is that everything needs to happen atomically, we want to be able to trim the set, check its cardinality, add an item to it, and set it's expiration, all without anything changing in the interim.‚Äî By Redis ‚Äî  local current_time = redis.call('TIME') local trim_time = tonumber(current_time[1]) - ARGV[1] redis.call('ZREMRANGEBYSCORE', KEYS[1], 0, trim_time) local request_count = redis.call('ZCARD', KEYS[1]) if request_count &lt; tonumber(ARGV[2]) then redis.call('ZADD', KEYS[1], current_time[1], current_time[1] .. current_time[2]) redis.call('EXPIRE', KEYS[1], ARGV[1]) return 0 end return 1   ","version":null,"tagName":"h3"},{"title":"Testing‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#testing","content":" I created 2 APIs with Rate-limit middleware and 2 test scripts for it. The script will look like this:  ./tests/rate_limiting/test_rate_limit_1.py url = &quot;http://localhost:8000/api/v1/utils/limiting1&quot; payload = &quot;&quot; def send_request(): now = datetime.now() response = requests.request(&quot;GET&quot;, url, data=payload) print(now, response.text) time.sleep(0.5) # On my local machine, the request function take 2 seconds to get a response. # Rule: 5/15s send_request() # R1 success at 2.5s, redis contains [R1] send_request() # R2 success at 5s, redis contains [R1, R2] send_request() # R3 success at 7.5s, redis contains [R1, R2, R3] send_request() # R4 success at 10s, redis contains [R1, R2, R3, R4] send_request() # R5 success at 12.5s, redis contains [R1, R2, R3, R4, R5] send_request() # R6 fails at 15s, redis contains [R1, R2, R3, R4, R5] send_request() # R7 success at 18.5s, redis contains [R2, R3, R4, R5, R7]   Now, let's see the results:    API server logs:    ","version":null,"tagName":"h2"},{"title":"Notes‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#notes","content":" Instead of Redis, you can implement a queue/map in your application to store request values. But consider the scalability of your system‚Äîimplementing in-app queues or maps like this is no longer suitable and can lead to redundancy and resource waste. Therefore, if your application has non-functional requirements for scalability, consider using shared storage solutions like Redis (Redis - The Real-time Data Platform) or Memcached (memcached - a distributed memory object caching system).  Additionally, to implement rate-limiting as middleware in API server, we can also implement it at network layer. This is also an interesting topic, and I hope to share more about it in other posts.  Some packages that you may be interested in:  laurentS/slowapi: A rate limiter for Starlette and FastAPI (github.com) (1.1k stars, 72 forks on 18/08/2024)long2ice/fastapi-limiter: A request rate limiter for fastapi (github.com) (465 stars, 52 forks on 18/08/2024)  ","version":null,"tagName":"h2"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#conclusion","content":" We have explored some aspects of rate-limiting and how to implement it in FastAPI.  I hope this post was useful. If you need a project to run a demo on your environment, here is my¬†Git repository.  See you again!  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting#references","content":" What is rate limiting? | Rate limiting and bots | CloudflareRate limiting best practices ¬∑ Cloudflare Web Application Firewall (WAF) docsHow to implement Sliding Window Rate Limiting app using ASP.NET Core &amp; RedisRedis and Lua Powered Sliding Window Rate Limiter (halodoc.io) ","version":null,"tagName":"h2"},{"title":"Essential modules for developing applications with FastAPI (P2 - Logging)","type":0,"sectionRef":"#","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging","content":"","keywords":"","version":null},{"title":"Framework/Library version‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P2 - Logging)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging#frameworklibrary-version","content":" This project uses Python 3.10 as the environment and Poetry as the package manager.  The code and examples in this post will use frameworks/libraries with the following versions.  ./pyproject.toml [tool.poetry.dependencies] python = &quot;^3.10&quot; uvicorn = {extras = [&quot;standard&quot;], version = &quot;^0.24.0.post1&quot;} fastapi = &quot;^0.109.1&quot; python-multipart = &quot;^0.0.7&quot; email-validator = &quot;^2.1.0.post1&quot; passlib = {extras = [&quot;bcrypt&quot;], version = &quot;^1.7.4&quot;} tenacity = &quot;^8.2.3&quot; pydantic = &quot;&gt;2.0&quot; emails = &quot;^0.6&quot; gunicorn = &quot;^21.2.0&quot; jinja2 = &quot;^3.1.2&quot; alembic = &quot;^1.12.1&quot; python-jose = {extras = [&quot;cryptography&quot;], version = &quot;^3.3.0&quot;} httpx = &quot;^0.25.1&quot; psycopg = {extras = [&quot;binary&quot;], version = &quot;^3.1.13&quot;} sqlmodel = &quot;^0.0.16&quot; # Pin bcrypt until passlib supports the latest bcrypt = &quot;4.0.1&quot; pydantic-settings = &quot;^2.2.1&quot; sentry-sdk = {extras = [&quot;fastapi&quot;], version = &quot;^1.40.6&quot;} psycopg2 = &quot;^2.9.9&quot; asyncpg = &quot;^0.29.0&quot;   ","version":null,"tagName":"h2"},{"title":"Logging‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P2 - Logging)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging#logging","content":" This module plays an important role in recording information, warnings, and errors during execution.  Log messages can provide information about system activities, input and output data, important events, and any issues that arise to aid in locating and fixing errors.  Let's dive deeper into this module.  ","version":null,"tagName":"h2"},{"title":"Log Level‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P2 - Logging)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging#log-level","content":" In Python, log level refers to the importance level of a log message and is used to specify the type of that message.    DEBUG: Log level for debug messages, typically used to record detailed information for debugging and development purposes.INFO: Log level for informational messages, often used to record normal operations in the application.WARNING: Log level for warning messages, usually used to record unexpected or potential situations that do not affect the application's operation.ERROR: Log level for error messages, typically used to record situations that cause errors during application execution.CRITICAL: Log level for critical error messages, commonly used to log fatal errors that may prevent the application from continuing.  In my experience, I often use DEBUG log level in the development environment. When the application is moved to production, I will hide the DEBUG level. Typically, only logs from INFO level or higher are allowed to be printed.  To get more knowledge about this part, you can refer to this https://stackoverflow.com/questions/2031163/when-to-use-the-different-log-levels.  ","version":null,"tagName":"h3"},{"title":"Log Format‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P2 - Logging)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging#log-format","content":" By default, the log format looks very simple. The problem is that this simplicity makes it lack information and become difficult to read.    Let's say you put a log inside a function and it's used multiple times. The special thing is that when the system is operating, you can't always sit next to the screen to check whether that function records errors or not? And at what point does the error occur? Ha ha‚Ä¶ a classic example.  To solve that problem, log formatting is the key solution. It helps you and your system get through the darkest days.  For example, I have different formats for files and consoles.  ./app/core/logger.py console_msg_format = &quot;%(asctime)s %(levelname)s: %(message)s&quot; # Create the root logger. logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Set up logging to the console. stream_handler = logging.StreamHandler() stream_formatter = logging.Formatter(console_msg_format) stream_handler.setFormatter(stream_formatter) logger.addHandler(stream_handler)   The logs will look like    Even though the log format has been changed, it still doesn't seem easy to follow. Next, what we're going to do to improve it is add color to the log. At this point, I offer a simple solution to make the log better.  ./app/core/logger.py class ColoredFormatter(logging.Formatter): COLOR_CODES = { 'DEBUG': '\\033[94m', # blue 'INFO': '\\033[92m', # green 'WARNING': '\\033[93m', # yellow 'ERROR': '\\033[91m', # red 'CRITICAL': '\\033[41m\\033[97m' # red background color and white text } RESET_CODE = '\\033[0m' # called to return to standard terminal text color def format(self, record): # Get the color corresponding to the log level color = self.COLOR_CODES.get(record.levelname, '') # Add color to log messages and reset color at the end formatted_message = f&quot;{color}{super().format(record)}{self.RESET_CODE}&quot; return formatted_message console_msg_format = &quot;%(asctime)s %(levelname)s: %(message)s&quot; # Create the root logger. logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Set up logging to the console. stream_handler = logging.StreamHandler() stream_formatter = ColoredFormatter(console_msg_format) stream_handler.setFormatter(stream_formatter) logger.addHandler(stream_handler)   The result ^_^:    ","version":null,"tagName":"h3"},{"title":"Logging to Multiple Sources‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P2 - Logging)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging#logging-to-multiple-sources","content":" We have customized the log format for printing to the console in the above sections. If we only print to the console, when we restart the application, the logs on the console will be reset... We have lost all the old logs.  So, we need to store logs to multiple sources. As far as I know, we can store locations like files, databases, or online log services.  You can integrate with other tools like Elasticsearch, Logstash, Kibana (ELK Stack).    Or Promtail, Loki and Grafana for simpler setup.    In this post, we will discuss how to log into a file and some considerations to remember regarding this process.  With just a few lines of code, you can log into a specific file anywhere you want.  ./app/core/logger.py # Create the root logger. logger = logging.getLogger() logger.setLevel(logging.DEBUG) logging.basicConfig(level=logging.DEBUG) # Set up logging to the file. file_handler = logging.FileHandler('app.log') # &lt;--- File location file_handler.setLevel(logging.DEBUG) formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s') file_handler.setFormatter(formatter) logger.addHandler(file_handler)   Let‚Äôs see the results.    As you can see, app.log automatically creates and contains all the logs we print. However, that's not enough. We cannot simply dump all the system logs into a single file and monitor the logs from there. It is terrible! (ÔºÉ¬∞–î¬∞)  To avoid that, what we need is a log rotation. It helps us manage file size, the maximum number of backup files if the file size exceeds the threshold.  ./app/core/logger.py # Define default logfile format. file_name_format = &quot;{year:04d}{month:02d}{day:02d}.log&quot; # Define the default logging message formats. file_msg_format = &quot;%(asctime)s %(levelname)-8s: %(message)s&quot; console_msg_format = &quot;%(levelname)s: %(message)s&quot; # Define the log rotation criteria. max_bytes = 1024**2 # ~ 1MB backup_count = 100 # Create the root logger. logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Validate the given directory. dir=&quot;log&quot; dir = os.path.normpath(dir) # Create a folder for the logfiles. if not os.path.exists(dir): os.makedirs(dir) # Construct the name of the logfile. t = datetime.datetime.now() file_name = file_name_format.format( year=t.year, month=t.month, day=t.day, ) file_name = os.path.join(dir, file_name) # Set up logging to the logfile. file_handler = logging.handlers.RotatingFileHandler( filename=file_name, maxBytes=max_bytes, backupCount=backup_count ) file_handler.setLevel(logging.DEBUG) file_formatter = logging.Formatter(file_msg_format) file_handler.setFormatter(file_formatter) logger.addHandler(file_handler)   If you're not familiar with how this module works, the RotatingFileHandler class has the following description:  Rollover occurs whenever the current log file is nearly maxBytes in length. If backupCount is &gt;= 1, the system will successively create new files with the same pathname as the base file, but with extensions &quot;.1&quot;, &quot;.2&quot; etc. appended to it. For example, with a backupCount of 5 and a base file name of &quot;app.log&quot;, you would get &quot;app.log&quot;, &quot;app.log.1&quot;, &quot;app.log.2&quot;, ... through to &quot;app.log.5&quot;. The file being written to is always &quot;app.log&quot; - when it gets filled up, it is closed and renamed to &quot;app.log.1&quot;, and if files &quot;app.log.1&quot;, &quot;app.log.2&quot; etc. exist, then they are renamed to &quot;app.log.2&quot;, &quot;app.log.3&quot; etc. respectively.  If maxBytes is zero, rollover never occurs.  In the example, I set the maxBytes value to 1MB, so if the size of the log file reaches 1MB, it will be moved to a backup file (20240405.log.1, 20240405.log.2), and new logs will continue to be written to the current file (20240405.log).    ","version":null,"tagName":"h3"},{"title":"Performance‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P2 - Logging)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging#performance","content":" Logging in the system may seem lightweight, but in reality, it still consumes time and a certain amount of system resources, especially when performing concurrent logging from multiple threads or processes.  Specifying log levels, optimizing log structure, and configuring rotation also significantly reduce the burden on the system. Additionally, we can consider asynchronous logging and reduce the number of logging operations to reduce the load on the storage system, especially when handling large log volumes.  In FastAPI, you can use Background Tasks to log asynchronously, which reduces system load and request latency.  from typing import Any from fastapi import BackgroundTasks import logging @self.router.get( &quot;/something&quot; ) async def handle_something( bg_task: BackgroundTasks ) -&gt; Any: bg_task.add_task(logging.info, f&quot;This is log&quot;) return { &quot;something&quot;: True }   ","version":null,"tagName":"h3"},{"title":"Integration with FastAPI‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P2 - Logging)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging#integration-with-fastapi","content":" In my experience, you should wrap up all configurations into a function and call it when initializing the FastAPI application. In the new FastAPI version, Lifespan Events will help you do that.  For instance:  ./app/main.py from contextlib import asynccontextmanager from app.core.logger import setup as setup_logging from app.core.config import settings @asynccontextmanager async def lifespan(app: FastAPI): # start up setup_logging(minLevel=logging.DEBUG) yield # shut down pass app = FastAPI( title=settings.PROJECT_NAME, openapi_url=f&quot;{settings.API_STR}{settings.API_VERSION_STR}/openapi.json&quot;, lifespan=lifespan )   ","version":null,"tagName":"h3"},{"title":"To sum up‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P2 - Logging)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging#to-sum-up","content":" We discussed an essential module in developing applications with FastAPI: Logging. By carefully and effectively using this module, we can build and maintain powerful and flexible FastAPI applications. Hopefully, this article has given you the overview and knowledge needed to use this important module in your projects.  If you need a project to run a demo on your environment, here are my¬†Git repository.  See you again!  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P2 - Logging)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging#references","content":" borntyping/python-colorlog: A colored formatter for the python logging module (github.com)acschaefer/duallog: Python package to enable simultaneous logging to console and logfile (github.com) ","version":null,"tagName":"h2"},{"title":"Essential modules for developing applications with FastAPI (P3 - Caching)","type":0,"sectionRef":"#","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching","content":"","keywords":"","version":null},{"title":"Framework/Library version‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P3 - Caching)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching#frameworklibrary-version","content":" This project uses Python 3.10 as the environment and Poetry as the package manager.  The code and examples in this post will use frameworks/libraries with the following versions.  ./pyproject.toml [tool.poetry.dependencies] python = &quot;^3.10&quot; uvicorn = {extras = [&quot;standard&quot;], version = &quot;^0.24.0.post1&quot;} fastapi = &quot;^0.109.1&quot; python-multipart = &quot;^0.0.7&quot; email-validator = &quot;^2.1.0.post1&quot; passlib = {extras = [&quot;bcrypt&quot;], version = &quot;^1.7.4&quot;} tenacity = &quot;^8.2.3&quot; pydantic = &quot;&gt;2.0&quot; emails = &quot;^0.6&quot; gunicorn = &quot;^21.2.0&quot; jinja2 = &quot;^3.1.2&quot; alembic = &quot;^1.12.1&quot; python-jose = {extras = [&quot;cryptography&quot;], version = &quot;^3.3.0&quot;} httpx = &quot;^0.25.1&quot; psycopg = {extras = [&quot;binary&quot;], version = &quot;^3.1.13&quot;} sqlmodel = &quot;^0.0.16&quot; # Pin bcrypt until passlib supports the latest bcrypt = &quot;4.0.1&quot; pydantic-settings = &quot;^2.2.1&quot; sentry-sdk = {extras = [&quot;fastapi&quot;], version = &quot;^1.40.6&quot;} psycopg2 = &quot;^2.9.9&quot; asyncpg = &quot;^0.29.0&quot; redis = {extras = [&quot;hiredis&quot;], version = &quot;^5.0.3&quot;} orjson = &quot;^3.10.0&quot;   ","version":null,"tagName":"h2"},{"title":"Caching‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P3 - Caching)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching#caching","content":" We will explore integrating a caching module into FastAPI. This caching module will automatically store the results of previous API requests, thereby improving response times and reducing server load. There are many other places where you can cache data, such as In-memory, Redis, DynamoDB,‚Ä¶ This section will implement a simple Redis caching solution for APIs.  First, we should install the Redis package.  poetry add redis # pip install redis   ","version":null,"tagName":"h2"},{"title":"Prepare an adapter for Redis‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P3 - Caching)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching#prepare-an-adapter-for-redis","content":" When working with Redis, we must manage connecting and disconnecting to Redis as necessary. Additionally, certain features require additional logic to integrate with the system seamlessly.  So, I define some functions that I want to use such as‚Ä¶  Connect.Disconnect.Add a key to Redis.Check the key exists in Redis.  Let‚Äôs build it.  ./app/core/redis.py from typing import Any, Dict, Tuple import redis.asyncio as aioredis import logging from app.utils import ORJsonCoder class RedisClient: async def connect(self, redis_url: str): self.pool = aioredis.ConnectionPool().from_url(redis_url) self.redis = aioredis.Redis.from_pool(self.pool) if await self.redis.ping(): logging.info(&quot;Redis connected&quot;) return True logging.warning(&quot;Cannot connect to Redis&quot;) return False async def add_to_cache(self, key: str, value: Dict, expire: int) -&gt; bool: response_data = None try: response_data = ORJsonCoder().encode(value) except TypeError: message = f&quot;Object of type {type(value)} is not JSON-serializable&quot; logging.error(message) return False cached = await self.redis.set(name=key, value=response_data, ex=expire) if cached: logging.info(f&quot;{key} added to cache&quot;) else: # pragma: no cover logging.warning(f&quot;Failed to cache key {key}&quot;) return cached async def check_cache(self, key: str) -&gt; Tuple[int, str]: pipe = self.redis.pipeline() ttl, in_cache = await pipe.ttl(key).get(key).execute() if in_cache: logging.info(f&quot;Key {key} found in cache&quot;) return (ttl, in_cache) async def disconnect(self): if await self.redis.ping(): await self.redis.aclose() logging.info(&quot;Redis disconnected&quot;) return None redis_client = RedisClient()   In this class, you might wonder about the ORJsonCoder module. Where is it? What can it do?  This module provides methods to perform JSON encoding and decoding using the orjson library, aimed at increasing speed and performance compared to standard JSON libraries in Python.  ./app/utils/orjson_coder.py from typing import Any, Union from fastapi.encoders import jsonable_encoder import orjson class ORJsonCoder: def encode(cls, value: Any) -&gt; bytes: return orjson.dumps( value, default=jsonable_encoder, option=orjson.OPT_NON_STR_KEYS | orjson.OPT_SERIALIZE_NUMPY, ) def decode(cls, value: Union[bytes | str]) -&gt; Any: return orjson.loads(value)   ","version":null,"tagName":"h3"},{"title":"Implement logic for caching‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P3 - Caching)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching#implement-logic-for-caching","content":" Next, we will create a module to provide caching features based on RedisClient. To clarify what we need, let's explore the concept of Redis a bit.  What is Redis?‚Äã  Redis (REmote¬†DIctionary¬†Server) is an open source, in-memory, NoSQL¬†key/value store that is used primarily as an application cache or quick-response database.‚Äî By IBM ‚Äî  So, to effectively use Redis as a cache, we need an efficient strategy for creating keys and values. Fortunately, Redis now supports various data types such as strings, hashes, lists, sets, sorted sets, and JSON‚Ä¶  Another thing to keep in mind is what to cache and how long to keep it in cache.  What to cache?‚Äã  We don't want to cache many keys that change continuously. We don't want to cache many keys that are requested very rarely. We want to cache keys that are requested often and change at a reasonable rate. For an example of key not changing at a reasonable rate, think of a global counter that is continuously¬†INCRemented.‚Äî By Redis Docs ‚Äî  How long to keep it?‚Äã  This is a question that you need to answer for yourself; everything depends on the logic of your system.  Some systems use Redis to implement a whitelist strategy for their authorization tokens, so the time to keep that key in the cache memory could be the token's lifespan.  They also implement data caching for high-traffic APIs with large response data, where the data returned from these APIs has minimal changes. The time to keep the key in the cache memory could be either the time it takes for a change to occur or just enough time to ensure users aren't stuck with outdated data for too long.  Suppose you have complex computational tasks that are time-consuming or require significant computational resources. In that case, you can use cron jobs (schedule jobs) to precompute the results and store them in the cache to optimize user experience and system resource utilization.  Next, let's take a quick look at this code snippet.  ./app/core/cache.py from typing import Any from urllib.parse import urlencode from fastapi import Request from fastapi.datastructures import QueryParams from app.core.redis import redis_client from app.utils import ORJsonCoder def query_params_builder(params: QueryParams) -&gt; str: sorted_query_params = sorted(params.items()) return urlencode(sorted_query_params, doseq=True) def req_key_builder(req: Request, **kwargs): return &quot;:&quot;.join([ req.method.lower(), req.url.path, query_params_builder(req.query_params) ]) async def add(req: Request, data: Any, expire: int = 60): cached = await redis_client.add_to_cache(req_key_builder(req), data, expire) if not cached: return False return True async def check_exist(req: Request) -&gt; str: key = req_key_builder(req) ttl, in_cache = await redis_client.check_cache(key) return in_cache def load_cache_data(data: str): return ORJsonCoder().decode(data)   query_params_builder: The order of sent query parameters may be inconsistent. Therefore, we need a function to ensure consistency for the query parameters.req_key_builder: This function is used to create a unique key based on the incoming request. The value of the key will look like get:/api/v1/users:limit=20&amp;skip=0.add: As the name suggests, add a key/value pair to Redis with the corresponding parameters.check_exist: Used to check whether the data exists in the cache or not.load_cache_data: Simply used to decode the data retrieved from the cache.  ","version":null,"tagName":"h3"},{"title":"Integration with FastAPI‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P3 - Caching)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching#integration-with-fastapi","content":" After all the preparations are complete, the remaining task is to integrate it into the FastAPI application seamlessly.  First, connect to Redis.  ./app/main.py from fastapi import FastAPI from contextlib import asynccontextmanager from app.core.config import settings from app.core.redis import redis_client @asynccontextmanager async def lifespan(app: FastAPI): # start up await redis_client.connect(str(settings.REDIS_URL)) yield # shut down await redis_client.disconnect() app = FastAPI( title=settings.PROJECT_NAME, openapi_url=f&quot;{settings.API_STR}{settings.API_VERSION_STR}/openapi.json&quot;, lifespan=lifespan )   Second, let's do a simple example. I have an API that returns user information as follows.  ./app/api/user.py @router.get(&quot;&quot;, response_model=List[User]) async def get_pagination_cache( skip: int = Query(0), limit: int = Query(20), session: AsyncSession = Depends(get_async_session) ) -&gt; Any: data = await user.get_pagination(session, skip, limit) return data   Now I will implement caching logic for this API. In FastAPI, you can use Background Tasks to cache asynchronously, which reduces system load and request latency.  ./app/api/user.py @router.get(&quot;&quot;, response_model=List[User]) async def get_pagination_cache( request: Request, bg_tasks: BackgroundTasks, skip: int = Query(0), limit: int = Query(20), session: AsyncSession = Depends(get_async_session) ) -&gt; Any: in_cache = await cache.check_exist(req=request) if in_cache: return cache.load_cache_data(in_cache) data = await user.get_pagination(session, skip, limit) bg_tasks.add_task(cache.add, req=request, data=data, expire=60) return data   When a user calls this API, it will check the corresponding key in Redis. If this key exists, it will return the value of the key without querying the database. If the key does not exist, the system will query the database to retrieve the data as usual. Once the data is obtained, we return it and simultaneously store it in the cache with an expiration time of 60 seconds.  I will insert approximately 20,000 records and use the /users API to retrieve that information. The larger the data, the more noticeable the difference in response time will be.  Let's see what changes now. In the first usage, the API response time falls around 223 ms.    In the second usage, the response time falls around 136 ms.    If you have Redis Insight on your machine, you can connect to Redis and view the values stored within it.    Additionally, you will need to pay attention to response headers such as Cache-Control, ETag, Vary,‚Ä¶  Some packages that you may be interested in:  long2ice/fastapi-cache (github.com) (1.1k stars, 132 forks on 18/04/2024)madkote/fastapi-plugins (github.com) (334 stars, 19 forks on 18/04/2024)comeuplater/fastapi_cache (github.com) (208 stars, 16 forks on 18/04/2024)a-luna/fastapi-redis-cache (github.com) (143 stars, 23 forks on 18/04/2024)mailgun/expiringdict (github.com) (335 stars, 75 forks on 18/04/2024)  ","version":null,"tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P3 - Caching)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching#conclusion","content":" Well, that‚Äôs it. I have discussed some approaches to implementing caching for APIs with FastAPI as well as some related aspects.  I hope this post was useful. If you need a project to run a demo on your environment, here is my¬†Git repository.  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P3 - Caching)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching#references","content":" What is Redis Explained? | IBMClient-side caching in Redis | Docs (redis.io)Understand Redis data types | Docs (redis.io) ","version":null,"tagName":"h2"},{"title":"Zero-downtime Deployments with Docker Compose & Nginx","type":0,"sectionRef":"#","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx","content":"","keywords":"","version":null},{"title":"Introduction‚Äã","type":1,"pageTitle":"Zero-downtime Deployments with Docker Compose & Nginx","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx#introduction","content":" A few months ago, I worked on a project that utilized Docker and Nginx to deploy the product on Digital Ocean‚Äôs VPS. Everything at that time was quite primitive, I had to set up everything from scratch. From containerizing the application to creating a CI/CD pipeline to build, manage, and deploy different Docker image versions.  Docker is a great tool and I love using it in my workflow. I define the Docker services in the configuration file, then pull, up and down the containers to make sure they are up to date. But we have a problem: the time between when I down the container and when I up it. It took¬†2 minutes¬†of downtime in total. That‚Äôs unacceptable for a product deployed for end-users.    So I implemented a Zero downtime deployment strategy for that project. The BLUE-GREEN strategy is a basic deployment process, but it‚Äôs great when simplicity gets the job done.  Now, let‚Äôs talk about some stuff.  ","version":null,"tagName":"h2"},{"title":"Configuration before applying‚Äã","type":1,"pageTitle":"Zero-downtime Deployments with Docker Compose & Nginx","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx#configuration-before-applying","content":" ","version":null,"tagName":"h2"},{"title":"Docker Compose‚Äã","type":1,"pageTitle":"Zero-downtime Deployments with Docker Compose & Nginx","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx#docker-compose","content":" I have a configuration file like this:  ./docker-compose.yml services: api: image: pxuanbach/simple-app ports: - '8000:8000' restart: on-failure   ","version":null,"tagName":"h3"},{"title":"Nginx‚Äã","type":1,"pageTitle":"Zero-downtime Deployments with Docker Compose & Nginx","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx#nginx","content":" The nginx.conf configuration will look like this:  ./nginx.conf server { listen 80 default_server; server_name api.app.com; location / { proxy_pass http://localhost:8000; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $host; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; client_max_body_size 64M; } }   ","version":null,"tagName":"h3"},{"title":"Deployment process‚Äã","type":1,"pageTitle":"Zero-downtime Deployments with Docker Compose & Nginx","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx#deployment-process","content":" It was very easy, and I just followed the steps‚Ä¶  docker compose pull docker compose down # ---DOWNTIME HERE--- docker compose up     Now let's move on to the BLUE-GREEN strategy.  ","version":null,"tagName":"h3"},{"title":"New configuration‚Äã","type":1,"pageTitle":"Zero-downtime Deployments with Docker Compose & Nginx","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx#new-configuration","content":" ","version":null,"tagName":"h2"},{"title":"Docker Compose‚Äã","type":1,"pageTitle":"Zero-downtime Deployments with Docker Compose & Nginx","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx#docker-compose-1","content":" To apply the BLUE-GREEN strategy, I need to update this configuration file a bit. I use the¬†Anchors and aliases¬†features to have a blue and green service with the same configuration. I only change the port number for the green service.  ./docker-compose.yml services: api_blue: &amp;api image: pxuanbach/simple-app ports: - '8000:8000' restart: on-failure api_green: &lt;&lt;: *api ports: - &quot;8001:8000&quot;   ","version":null,"tagName":"h3"},{"title":"Nginx‚Äã","type":1,"pageTitle":"Zero-downtime Deployments with Docker Compose & Nginx","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx#nginx-1","content":" Create a copy of the nginx configuration corresponding to the service name and port. For example¬†api_green.conf:  ./api_green.conf server { listen 80 default_server; server_name api.app.com; location / { proxy_pass http://localhost:8001; ... } }   ","version":null,"tagName":"h3"},{"title":"Zero Downtime Deployment‚Äã","type":1,"pageTitle":"Zero-downtime Deployments with Docker Compose & Nginx","url":"/blog/zero-downtime-deployment-with-docker-compose-nginx#zero-downtime-deployment","content":" To achieve the goal, I must use the Bash/Shell script. This script will make use of the Docker command line as well as the Nginx. Its goal is to implement the BLUE-GREEN strategy by identifying which service, BLUE or GREEN, is currently active and then standing up the inactive environment in parallel. To avoid downtime, I will update the Nginx configuration before stopping the old container.  ./pull.run-service.sh #!/bin/bash # Step 1 BLUE_SERVICE=&quot;api_blue&quot; BLUE_SERVICE_PORT=8000 GREEN_SERVICE=&quot;api_green&quot; GREEN_SERVICE_PORT=8001 TIMEOUT=60 # Timeout in seconds SLEEP_INTERVAL=5 # Time to sleep between retries in seconds MAX_RETRIES=$((TIMEOUT / SLEEP_INTERVAL)) # Step 2 if docker ps --format &quot;{{.Names}}&quot; | grep -q &quot;$BLUE_SERVICE&quot;; then ACTIVE_SERVICE=$BLUE_SERVICE INACTIVE_SERVICE=$GREEN_SERVICE elif docker ps --format &quot;{{.Names}}&quot; | grep -q &quot;$GREEN_SERVICE&quot;; then ACTIVE_SERVICE=$GREEN_SERVICE INACTIVE_SERVICE=$BLUE_SERVICE else ACTIVE_SERVICE=&quot;&quot; INACTIVE_SERVICE=$BLUE_SERVICE fi echo &quot;Starting $INACTIVE_SERVICE container&quot; docker compose pull $INACTIVE_SERVICE docker compose up -d $INACTIVE_SERVICE # Step 3 # Wait for the new environment to become healthy echo &quot;Waiting for $INACTIVE_SERVICE to become healthy...&quot; sleep 10 i=0 while [ &quot;$i&quot; -le $MAX_RETRIES ]; do HEALTH_CHECK_URL=&quot;http://localhost:8000/health&quot; if [ &quot;$INACTIVE_SERVICE&quot; = &quot;$BLUE_SERVICE&quot; ]; then HEALTH_CHECK_URL=&quot;http://localhost:$BLUE_SERVICE_PORT/health&quot; else HEALTH_CHECK_URL=&quot;http://localhost:$GREEN_SERVICE_PORT/health&quot; fi response=$(curl -s -o /dev/null -w &quot;%{http_code}&quot; $HEALTH_CHECK_URL) # Check the HTTP status code if [ $response -eq 200 ]; then echo &quot;$INACTIVE_SERVICE is healthy&quot; break else echo &quot;Health check failed. API returned HTTP status code: $response&quot; fi i=$(( i + 1 )) sleep &quot;$SLEEP_INTERVAL&quot; done # Step 4 # update Nginx config echo &quot;Update Nginx config to $INACTIVE_SERVICE&quot; cp ./$INACTIVE_SERVICE.conf /your/config/path/api.conf # restart nginx nginx -s reload; sleep 5 # Step 5 # remove OLD CONTAINER echo &quot;Remove OLD CONTAINER: $ACTIVE_SERVICE&quot; docker compose rm -fsv $ACTIVE_SERVICE # remove unused images (docker images -q --filter 'dangling=true' -q | xargs docker rmi) || true   Let‚Äôs walk through the script step by step:  I define the name and port of the blue and green services. And the maximum retry time to check the status of the container. The value depends on your container initialization time.Execute the docker command to find the inactive service and start it.Check the status of the newly initialized container.Update Nginx configuration and reload it. Using¬†nginx -s reload¬†to reload Nginx¬†usually does not cause downtime. This is because the command only tells Nginx to reload its configuration, it does not restart the entire process.Clean up some unused stuff (old docker image, old container).  In some cases the command¬†docker compose rm -fsv¬†may not work. Easily change it to:  docker compose stop $ACTIVE_SERVICE docker compose rm -f $ACTIVE_SERVICE   To deploy the new version, simply run the created script.  ./pull.run-service.sh   Conclusion  As you can see, we can automate the deployment process with just 1 Bash script. The primary objective is to redirect the proxy to the newest container and then remove the old one.  If you need a project to run a demo on your environment, here are my Git repository.  References  Zero-Downtime Deployments with Docker Compose ‚Äì Max Countrymandocker compose rm | Docker Docs ","version":null,"tagName":"h3"},{"title":"Tutorial Intro","type":0,"sectionRef":"#","url":"/docs/intro","content":"","keywords":"","version":"Next"},{"title":"Getting Started‚Äã","type":1,"pageTitle":"Tutorial Intro","url":"/docs/intro#getting-started","content":" Get started by creating a new site.  Or try Docusaurus immediately with docusaurus.new.  ","version":"Next","tagName":"h2"},{"title":"What you'll need‚Äã","type":1,"pageTitle":"Tutorial Intro","url":"/docs/intro#what-youll-need","content":" Node.js version 18.0 or above: When installing Node.js, you are recommended to check all checkboxes related to dependencies.  ","version":"Next","tagName":"h3"},{"title":"Generate a new site‚Äã","type":1,"pageTitle":"Tutorial Intro","url":"/docs/intro#generate-a-new-site","content":" Generate a new Docusaurus site using the classic template.  The classic template will automatically be added to your project after you run the command:  npm init docusaurus@latest my-website classic   You can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.  The command also installs all necessary dependencies you need to run Docusaurus.  ","version":"Next","tagName":"h2"},{"title":"Start your site‚Äã","type":1,"pageTitle":"Tutorial Intro","url":"/docs/intro#start-your-site","content":" Run the development server:  cd my-website npm run start   The cd command changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.  The npm run start command builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.  Open docs/intro.md (this page) and edit some lines: the site reloads automatically and displays your changes. ","version":"Next","tagName":"h2"},{"title":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","type":0,"sectionRef":"#","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler","content":"","keywords":"","version":null},{"title":"Framework/Library version‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#frameworklibrary-version","content":" This project uses Python 3.10 as the environment and Poetry as the package manager.  The code and examples in this post will use frameworks/libraries with the following versions.  ./pyproject.toml [tool.poetry.dependencies] python = &quot;^3.10&quot; uvicorn = {extras = [&quot;standard&quot;], version = &quot;^0.24.0.post1&quot;} fastapi = &quot;^0.109.1&quot; python-multipart = &quot;^0.0.7&quot; email-validator = &quot;^2.1.0.post1&quot; passlib = {extras = [&quot;bcrypt&quot;], version = &quot;^1.7.4&quot;} tenacity = &quot;^8.2.3&quot; pydantic = &quot;&gt;2.0&quot; emails = &quot;^0.6&quot; gunicorn = &quot;^21.2.0&quot; jinja2 = &quot;^3.1.2&quot; alembic = &quot;^1.12.1&quot; python-jose = {extras = [&quot;cryptography&quot;], version = &quot;^3.3.0&quot;} httpx = &quot;^0.25.1&quot; psycopg = {extras = [&quot;binary&quot;], version = &quot;^3.1.13&quot;} sqlmodel = &quot;^0.0.16&quot; # Pin bcrypt until passlib supports the latest bcrypt = &quot;4.0.1&quot; pydantic-settings = &quot;^2.2.1&quot; sentry-sdk = {extras = [&quot;fastapi&quot;], version = &quot;^1.40.6&quot;} psycopg2 = &quot;^2.9.9&quot; asyncpg = &quot;^0.29.0&quot; redis = {extras = [&quot;hiredis&quot;], version = &quot;^5.0.3&quot;} orjson = &quot;^3.10.0&quot; apscheduler = &quot;^3.10.4&quot;   ","version":null,"tagName":"h2"},{"title":"Job Scheduler‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#job-scheduler","content":" In the real world, the Job Scheduler module stands out as a fundamental tool for automating tasks and managing scheduled activities within web applications. FastAPI, with its asynchronous capabilities, integrates seamlessly with Job Scheduler modules, allowing efficient automation of recurring tasks.  I will introduce a library that I am using to build this module for my applications. It offers many options, powerful features, and operates flexibly.  It's APScheduler (6k stars, 693 forks on 28/07/2024). If you are wondering why, the answer comes from a ticket in my job, which required adding a feature to schedule certain tasks during the day in the FastAPI application. Then, I wandered through various blogs, forums, and stopped at a discussion in the FastAPI repository. Many libraries were mentioned, and then I stopped here.  This library supports job storage with SQLAlchemy (SQLAlchemyJobStore) and job scheduling with asyncio (AsyncIOScheduler).  This library has three built-in scheduling systems  Cron-style scheduling (with optional start/end times)Interval-based execution (runs jobs on even intervals, with optional start/end times)One-off delayed execution (runs jobs once, on a set date/time)  So, we can organize the module flexibly to take advantage of those benefits.  ","version":null,"tagName":"h2"},{"title":"Install‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#install","content":" First, we should install the APScheduler package.  poetry add apscheduler # pip install apscheduler   ","version":null,"tagName":"h3"},{"title":"Prepare before coding‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#prepare-before-coding","content":" Because APScheduler provides flexibility in adding and removing jobs at runtime, I usually build it as a regular entity in the system. That means there will be APIs performing CRD operations for it (no need for U - Update, because we only need to delete and re-add that job back into the system).  We will have a separate database to store jobs, typically I will use SQLite or PostgreSQL. It's lightweight and compatible with SQLAlchemy (SQLAlchemyJobStore).  In practice, I almost exclusively work with tasks that use cron or interval triggers. I rarely use date. However, for the sake of completeness in this article, I will also write APIs to support it.  ","version":null,"tagName":"h3"},{"title":"Create a Scheduler instance‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#create-a-scheduler-instance","content":" In the first steps, we just need to declare an instance representing the Scheduler.  ./app/core/scheduler.py from apscheduler.schedulers.asyncio import AsyncIOScheduler from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore from app.core.config import settings jobstores = { 'default': SQLAlchemyJobStore(url=settings.JOB_DATABASE_URI) } scheduler = AsyncIOScheduler(jobstores=jobstores)   ","version":null,"tagName":"h3"},{"title":"Integration with FastAPI‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#integration-with-fastapi","content":" We also use the lifespan function to manage the lifecycle of the scheduler instance.  ./app/main.py from fastapi import FastAPI from contextlib import asynccontextmanager from app.core.scheduler import scheduler @asynccontextmanager async def lifespan(app: FastAPI): # start up try: scheduler.start() except Exception as e: logging.error(&quot;Unable to Create Schedule Object - [%s]&quot;, str(e)) yield # shut down scheduler.shutdown() app = FastAPI( title=settings.PROJECT_NAME, lifespan=lifespan )   When running the project, you will see the log printed on the console as follows:    This is just the warm-up part, now let's move on to the main part.  ","version":null,"tagName":"h3"},{"title":"Build Job Creation API‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#build-job-creation-api","content":" First of all, initialize an APIRouter instance and register it in the FastAPI application.  ./app/api/jobs.py import logging from fastapi import APIRouter, HTTPException, Request, BackgroundTasks, status router = APIRouter(prefix=&quot;/jobs&quot;)   ./app/api/__init__.py from fastapi import APIRouter from app.api import user, utils, jobs ... router.include_router(jobs.router, tags=[&quot;Job&quot;])   Now, let's talk about the interesting things about job creation API. As mentioned above, we will create an API to be able to create a new job while the application is running.  Schema/Model‚Äã  The payload schema would look as follows:  ./app/models/job.py from datetime import datetime from typing import Any, List, Literal, Optional, Union from sqlmodel import SQLModel class CronArgs(SQLModel): year: Optional[str] = &quot;*&quot; month: Optional[str] = &quot;*&quot; day: Optional[str] = &quot;*&quot; week: Optional[str] = &quot;*&quot; day_of_week: Optional[str] = &quot;*&quot; hour: Optional[str] = &quot;*&quot; minute: Optional[str] = &quot;*&quot; second: Optional[str] = &quot;5&quot; class IntervalArgs(SQLModel): seconds: Optional[int] = 10 minutes: Optional[int] = None hours: Optional[int] = None days: Optional[int] = None weeks: Optional[int] = None class DateArgs(SQLModel): args: List[Any] = [] run_date: datetime = datetime.now() class JobCreate(SQLModel): job_id: str from_file: bool = True type: Literal['cron', 'interval', 'date'] = 'cron' args: Optional[Union[DateArgs, IntervalArgs, CronArgs]] = None   In this example, the API can create all 3 types of jobs simultaneously, so I designed it as above. However, it might be beneficial to decouple these functionalities for improved maintainability and extensibility.  API Logic‚Äã  Next, let's take a quick look at this code snippet.  ./app/api/jobs.py @router.post(&quot;&quot;, response_model=JobCreateDeleteResponse) async def add_job_to_scheduler(obj_in: JobCreate) -&gt; JobCreateDeleteResponse: # Find job folder job_folder = path.join(&quot;app&quot;, settings.JOB_DIR, obj_in.job_id) if not path.exists(job_folder): raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=&quot;Job folder not found.&quot;) _timers = obj_in.args # Get timer parameters if `.schedule` file exists if obj_in.from_file: _timers = {} _sched_path = path.join(job_folder, &quot;.schedule&quot;) if not path.exists(_sched_path): raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=&quot;Schedule file not found&quot;) # read parameters from `.schedule` file sched = read_file_line_by_line(_sched_path) for i in range(len(sched)): if i == 0 or str(sched[i]).startswith('#') or sched[i] == '' or sched[i] is None: continue _interval_timer = str(sched[i]).split(&quot;=&quot;) _timers.update({_interval_timer[0]: _interval_timer[1]}) # Get cron-job timer parameters if type equals &quot;cron&quot; if obj_in.type == &quot;cron&quot;: _timers = CronArgs.model_validate(_timers) # Get interval-job timer parameters if type equals &quot;interval&quot; elif obj_in.type == &quot;interval&quot;: _timers = IntervalArgs.model_validate(_timers) # Get date-off job timer parameters if type equals &quot;date&quot; elif obj_in.type == &quot;date&quot;: _timers = DateArgs.model_validate(_timers) # find job module in `./app/jobs` folder, # register the `call` function inside the module to the scheduler with timer parameters _job_module = importlib.import_module(f&quot;app.jobs.{obj_in.job_id}.main&quot;) try: job = scheduler.add_job( _job_module.call, obj_in.type, id=obj_in.job_id, **_timers.model_dump(exclude_none=True) ) except ConflictingIdError: logging.warning(f&quot;Job {obj_in.job_id} already exists&quot;) raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=&quot;Job already exists&quot;) except Exception as e: logging.error(f&quot;Add job {obj_in.job_id} - {str(e)}&quot;) raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=&quot;An error occurred&quot;) return JobCreateDeleteResponse(scheduled=True, job_id=job.id)   How does it work?  From line 4-6, to register a job, we must define the job into ./app/jobs folder. The name of the job folder must be equal to the job_id. For example: From line 10-31, I provide 2 options to create a new job. Register with .schedule file, this file will be placed in the job folder.Register via API payload (Schema/Model) that I mentioned above. In line 35, import the module dynamically using job_id via the importlib library. From line 37-42, try to register the job into the scheduler's job store.  We will test it later. Next I will introduce the Get all jobs API.  ","version":null,"tagName":"h3"},{"title":"Get All Jobs API‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#get-all-jobs-api","content":" This API is simpler than Job Creation API.  The code will look like this:  ./app/api/jobs.py @router.get(&quot;&quot;) async def get_scheduled_jobs(): schedules = [] for job in scheduler.get_jobs(): schedules.append({ &quot;job_id&quot;: str(job.id), &quot;run_frequency&quot;: str(job.trigger), &quot;next_run&quot;: str(job.next_run_time) }) return { &quot;total&quot;: len(schedules), &quot;jobs&quot;: schedules }   How does it work?  It uses APScheduler‚Äôs get_jobs API to get all registered jobs.Loop through all jobs to retrieve the necessary data.Returns total number of registered jobs and job information.  ","version":null,"tagName":"h3"},{"title":"Delete Job API‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#delete-job-api","content":" Using the job_id as a unique identifier, we will invoke the APScheduler‚Äôs remove_job API to delete the corresponding job from the job store.  ./app/api/jobs.py @router.delete(&quot;/{job_id}&quot;, response_model=JobCreateDeleteResponse) async def remove_job_from_scheduler(job_id: str) -&gt; JobCreateDeleteResponse: try: scheduler.remove_job(job_id) except Exception as e: logging.error(f&quot;Delete job {job_id} - {str(e)}&quot;) raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, mdetail=&quot;Job deleted failed&quot;) return JobCreateDeleteResponse(scheduled=False, job_id=job_id)   ","version":null,"tagName":"h3"},{"title":"Let's see the results‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#lets-see-the-results","content":" I created some scripts in ./tests folder for testing.  For instance, a script will look like:  ./tests/job_scheduler/get_list_jobs.py import requests url = &quot;http://localhost:8000/api/v1/jobs&quot; payload = &quot;&quot; headers = { &quot;Content-Type&quot;: &quot;application/json&quot;, } response = requests.request(&quot;GET&quot;, url, data=payload, headers=headers) print(response.text)   Let's see how our job scheduler looks like.    First, I create a cron-job and look at the logs. As you can see, the job has been successfully registered in the job store and runs every 10 seconds.  Now, delete it using the delete_job.py script and create an interval job.    Wow, it‚Äôs work. Then test the Get all jobs API using get_list_jobs.py script. We can see it prints to the console that we have a total of 1 job in the job store.  ","version":null,"tagName":"h2"},{"title":"Notes‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#notes","content":" In addition to the benefits it brings, we also need to pay attention to a few other things:  The use of an in-app scheduler can lead to higher resource consumption by the application, and it generates a larger amount of logs, which can make error tracking more challenging. To address this issue, you can separate it into a dedicated application specifically for executing scheduled jobs. Your main application can call the job scheduler app via HTTP, GRPC, etc., to register jobs into the job store.Beyond AsyncIOScheduler, APScheduler offers a variety of job schedulers and storage options. Explore the documentation to select the optimal tool for your system. https://apscheduler.readthedocs.io/en/3.x/userguide.html#choosing-the-right-scheduler-job-store-s-executor-s-and-trigger-sTo avoid your scheduler consuming a lot of database connections, try to limit the number of concurrent executions. https://apscheduler.readthedocs.io/en/3.x/userguide.html#limiting-the-number-of-concurrently-executing-instances-of-a-job  Some packages that you may be interested in:  celery/celery: Distributed Task Queue (development branch) (github.com) (24.2k stars, 4.6k forks on 28/07/2024)samuelcolvin/arq (github.com) (2k stars, 170 forks on 28/07/2024) - The author is the creator of Pydantic, an important module integrated with FastAPI.dmontagu/fastapi-utils (github.com) (1.8k stars, 163 forks on 28/07/2024)aio-libs/aiojobs (github.com) (821 stars, 66 forks on 28/07/2024)madkote/fastapi-plugins (github.com) (355 stars, 19 forks on 28/07/2024)  ","version":null,"tagName":"h2"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#conclusion","content":" I hope this post was useful. If you need a project to run a demo on your environment, here is my¬†Git repository.  Have a great weekend!  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","url":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler#references","content":" https://apscheduler.readthedocs.io/en/3.x/userguide.htmlhow can i run scheduling tasks using fastapi's ¬∑ tiangolo/fastapi ¬∑ Discussion #9143 (github.com) ","version":null,"tagName":"h2"}],"options":{"id":"default"}}