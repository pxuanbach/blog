"use strict";(self.webpackChunkme=self.webpackChunkme||[]).push([[2074],{5754:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"happy-new-year-2025","metadata":{"permalink":"/blog/happy-new-year-2025","source":"@site/blog/12_happy-new-year-2025/index.md","title":"Happy New Year 2025","description":"Hi everyone, I hope you and your family are all doing well! \ud83c\udf86\ud83c\udf86\ud83c\udf86","date":"2025-01-29T10:00:00.000Z","tags":[{"label":"hello world","permalink":"/blog/tags/hello-world"}],"readingTime":0.98,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"happy-new-year-2025","title":"Happy New Year 2025","authors":["pxuanbach"],"tags":["hello world"],"date":"2025-01-29T10:00","image":"/img/12_happy-new-year-2025/featured.png"},"unlisted":false,"nextItem":{"title":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring"}},"content":"Hi everyone, I hope you and your family are all doing well! \ud83c\udf86\ud83c\udf86\ud83c\udf86\\r\\n\\r\\nA new year has arrived, many goals from the previous year remain unfinished. But don\'t worry, we can carry them into this year and accomplish them now.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nSometimes, I can not complete everything on my to-do list. However, I have made progress on each task, and that is still growth.\\r\\n\\r\\n**Just chill and get better every day.**\\r\\n\\r\\nRecently, I have been stuck with some study projects in my graduate school. \\r\\n\\r\\nIt is a big change \ud83e\udd72 when I was working, I could just type a few keywords into Google and find some documents to solve my problems. But in an academic environment, the information is not just in technical documents; it is also in published papers and their citations. \\r\\n\\r\\nI have spent so much time just trying to understand half of a paper! LOL \ud83d\ude02\\r\\n\\r\\nTo be honest, after work and studying, I feel a bit lazy. So I have not been writing new posts lately, even though I have a lot of ideas for them. \\r\\n\\r\\nI think I should add a new task to my to-do list: *writing new blog posts*."},{"id":"essential-modules-for-developing-applications-with-fastapi-p6-monitoring","metadata":{"permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring","source":"@site/blog/10_essential-modules-for-developing-applications-with-fastapi-p6/index.md","title":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","description":"Hello, FastAPI and I are here to see you again. Over the past week, I started onboarding at a new company, joined a new environment, and met new people, which made me really excited.","date":"2024-09-02T10:00:00.000Z","tags":[{"label":"fastapi","permalink":"/blog/tags/fastapi"},{"label":"python","permalink":"/blog/tags/python"},{"label":"monitoring","permalink":"/blog/tags/monitoring"},{"label":"grafana","permalink":"/blog/tags/grafana"},{"label":"prometheus","permalink":"/blog/tags/prometheus"},{"label":"essential-modules-fastapi","permalink":"/blog/tags/essential-modules-fastapi"}],"readingTime":4.98,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"essential-modules-for-developing-applications-with-fastapi-p6-monitoring","title":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","authors":["pxuanbach"],"tags":["fastapi","python","monitoring","grafana","prometheus","essential-modules-fastapi"],"date":"2024-09-02T10:00","image":"/img/10_essential-modules-for-developing-applications-with-fastapi-p6/featured.png"},"unlisted":false,"prevItem":{"title":"Happy New Year 2025","permalink":"/blog/happy-new-year-2025"},"nextItem":{"title":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting"}},"content":"Hello, FastAPI and I are here to see you again. Over the past week, I started onboarding at a new company, joined a new environment, and met new people, which made me really excited.\\r\\n\\r\\nGetting straight to today\'s main topic, I will introduce a module that integrates with FastAPI, which helps us track the statistics of each API in the application.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## **Framework/Library version**\\r\\n\\r\\nThis project uses [Python](https://www.python.org/) 3.10 as the environment and [Poetry](https://python-poetry.org/) as the package manager.\\r\\n\\r\\nThe code and examples in this post will use frameworks/libraries with the following versions.\\r\\n\\r\\n```toml\\r\\n[tool.poetry.dependencies]\\r\\npython = \\"^3.10\\"\\r\\nuvicorn = {extras = [\\"standard\\"], version = \\"^0.24.0.post1\\"}\\r\\nfastapi = \\"^0.109.1\\"\\r\\npython-multipart = \\"^0.0.7\\"\\r\\nemail-validator = \\"^2.1.0.post1\\"\\r\\npasslib = {extras = [\\"bcrypt\\"], version = \\"^1.7.4\\"}\\r\\ntenacity = \\"^8.2.3\\"\\r\\npydantic = \\">2.0\\"\\r\\nemails = \\"^0.6\\"\\r\\ngunicorn = \\"^21.2.0\\"\\r\\njinja2 = \\"^3.1.2\\"\\r\\nalembic = \\"^1.12.1\\"\\r\\npython-jose = {extras = [\\"cryptography\\"], version = \\"^3.3.0\\"}\\r\\nhttpx = \\"^0.25.1\\"\\r\\npsycopg = {extras = [\\"binary\\"], version = \\"^3.1.13\\"}\\r\\n\\r\\nsqlmodel = \\"^0.0.16\\"\\r\\n\\r\\n# Pin bcrypt until passlib supports the latest\\r\\nbcrypt = \\"4.0.1\\"\\r\\npydantic-settings = \\"^2.2.1\\"\\r\\nsentry-sdk = {extras = [\\"fastapi\\"], version = \\"^1.40.6\\"}\\r\\npsycopg2 = \\"^2.9.9\\"\\r\\nasyncpg = \\"^0.29.0\\"\\r\\nredis = {extras = [\\"hiredis\\"], version = \\"^5.0.3\\"}\\r\\norjson = \\"^3.10.0\\"\\r\\napscheduler = \\"^3.10.4\\"\\r\\nprometheus-fastapi-instrumentator = \\"^7.0.0\\"\\r\\n```\\r\\n\\r\\n## Monitoring\\r\\n\\r\\nI am confident that this is one of the most important modules when you want to deploy a product to a production environment.\\r\\n\\r\\nCurrently, many packages can help you monitor your FastAPI app. The package I like to use the most is [Prometheus FastAPI Instrumentator](https://github.com/trallnag/prometheus-fastapi-instrumentator) (919 stars, 83 forks on 03/09/2024). I often use it along with Prometheus and Grafana, creating visual dashboards from those metrics helps me easily monitor the application. I will discuss this stack in another article.\\r\\n\\r\\n![fastapi-instrumentator-prometheus.jpg](./fastapi-instrumentator-prometheus.jpg)\\r\\n\\r\\nHow it\u2019s work?\\r\\n\\r\\n1. The API server exports an endpoint `/metrics`, which tracks the HTTP request count, request/response size in bytes, request duration, and more.\\r\\n2. We register the `/metrics` endpoint and set the crawl job duration for the Prometheus service. Next, this service automatically ingests metrics data from the API server, storing the collected data for analysis.\\r\\n3. Grafana acts as an admin dashboard, retrieving data from Prometheus and visualizing it in specialized graphs, such as time-series, line charts.\\r\\n\\r\\nLet\'s integrate this package with FastAPI and deploy Prometheus and Grafana to visualize its metrics.\\r\\n\\r\\n### Integrating with FastAPI\\r\\n\\r\\nWe need to initialize an instance of the instrumentator and export it when the application starts.\\r\\n\\r\\n```python {3,8,19-23} showLineNumbers title=\\"./app/main.py\\"\\r\\nfrom fastapi import FastAPI\\r\\nfrom contextlib import asynccontextmanager\\r\\nfrom prometheus_fastapi_instrumentator import Instrumentator\\r\\n\\r\\n@asynccontextmanager\\r\\nasync def lifespan(app: FastAPI):\\r\\n    # start up\\r\\n    instrumentator.expose(app)\\r\\n    yield\\r\\n    # shut down\\r\\n    pass\\r\\n\\r\\napp = FastAPI(\\r\\n    title=settings.PROJECT_NAME,\\r\\n    openapi_url=f\\"{settings.API_STR}{settings.API_VERSION_STR}/openapi.json\\",\\r\\n    generate_unique_id_function=custom_generate_unique_id,\\r\\n    lifespan=lifespan\\r\\n)\\r\\ninstrumentator = Instrumentator(\\r\\n    should_group_status_codes=False,\\r\\n    should_ignore_untemplated=True,\\r\\n    excluded_handlers=[\\".*admin.*\\", \\"/metrics\\"],\\r\\n).instrument(app)\\r\\n```\\r\\n\\r\\nAfter initializing and running the application, you can easily find monitoring metrics at the `/metrics` endpoint.\\r\\n\\r\\n![metrics-api-result.png](./metrics-api-result.png#center)\\r\\n\\r\\n### Setting up Prometheus and Grafana services\\r\\n\\r\\nTo quickly see the results, I recommend that you simply copy and paste as instructed in this section. \\r\\n\\r\\nFirst, create a `docker-compose.yaml` file like this:\\r\\n\\r\\n```yaml showLineNumbers title=\\"./monitoring/docker-compose.yaml\\"\\r\\nservices:\\r\\n  prometheus:\\r\\n    image: prom/prometheus:v2.54.0\\r\\n    volumes:\\r\\n      - ./prometheus.yaml:/etc/prometheus/prometheus.yml\\r\\n      - prometheus-data:/prometheus\\r\\n    command:\\r\\n      - \'--config.file=/etc/prometheus/prometheus.yml\'\\r\\n    restart: unless-stopped\\r\\n\\r\\n  grafana:\\r\\n    image: grafana/grafana:10.4.7\\r\\n    ports:\\r\\n      - \\"3001:3000\\"\\r\\n    volumes:\\r\\n      - grafana-data:/var/lib/grafana\\r\\n\\r\\nvolumes:\\r\\n  prometheus-data:\\r\\n  grafana-data:\\r\\n```\\r\\n\\r\\nNext, create a `prometheus.yaml` file in the same location.\\r\\n\\r\\n```yaml showLineNumbers {10} title=\\"./monitoring/prometheus.yaml\\"\\r\\nglobal:\\r\\n  scrape_interval: 15s\\r\\n\\r\\nscrape_configs:\\r\\n  - job_name: fastapi\\r\\n    scrape_interval: 15s\\r\\n    scrape_timeout: 10s\\r\\n    metrics_path: \'/metrics\'\\r\\n    static_configs:\\r\\n    - targets: [\'host.docker.internal:8000\']\\r\\n```\\r\\n\\r\\nYou might be curious about the `host.docker.internal` address. What is it?\\r\\n\\r\\n- The reason for this is that Docker network works differently on Linux, Windows, and macOS.\\r\\n- On Linux, Docker uses the system\'s built-in networking features.\\r\\n- But on Windows and macOS, Docker runs inside a virtual machine. Because of this, Windows and macOS need a special way for containers to talk to the host machine, which is why we use `host.docker.internal`.\\r\\n\\r\\nSince my OS is Windows, I\'m using it as the host for the Prometheus service so that it can call the FastAPI server, which is not containerized.\\r\\n\\r\\n![connect-to-service-outside-docker-deamon](./connect-to-service-outside-docker-deamon.png#center)\\r\\n\\r\\nNow, docker compose up!\\r\\n\\r\\n```bash\\r\\ndocker compose -f ./monitoring/docker-compose.yaml up -d\\r\\n```\\r\\n\\r\\n![docker-compose-up-prometheus-grafana](./docker-compose-up-prometheus-grafana.png#center)\\r\\n\\r\\n### Configuring Grafana Dashboard\\r\\n\\r\\nOnce the services are up and running, you will see logs for requests to the /metrics endpoint being generated every 15 seconds (the `scrape_interval` of Prometheus).\\r\\n\\r\\n![prometheus-crawl-metrics-api-logs](./prometheus-crawl-metrics-api-logs.png#center)\\r\\n\\r\\nNext, open your browser and go to `127.0.0.1:3001`; the Grafana login page will appear. The default username and password are **admin**.\\r\\n\\r\\nAfter logging in, add Prometheus as a Data source with the connection URL set to `http://prometheus:9090`.\\r\\n\\r\\nWhy is `prometheus:9090`? \\r\\n\\r\\n- Containers with a shared Docker Compose configuration can communicate with each other using the service name as the domain.\\r\\n- 9090 is the default port of the Prometheus service.\\r\\n\\r\\n![save-and-test-prometheus-successfully](./save-and-test-prometheus-successfully.png#center)\\r\\n\\r\\nBack to Home, click \u201cCreate your first dashboard\u201d > \\"Import a dashboard\u201d. You will see the following screen.\\r\\n\\r\\n![import-grafana-dashboard](./import-grafana-dashboard.png#center)\\r\\n\\r\\nYou can easily find the [`dashboard.json`](https://github.com/pxuanbach/fastapi-essential-modules/blob/module/monitoring/monitoring/dashboard.json) file in my github repository. Import that file and see the result.\\r\\n\\r\\n![grafana-fastapi-dashboard](./grafana-fastapi-dashboard.png#center)\\r\\n\\r\\nNiceee!!!\\r\\n\\r\\n## Notes\\r\\n\\r\\nThese are not all the statistics that need to be tracked. In the real world, we need to track many more statistics, such as logs, RAM usage, CPU usage, total connections used, etc. However, for the scope of this article, this is enough to show you how we can monitor a FastAPI application.\\r\\n\\r\\nAdditionally, I would like to mention the use of packages that help monitor the application. These packages also consume resources to monitor the system, and they can become a performance bottleneck for the application. Always use it carefully, so you don\'t get into a mess.\\r\\n\\r\\nFurthermore, you can also refer to a few other libraries such as\u2026\\r\\n\\r\\n- [stephenhillier/starlette_exporter](https://github.com/stephenhillier/starlette_exporter) (310 stars, 35 forks on 03/09/2024)\\r\\n- [perdy/starlette-prometheus](https://github.com/perdy/starlette-prometheus) (272 stars, 31 forks on 03/09/2024)\\r\\n- [acidjunk/starlette-opentracing](https://github.com/acidjunk/starlette-opentracing) (66 stars, 6 forks on 03/09/2024)\\r\\n- [open-telemetry/opentelemetry-instrumentation-fastapi](https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation/opentelemetry-instrumentation-fastapi) (###)\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nI hope this post was useful. If you need a project to run a demo on your environment, here is my\xa0[Git repository](https://github.com/pxuanbach/fastapi-essential-modules/tree/module/monitoring).\\r\\n\\r\\nSee you again!\\r\\n\\r\\n## References\\r\\n\\r\\n- [Kludex/fastapi-prometheus-grafana: FasAPI + Prometheus + Grafana! :tada: (github.com)](https://github.com/Kludex/fastapi-prometheus-grafana?tab=readme-ov-file)"},{"id":"essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting","metadata":{"permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting","source":"@site/blog/09_essential-modules-for-developing-applications-with-fastapi-p5/index.md","title":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","description":"Another week has passed, how was your weekend? I am planning my future journey.","date":"2024-08-15T10:00:00.000Z","tags":[{"label":"fastapi","permalink":"/blog/tags/fastapi"},{"label":"redis","permalink":"/blog/tags/redis"},{"label":"rate-limit","permalink":"/blog/tags/rate-limit"},{"label":"python","permalink":"/blog/tags/python"},{"label":"essential-modules-fastapi","permalink":"/blog/tags/essential-modules-fastapi"}],"readingTime":9.74,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting","title":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","authors":["pxuanbach"],"tags":["fastapi","redis","rate-limit","python","essential-modules-fastapi"],"date":"2024-08-15T10:00","image":"/img/09_essential-modules-for-developing-applications-with-fastapi-p5/featured.png"},"unlisted":false,"prevItem":{"title":"Essential modules for developing applications with FastAPI (P6 - Monitoring)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p6-monitoring"},"nextItem":{"title":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler"}},"content":"Another week has passed, how was your weekend? I am planning my future journey.\\r\\n\\r\\nIn this article, I will introduce you to a crucial module to protect your system. As the title of the article, it is about Rate-limiting. Let\'s explore how we can apply it to FastAPI!\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## **Framework/Library version**\\r\\n\\r\\nThis project uses [Python](https://www.python.org/) 3.10 as the environment and [Poetry](https://python-poetry.org/) as the package manager.\\r\\n\\r\\nThe code and examples in this post will use frameworks/libraries with the following versions.\\r\\n\\r\\n```toml\\r\\n[tool.poetry.dependencies]\\r\\npython = \\"^3.10\\"\\r\\nuvicorn = {extras = [\\"standard\\"], version = \\"^0.24.0.post1\\"}\\r\\nfastapi = \\"^0.109.1\\"\\r\\npython-multipart = \\"^0.0.7\\"\\r\\nemail-validator = \\"^2.1.0.post1\\"\\r\\npasslib = {extras = [\\"bcrypt\\"], version = \\"^1.7.4\\"}\\r\\ntenacity = \\"^8.2.3\\"\\r\\npydantic = \\">2.0\\"\\r\\nemails = \\"^0.6\\"\\r\\ngunicorn = \\"^21.2.0\\"\\r\\njinja2 = \\"^3.1.2\\"\\r\\nalembic = \\"^1.12.1\\"\\r\\npython-jose = {extras = [\\"cryptography\\"], version = \\"^3.3.0\\"}\\r\\nhttpx = \\"^0.25.1\\"\\r\\npsycopg = {extras = [\\"binary\\"], version = \\"^3.1.13\\"}\\r\\n\\r\\nsqlmodel = \\"^0.0.16\\"\\r\\n\\r\\n# Pin bcrypt until passlib supports the latest\\r\\nbcrypt = \\"4.0.1\\"\\r\\npydantic-settings = \\"^2.2.1\\"\\r\\nsentry-sdk = {extras = [\\"fastapi\\"], version = \\"^1.40.6\\"}\\r\\npsycopg2 = \\"^2.9.9\\"\\r\\nasyncpg = \\"^0.29.0\\"\\r\\nredis = {extras = [\\"hiredis\\"], version = \\"^5.0.3\\"}\\r\\norjson = \\"^3.10.0\\"\\r\\napscheduler = \\"^3.10.4\\"\\r\\n```\\r\\n\\r\\n## Rate-limiting Overview\\r\\n\\r\\n### What exactly is rate-limiting?\\r\\n\\r\\n> Rate limiting is a strategy for limiting network traffic. It puts a cap on how often someone can repeat an action within a certain timeframe \u2013 for instance, trying to log in to an account. Rate limiting can help stop certain kinds of malicious\xa0[bot activity](https://www.cloudflare.com/learning/bots/what-is-a-bot/). It can also reduce strain on web servers.\\r\\n> [*\u2014 By Cloud Flare \u2014*](https://www.cloudflare.com/learning/bots/what-is-rate-limiting/)\\r\\n\\r\\nBesides, it also offers other benefits, such as monitoring API usage, enforcing API usage policies, and even implementing some business logic to differentiate customer permission levels.\\r\\n\\r\\nThere are many types of rate-limiting to meet various needs, including Fixed Window Counter, Sliding Window Counter, Token Bucket, and more. Additionally, rule limiting is also a topic of concern. I will introduce Fixed Window Counter rate-limiting in this article with an IP-based limiting mechanism.\\r\\n\\r\\n### How does it work?\\r\\n\\r\\nI\'ll give a few simple examples to describe how Rate-limit works; take a look at the image below:\\r\\n\\r\\n![How It Work](./how-it-work.png#center)\\r\\n\\r\\nSuppose we have a Client sending requests to a Server, and we set a rule on the Server to only accept 2 requests within 1 minute. Therefore, if more than 2 requests are sent from the Client, starting from the 3rd request, an error will occur and it will be responded to immediately.\\r\\n\\r\\nPerhaps you are curious as to why the 1-minute time frame is not calculated from point **(1)** but rather from point **(2)**. To understand this, you\'ll need to learn about Rate-Limit algorithms. If the time were calculated from point **(1)** and I could successfully make 2 requests after 1 minute, that would be the Fixed Window Counter algorithm. I calculate the time from point **(2)** because I am applying the Sliding Window Counter algorithm.\\r\\n\\r\\n**Fixed Window Counter algorithm** can be described as follows:\\r\\n\\r\\n![Fixed Window Counter Example](./fixed-window-counter-example.png#center)\\r\\n\\r\\n- We still use the 2 requests per minute rule.\\r\\n- **R1** and **R2** belong to Frame 1, and both responded successfully. At this point, Server will count **R1** and **R2**, with count = 2.\\r\\n- **R3** failed because Frame 1 reached limit (count = 2 = limit).\\r\\n- **R5** and **R6** belong to Frame 2, and do the same with **R1**, **R2**.\\r\\n\\r\\n**Sliding Window Counter algorithm** can be described as follows:\\r\\n\\r\\n![Sliding Window Counter Example](./sliding-window-counter-example.png#center)\\r\\n\\r\\n- **R1** was successful because Frame 1 had no previous requests. We store **R1** with time = 60.\\r\\n- **R2** also executes successfully. Because in Frame 2, there is only 1 request, which is **R1** (count = 1). At this stage, we adds **R2** with time = 75 to cache.\\r\\n- **R3** failed because Frame 3 reached limit. We will check how many requests have been stored from around 30 to 90. Oh, we have **R1** and **R2** with corresponding times of 60 and 75.\\r\\n- **R4** was successful because Frame 4 only contains **R2** (count = 1). At this point, we will cache **R2** and **R4**.\\r\\n\\r\\n### Prepare before coding\\r\\n\\r\\nIn [part 3 - caching](../05_essential-modules-for-developing-applications-with-fastapi-p3/index.md), we integrated Redis into our system. So I\'m going to reuse it to store some stuff.\\r\\n\\r\\nNow, let\'s find out what we are going to do.\\r\\n\\r\\nIn the API Server, I usually create a rate-limit module and treat it as middleware. First, the request will pass through the Rate-limit middleware, which will check whether this request is allowed to proceed to the next middleware or the execution layer.\\r\\n\\r\\n![Request to Rate-limit Middleware](./request-to-rate-limit-middleware.png#center)\\r\\n\\r\\nWhen working with FastAPI, we have many ways to implement this module. I can list a few methods as follows:\\r\\n\\r\\n1. Take advantage of the Dependencies feature in FastAPI.\\r\\n2. Use Python\'s decorator feature.\\r\\n3. Build a middleware in the FastAPI application, similar in concept to Global Dependencies.\\r\\n\\r\\nIn this article, I choose option 1 for simplicity and efficiency. \\r\\n\\r\\n> \u201cSimplicity is the soul of efficiency.\u201d \ud83d\ude01\\r\\n> \\r\\n\\r\\nNext, I will create a rate-limit class. It will be responsible for connecting to Redis and executing the algorithm to filter requests. Additionally, we\'ll need some helper functions to process the input data. Now, let\'s start coding!\\r\\n\\r\\n## Implement Rate-limit module\\r\\n\\r\\n### Initialize RateLimiter class\\r\\n\\r\\nThis is the core of this module. The RateLimiter class has the following constructor parameters:\\r\\n\\r\\n```python {4-8} showLineNumbers title=\\"./app/core/rate_limiter.py\\"\\r\\nclass RateLimiter():\\r\\n    def __init__(\\r\\n        self,\\r\\n        rule: str,\\r\\n        exception_message: str = \\"Too many Request!\\",\\r\\n        exception_status: int = status.HTTP_429_TOO_MANY_REQUESTS,\\r\\n        redis: RedisClient = redis_client,\\r\\n        lua_script: str = SLIDING_WINDOW_COUNTER\\r\\n    ) -> None:\\r\\n        (\\r\\n            self.limit,  # count requests in duration time\\r\\n            self.duration_in_second\\r\\n        ) = retrieve_rule(rule)\\r\\n        self.exp_message = exception_message\\r\\n        self.exp_status = exception_status\\r\\n        if redis_client:\\r\\n            self.redis_client = redis\\r\\n        self.lua_script = lua_script\\r\\n        self.lua_sha = \\"\\"\\r\\n```\\r\\n\\r\\n- rule (str): This is the parameter that takes the filtering rule for the API. For example, `5/15s` .\\r\\n- exception_message (str): An optional parameter to customize the error message sent when a request violates the rule. Default, `Too many Request!`.\\r\\n- exception_status (int): An optional parameter to customize the error status code sent when a request violates the rule. Default, `429`.\\r\\n- redis (RedisClient): An optional parameter to change the Redis instance used.\\r\\n- lua_script (str): An optional parameter to change the script running the algorithm on the Redis server. You can find it on [Sliding Window Rate Limiting app using ASP.NET (redis.io)](https://redis.io/learn/develop/dotnet/aspnetcore/rate-limiting/sliding-window#sliding-window-rate-limiter-lua-script).\\r\\n\\r\\nThere are some functions and information you might be curious about. Don\u2019t worry, I will explain them in the next section.\\r\\n\\r\\n### RateLimiter execution function\\r\\n\\r\\nSince we are defining a class, it operates within Dependencies based on the `__call__` method.\\r\\n\\r\\n```python {4,7,9-13,15-17} showLineNumbers title=\\"./app/core/rate_limiter.py\\"\\r\\nclass RateLimiter():\\r\\n    ...\\r\\n    async def __call__(self, request: Request) -> Any:\\r\\n        if not await self.redis_client.ping():\\r\\n            raise RedisUnavailableException\\r\\n\\r\\n        key = self.req_key_builder(request)\\r\\n\\r\\n        try:\\r\\n            is_valid = await self.check(key)\\r\\n        except NoScriptError:\\r\\n            self.lua_sha = await self.redis_client.load_script(self.lua_script)\\r\\n            is_valid = await self.check(key)\\r\\n\\r\\n        if is_valid == 0:\\r\\n            return True\\r\\n        raise HTTPException(status_code=self.exp_status, detail=self.exp_message)\\r\\n```\\r\\n\\r\\nIn this function, it performs 4 steps:\\r\\n\\r\\n1. In line **4**, ensure the server is connected to Redis.\\r\\n2. In line **7**, create a key based on the incoming request. We will identify incoming requests based on request method, path and client IP. Example: `get:127.0.0.1:/api/v1/utils/limiting`\\r\\n    \\r\\n    ```python showLineNumbers title=\\"./app/core/rate_limiter.py\\"\\r\\n    class RateLimiter():\\r\\n        ...\\r\\n        @staticmethod\\r\\n        def req_key_builder(req: Request, **kwargs):\\r\\n            return \\":\\".join([req.method.lower(), req.client.host, req.url.path])\\r\\n    ```\\r\\n    \\r\\n3. From line **9-13**, call `check` method with key parameter created in the previous step. If the Lua script hasn\'t been loaded onto Redis, load it and then call `check` method again.\\r\\n    \\r\\n    ```python showLineNumbers title=\\"./app/core/rate_limiter.py\\"\\r\\n    class RateLimiter():\\r\\n        ...\\t\\r\\n        async def check(self, key: str, **kwargs):\\r\\n            return await self.redis_client.evaluate_sha(\\r\\n                self.lua_sha, 1, [key, str(self.duration_in_second), str(self.limit)]\\r\\n            )\\r\\n    ```\\r\\n    \\r\\n    It will make a call to the Redis server and get the result from executing the Lua script we loaded earlier into Redis.\\r\\n    \\r\\n4. From line **15-17**, get the result from the `check` method and perform the corresponding action.\\r\\n\\r\\n### retrieve_rule function\\r\\n\\r\\nThis function simply takes a valid string and extracts the corresponding values. The desired outcome is to determine the number of requests allowed within a given time frame (in seconds).\\r\\n\\r\\n```python showLineNumbers title=\\"./app/core/rate_limiter.py\\"\\r\\nPATTERN: Final[str] = \\"(\\\\d+)\\\\/((\\\\d+)(s|m|h))+\\"\\r\\n\\r\\ndef retrieve_rule(rule: str):\\r\\n    try:\\r\\n        limit = re.search(PATTERN, rule).group(1)\\r\\n        duration = re.search(PATTERN, rule).group(3)\\r\\n        period = re.search(PATTERN, rule).group(4)\\r\\n        limit = int(limit)\\r\\n        duration = int(duration)\\r\\n    except (re.error, AttributeError, ValueError):\\r\\n        raise RetrieveRuleException\\r\\n    \\r\\n    if limit < 1 or duration < 0:\\r\\n        raise LimitRuleException\\r\\n\\r\\n    duration_in_s = duration    # second\\r\\n    if period == \\"m\\":\\r\\n        duration_in_s = duration * 60\\r\\n    elif period == \\"h\\":\\r\\n        duration_in_s = duration * 60 * 60\\r\\n    return limit, duration_in_s\\r\\n```\\r\\n\\r\\n### Why use Lua scripts?\\r\\n\\r\\nBasically, the execution of the algorithm will be handled by Redis, which reduces the load on the API Server when there is a large number of incoming requests. Additionally, it is faster than running the algorithm on the API Server.\\r\\n\\r\\nLua scripts block everything. You can\'t run two Lua scripts in parallel. So it ensures atomicity for our operations.\\r\\n\\r\\n> The trick here is that everything needs to happen atomically, we want to be able to trim the set, check its cardinality, add an item to it, and set it\'s expiration, all without anything changing in the interim.\\r\\n[\u2014 By Redis \u2014](https://redis.io/learn/develop/dotnet/aspnetcore/rate-limiting/sliding-window#sliding-window-rate-limiter-lua-script)\\r\\n> \\r\\n\\r\\n```lua showLineNumbers\\r\\nlocal current_time = redis.call(\'TIME\')\\r\\nlocal trim_time = tonumber(current_time[1]) - ARGV[1]\\r\\nredis.call(\'ZREMRANGEBYSCORE\', KEYS[1], 0, trim_time)\\r\\nlocal request_count = redis.call(\'ZCARD\', KEYS[1])\\r\\n\\r\\nif request_count < tonumber(ARGV[2]) then\\r\\n    redis.call(\'ZADD\', KEYS[1], current_time[1], current_time[1] .. current_time[2])\\r\\n    redis.call(\'EXPIRE\', KEYS[1], ARGV[1])\\r\\n    return 0\\r\\nend\\r\\nreturn 1\\r\\n```\\r\\n\\r\\n## Testing\\r\\n\\r\\nI created 2 APIs with Rate-limit middleware and 2 test scripts for it. The script will look like this:\\r\\n\\r\\n```python showLineNumbers title=\\"./tests/rate_limiting/test_rate_limit_1.py\\"\\r\\nurl = \\"http://localhost:8000/api/v1/utils/limiting1\\"\\r\\npayload = \\"\\"\\r\\n\\r\\ndef send_request():\\r\\n    now = datetime.now()\\r\\n    response = requests.request(\\"GET\\", url, data=payload)\\r\\n    print(now, response.text)\\r\\n    time.sleep(0.5)\\r\\n\\r\\n# On my local machine, the request function take 2 seconds to get a response.\\r\\n# Rule: 5/15s\\r\\nsend_request()  # R1 success at 2.5s, redis contains [R1]\\r\\nsend_request()  # R2 success at 5s, redis contains [R1, R2]\\r\\nsend_request()  # R3 success at 7.5s, redis contains [R1, R2, R3]\\r\\nsend_request()  # R4 success at 10s, redis contains [R1, R2, R3, R4]\\r\\nsend_request()  # R5 success at 12.5s, redis contains [R1, R2, R3, R4, R5]\\r\\nsend_request()  # R6 fails at 15s, redis contains [R1, R2, R3, R4, R5]\\r\\nsend_request()  # R7 success at 18.5s, redis contains [R2, R3, R4, R5, R7]\\r\\n```\\r\\n\\r\\nNow, let\'s see the results:\\r\\n\\r\\n![Test results](./test-results.png#center)\\r\\n\\r\\nAPI server logs:\\r\\n\\r\\n![API Server Logs](./api-server-logs.png#center)\\r\\n\\r\\n## Notes\\r\\n\\r\\nInstead of Redis, you can implement a queue/map in your application to store request values. But consider the scalability of your system\u2014implementing in-app queues or maps like this is no longer suitable and can lead to redundancy and resource waste. Therefore, if your application has non-functional requirements for scalability, consider using shared storage solutions like **Redis** ([Redis - The Real-time Data Platform](https://redis.io/)) or **Memcached** ([memcached - a distributed memory object caching system](https://memcached.org/)).\\r\\n\\r\\nAdditionally, to implement rate-limiting as middleware in API server, we can also implement it at **network layer**. This is also an interesting topic, and I hope to share more about it in other posts.\\r\\n\\r\\nSome packages that you may be interested in:\\r\\n\\r\\n- [laurentS/slowapi: A rate limiter for Starlette and FastAPI (github.com)](https://github.com/laurentS/slowapi) (1.1k stars, 72 forks on 18/08/2024)\\r\\n- [long2ice/fastapi-limiter: A request rate limiter for fastapi (github.com)](https://github.com/long2ice/fastapi-limiter) (465 stars, 52 forks on 18/08/2024)\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nWe have explored some aspects of rate-limiting and how to implement it in FastAPI.\\r\\n\\r\\nI hope this post was useful. If you need a project to run a demo on your environment, here is my\xa0[Git repository](https://github.com/pxuanbach/fastapi-essential-modules/tree/module/rate-limiting).\\r\\n\\r\\nSee you again!\\r\\n\\r\\n## References\\r\\n\\r\\n- [What is rate limiting? | Rate limiting and bots | Cloudflare](https://www.cloudflare.com/learning/bots/what-is-rate-limiting/)\\r\\n- [Rate limiting best practices \xb7 Cloudflare Web Application Firewall (WAF) docs](https://developers.cloudflare.com/waf/rate-limiting-rules/best-practices/)\\r\\n- [How to implement Sliding Window Rate Limiting app using ASP.NET Core & Redis](https://redis.io/learn/develop/dotnet/aspnetcore/rate-limiting/sliding-window#sliding-window-rate-limiter-lua-script)\\r\\n- [Redis and Lua Powered Sliding Window Rate Limiter (halodoc.io)](https://blogs.halodoc.io/taming-the-traffic-redis-and-lua-powered-sliding-window-rate-limiter-in-action/)"},{"id":"essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler","metadata":{"permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler","source":"@site/blog/08_essential-modules-for-developing-applications-with-fastapi-p4/index.md","title":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","description":"Welcome back!","date":"2024-07-28T10:00:00.000Z","tags":[{"label":"fastapi","permalink":"/blog/tags/fastapi"},{"label":"apscheduler","permalink":"/blog/tags/apscheduler"},{"label":"job-scheduler","permalink":"/blog/tags/job-scheduler"},{"label":"python","permalink":"/blog/tags/python"},{"label":"essential-modules-fastapi","permalink":"/blog/tags/essential-modules-fastapi"}],"readingTime":8.74,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler","title":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","authors":["pxuanbach"],"tags":["fastapi","apscheduler","job-scheduler","python","essential-modules-fastapi"],"date":"2024-07-28T10:00","image":"/img/02_essential-modules-for-developing-applications-with-fastapi-p1/featured.png"},"unlisted":false,"prevItem":{"title":"Essential modules for developing applications with FastAPI (P5 - Rate-limiting)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p5-rate-limiting"},"nextItem":{"title":"Asynchronous Request Batching Design Pattern in Node.js","permalink":"/blog/asynchronous-request-batching-design-pattern-in-nodejs"}},"content":"Welcome back!\\r\\n\\r\\nIn part 4 of our series, we delve deeper into essential tools that streamline development and enhance functionality. Let\'s dive in!\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Framework/Library version\\r\\n\\r\\nThis project uses [Python](https://www.python.org/) 3.10 as the environment and [Poetry](https://python-poetry.org/) as the package manager.\\r\\n\\r\\nThe code and examples in this post will use frameworks/libraries with the following versions.\\r\\n\\r\\n```toml showLineNumbers title=\\"./pyproject.toml\\"\\r\\n[tool.poetry.dependencies]\\r\\npython = \\"^3.10\\"\\r\\nuvicorn = {extras = [\\"standard\\"], version = \\"^0.24.0.post1\\"}\\r\\nfastapi = \\"^0.109.1\\"\\r\\npython-multipart = \\"^0.0.7\\"\\r\\nemail-validator = \\"^2.1.0.post1\\"\\r\\npasslib = {extras = [\\"bcrypt\\"], version = \\"^1.7.4\\"}\\r\\ntenacity = \\"^8.2.3\\"\\r\\npydantic = \\">2.0\\"\\r\\nemails = \\"^0.6\\"\\r\\ngunicorn = \\"^21.2.0\\"\\r\\njinja2 = \\"^3.1.2\\"\\r\\nalembic = \\"^1.12.1\\"\\r\\npython-jose = {extras = [\\"cryptography\\"], version = \\"^3.3.0\\"}\\r\\nhttpx = \\"^0.25.1\\"\\r\\npsycopg = {extras = [\\"binary\\"], version = \\"^3.1.13\\"}\\r\\n\\r\\nsqlmodel = \\"^0.0.16\\"\\r\\n\\r\\n# Pin bcrypt until passlib supports the latest\\r\\nbcrypt = \\"4.0.1\\"\\r\\npydantic-settings = \\"^2.2.1\\"\\r\\nsentry-sdk = {extras = [\\"fastapi\\"], version = \\"^1.40.6\\"}\\r\\npsycopg2 = \\"^2.9.9\\"\\r\\nasyncpg = \\"^0.29.0\\"\\r\\nredis = {extras = [\\"hiredis\\"], version = \\"^5.0.3\\"}\\r\\norjson = \\"^3.10.0\\"\\r\\napscheduler = \\"^3.10.4\\"\\r\\n```\\r\\n\\r\\n## Job Scheduler\\r\\n\\r\\nIn the real world, the Job Scheduler module stands out as a fundamental tool for automating tasks and managing scheduled activities within web applications. FastAPI, with its asynchronous capabilities, integrates seamlessly with Job Scheduler modules, allowing efficient automation of recurring tasks.\\r\\n\\r\\nI will introduce a library that I am using to build this module for my applications. It offers many options, powerful features, and operates flexibly. \\r\\n\\r\\nIt\'s [APScheduler](https://github.com/agronholm/apscheduler) (6k stars, 693 forks on 28/07/2024). If you are wondering why, the answer comes from a ticket in my job, which required adding a feature to schedule certain tasks during the day in the FastAPI application. Then, I wandered through various blogs, forums, and stopped at a [discussion](https://github.com/tiangolo/fastapi/discussions/9143) in the FastAPI repository. Many libraries were mentioned, and then I stopped [here](https://github.com/tiangolo/fastapi/discussions/9143#discussioncomment-5157569). \\r\\n\\r\\nThis library supports job storage with SQLAlchemy (`SQLAlchemyJobStore`) and job scheduling with asyncio (`AsyncIOScheduler`).\\r\\n\\r\\nThis library has three built-in scheduling systems\\r\\n\\r\\n- Cron-style scheduling (with optional start/end times)\\r\\n- Interval-based execution (runs jobs on even intervals, with optional start/end times)\\r\\n- One-off delayed execution (runs jobs once, on a set date/time)\\r\\n\\r\\nSo, we can organize the module flexibly to take advantage of those benefits.\\r\\n\\r\\n### Install\\r\\n\\r\\nFirst, we should install the APScheduler package. \\r\\n\\r\\n```bash\\r\\npoetry add apscheduler   # pip install apscheduler\\r\\n```\\r\\n\\r\\n### Prepare before coding\\r\\n\\r\\nBecause APScheduler provides flexibility in adding and removing jobs at runtime, I usually build it as a regular entity in the system. That means there will be APIs performing CRD operations for it (no need for U - Update, because we only need to delete and re-add that job back into the system).\\r\\n\\r\\nWe will have a separate database to store jobs, typically I will use SQLite or PostgreSQL. It\'s lightweight and compatible with SQLAlchemy (`SQLAlchemyJobStore`).\\r\\n\\r\\nIn practice, I almost exclusively work with tasks that use `cron` or `interval` triggers. I rarely use `date`. However, for the sake of completeness in this article, I will also write APIs to support it.\\r\\n\\r\\n### Create a Scheduler instance\\r\\n\\r\\nIn the first steps, we just need to declare an instance representing the Scheduler.\\r\\n\\r\\n```python showLineNumbers title=\\"./app/core/scheduler.py\\"\\r\\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\\r\\nfrom apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\\r\\nfrom app.core.config import settings\\r\\n\\r\\njobstores = { \'default\': SQLAlchemyJobStore(url=settings.JOB_DATABASE_URI) }\\r\\nscheduler = AsyncIOScheduler(jobstores=jobstores)\\r\\n```\\r\\n\\r\\n### Integration with FastAPI\\r\\n\\r\\nWe also use the lifespan function to manage the lifecycle of the scheduler instance.\\r\\n\\r\\n```python showLineNumbers {9,18} title=\\"./app/main.py\\"\\r\\nfrom fastapi import FastAPI\\r\\nfrom contextlib import asynccontextmanager\\r\\nfrom app.core.scheduler import scheduler\\r\\n\\r\\n@asynccontextmanager\\r\\nasync def lifespan(app: FastAPI):\\r\\n    # start up\\r\\n    try:\\r\\n        scheduler.start() \\r\\n    except Exception as e:    \\r\\n        logging.error(\\"Unable to Create Schedule Object - [%s]\\", str(e))   \\r\\n    yield\\r\\n    # shut down\\r\\n    scheduler.shutdown()\\r\\n    \\r\\napp = FastAPI(\\r\\n    title=settings.PROJECT_NAME,\\r\\n    lifespan=lifespan\\r\\n)\\r\\n```\\r\\n\\r\\nWhen running the project, you will see the log printed on the console as follows:\\r\\n\\r\\n![Startup Log](./start-up-log.png)\\r\\n\\r\\nThis is just the warm-up part, now let\'s move on to the main part.\\r\\n\\r\\n### Build Job Creation API\\r\\n\\r\\nFirst of all, initialize an APIRouter instance and register it in the FastAPI application.\\r\\n\\r\\n```python showLineNumbers title=\\"./app/api/jobs.py\\"\\r\\nimport logging\\r\\nfrom fastapi import APIRouter, HTTPException, Request, BackgroundTasks, status\\r\\n\\r\\nrouter = APIRouter(prefix=\\"/jobs\\")\\r\\n```\\r\\n\\r\\n```python showLineNumbers {4} title=\\"./app/api/__init__.py\\"\\r\\nfrom fastapi import APIRouter\\r\\nfrom app.api import user, utils, jobs\\r\\n...\\r\\nrouter.include_router(jobs.router, tags=[\\"Job\\"])\\r\\n```\\r\\n\\r\\nNow, let\'s talk about the interesting things about job creation API. As mentioned above, we will create an API to be able to create a new job while the application is running. \\r\\n\\r\\n#### Schema/Model\\r\\n\\r\\nThe payload schema would look as follows:\\r\\n\\r\\n```python showLineNumbers title=\\"./app/models/job.py\\"\\r\\nfrom datetime import datetime\\r\\nfrom typing import Any, List, Literal, Optional, Union\\r\\nfrom sqlmodel import SQLModel\\r\\n\\r\\nclass CronArgs(SQLModel):\\r\\n    year: Optional[str] = \\"*\\"\\r\\n    month: Optional[str] = \\"*\\"\\r\\n    day: Optional[str] = \\"*\\" \\r\\n    week: Optional[str] = \\"*\\"\\r\\n    day_of_week: Optional[str] = \\"*\\"\\r\\n    hour: Optional[str] = \\"*\\"\\r\\n    minute: Optional[str] = \\"*\\"\\r\\n    second: Optional[str] = \\"5\\"\\r\\n\\r\\nclass IntervalArgs(SQLModel):\\r\\n    seconds: Optional[int] = 10\\r\\n    minutes: Optional[int] = None\\r\\n    hours: Optional[int] = None\\r\\n    days: Optional[int] = None\\r\\n    weeks: Optional[int] = None\\r\\n\\r\\nclass DateArgs(SQLModel):\\r\\n    args: List[Any] = []\\r\\n    run_date: datetime = datetime.now()\\r\\n\\r\\nclass JobCreate(SQLModel):\\r\\n    job_id: str\\r\\n    from_file: bool = True\\r\\n    type: Literal[\'cron\', \'interval\', \'date\'] = \'cron\'\\r\\n    args: Optional[Union[DateArgs, IntervalArgs, CronArgs]] = None\\r\\n```\\r\\n\\r\\nIn this example, the API can create all 3 types of jobs simultaneously, so I designed it as above. However, it might be beneficial to decouple these functionalities for improved maintainability and extensibility.\\r\\n\\r\\n#### API Logic\\r\\n\\r\\nNext, let\'s take a quick look at this code snippet.\\r\\n\\r\\n```python showLineNumbers {4-6,10-31,35,37-42} title=\\"./app/api/jobs.py\\"\\r\\n@router.post(\\"\\", response_model=JobCreateDeleteResponse)\\r\\nasync def add_job_to_scheduler(obj_in: JobCreate) -> JobCreateDeleteResponse:\\r\\n    # Find job folder\\r\\n    job_folder = path.join(\\"app\\", settings.JOB_DIR, obj_in.job_id)\\r\\n    if not path.exists(job_folder):\\r\\n        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\\"Job folder not found.\\")\\r\\n    \\r\\n    _timers = obj_in.args\\r\\n    # Get timer parameters if `.schedule` file exists\\r\\n    if obj_in.from_file:\\r\\n        _timers = {}\\r\\n        _sched_path = path.join(job_folder, \\".schedule\\")\\r\\n        if not path.exists(_sched_path):\\r\\n            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\\"Schedule file not found\\")\\r\\n\\r\\n        # read parameters from `.schedule` file\\r\\n        sched = read_file_line_by_line(_sched_path)\\r\\n        for i in range(len(sched)):\\r\\n            if i == 0 or str(sched[i]).startswith(\'#\') or sched[i] == \'\' or sched[i] is None:\\r\\n                continue \\r\\n            _interval_timer = str(sched[i]).split(\\"=\\")\\r\\n            _timers.update({_interval_timer[0]: _interval_timer[1]})\\r\\n    # Get cron-job timer parameters if type equals \\"cron\\"\\r\\n    if obj_in.type == \\"cron\\":\\r\\n        _timers = CronArgs.model_validate(_timers)\\r\\n    # Get interval-job timer parameters if type equals \\"interval\\"\\r\\n    elif obj_in.type == \\"interval\\":\\r\\n        _timers = IntervalArgs.model_validate(_timers)\\r\\n    # Get date-off job timer parameters if type equals \\"date\\"\\r\\n    elif obj_in.type == \\"date\\":\\r\\n        _timers = DateArgs.model_validate(_timers)\\r\\n\\r\\n    # find job module in `./app/jobs` folder, \\r\\n    # register the `call` function inside the module to the scheduler with timer parameters\\r\\n    _job_module = importlib.import_module(f\\"app.jobs.{obj_in.job_id}.main\\")\\r\\n    try:\\r\\n        job = scheduler.add_job(\\r\\n            _job_module.call, \\r\\n            obj_in.type, \\r\\n            id=obj_in.job_id,\\r\\n            **_timers.model_dump(exclude_none=True)\\r\\n        )\\r\\n    except ConflictingIdError:\\r\\n        logging.warning(f\\"Job {obj_in.job_id} already exists\\")\\r\\n        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=\\"Job already exists\\")\\r\\n    except Exception as e:\\r\\n        logging.error(f\\"Add job {obj_in.job_id} - {str(e)}\\")\\r\\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\\"An error occurred\\")\\r\\n    return JobCreateDeleteResponse(scheduled=True, job_id=job.id)\\r\\n```\\r\\n\\r\\nHow does it work?\\r\\n\\r\\n1. From line **4-6**, to register a job, we must define the job into `./app/jobs` folder. The name of the job folder must be equal to the `job_id`. For example:\\r\\n    \\r\\n    ![Define Job in Jobs folder](./define-job-in-jobs-folder.png)\\r\\n    \\r\\n2. From line **10-31**, I provide 2 options to create a new job.\\r\\n    1. Register with `.schedule` file, this file will be placed in the job folder.\\r\\n    2. Register via API payload (Schema/Model) that I mentioned above.\\r\\n3. In line **35**, import the module dynamically using `job_id` via the `importlib` library.\\r\\n4. From line **37-42**, try to register the job into the scheduler\'s job store.\\r\\n\\r\\nWe will test it later. Next I will introduce the Get all jobs API.\\r\\n\\r\\n### Get All Jobs API\\r\\n\\r\\nThis API is simpler than Job Creation API.\\r\\n\\r\\nThe code will look like this:\\r\\n\\r\\n```python showLineNumbers {4} title=\\"./app/api/jobs.py\\"\\r\\n@router.get(\\"\\")\\r\\nasync def get_scheduled_jobs():\\r\\n    schedules = []\\r\\n    for job in scheduler.get_jobs():\\r\\n        schedules.append({\\r\\n            \\"job_id\\": str(job.id), \\r\\n            \\"run_frequency\\": str(job.trigger), \\r\\n            \\"next_run\\": str(job.next_run_time)\\r\\n        })\\r\\n    return { \\"total\\": len(schedules), \\"jobs\\": schedules }\\r\\n```\\r\\n\\r\\nHow does it work?\\r\\n\\r\\n1. It uses APScheduler\u2019s `get_jobs` API to get all registered jobs.\\r\\n2. Loop through all jobs to retrieve the necessary data.\\r\\n3. Returns total number of registered jobs and job information.\\r\\n\\r\\n### Delete Job API\\r\\n\\r\\nUsing the `job_id` as a unique identifier, we will invoke the APScheduler\u2019s `remove_job` API to delete the corresponding job from the job store.\\r\\n\\r\\n```python showLineNumbers {4} title=\\"./app/api/jobs.py\\"\\r\\n@router.delete(\\"/{job_id}\\", response_model=JobCreateDeleteResponse)\\r\\nasync def remove_job_from_scheduler(job_id: str) -> JobCreateDeleteResponse:\\r\\n    try:\\r\\n        scheduler.remove_job(job_id)\\r\\n    except Exception as e:\\r\\n        logging.error(f\\"Delete job {job_id} - {str(e)}\\")\\r\\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, mdetail=\\"Job deleted failed\\")\\r\\n    return JobCreateDeleteResponse(scheduled=False, job_id=job_id)\\r\\n```\\r\\n\\r\\n## Let\'s see the results\\r\\n\\r\\nI created some scripts in `./tests` folder for testing.\\r\\n\\r\\nFor instance, a script will look like:\\r\\n\\r\\n```python showLineNumbers title=\\"./tests/job_scheduler/get_list_jobs.py\\"\\r\\nimport requests\\r\\n\\r\\nurl = \\"http://localhost:8000/api/v1/jobs\\"\\r\\npayload = \\"\\"\\r\\nheaders = {\\r\\n    \\"Content-Type\\": \\"application/json\\",\\r\\n}\\r\\nresponse = requests.request(\\"GET\\", url, data=payload, headers=headers)\\r\\n\\r\\nprint(response.text)\\r\\n```\\r\\n\\r\\nLet\'s see how our job scheduler looks like. \\r\\n\\r\\n![Test Create A New Cron-job](./test-create-cron-job.png)\\r\\n\\r\\nFirst, I create a cron-job and look at the logs. As you can see, the job has been successfully registered in the job store and runs every 10 seconds.\\r\\n\\r\\nNow, delete it using the `delete_job.py` script and create an interval job.\\r\\n\\r\\n![Test Delete, Create A New Interval Job, Get Jobs](./test-delete-create-get-jobs.png)\\r\\n\\r\\nWow, it\u2019s work. Then test the Get all jobs API using `get_list_jobs.py` script. We can see it prints to the console that we have a total of 1 job in the job store.\\r\\n\\r\\n## Notes\\r\\n\\r\\nIn addition to the benefits it brings, we also need to pay attention to a few other things:\\r\\n\\r\\n- The use of an in-app scheduler can lead to higher resource consumption by the application, and it generates a larger amount of logs, which can make error tracking more challenging. To address this issue, you can separate it into a dedicated application specifically for executing scheduled jobs. Your main application can call the job scheduler app via HTTP, GRPC, etc., to register jobs into the job store.\\r\\n- Beyond `AsyncIOScheduler`, APScheduler offers a variety of job schedulers and storage options. Explore the documentation to select the optimal tool for your system. https://apscheduler.readthedocs.io/en/3.x/userguide.html#choosing-the-right-scheduler-job-store-s-executor-s-and-trigger-s\\r\\n- To avoid your scheduler consuming a lot of database connections, try to limit the number of concurrent executions. https://apscheduler.readthedocs.io/en/3.x/userguide.html#limiting-the-number-of-concurrently-executing-instances-of-a-job\\r\\n\\r\\nSome packages that you may be interested in:\\r\\n\\r\\n- [celery/celery: Distributed Task Queue (development branch) (github.com)](https://github.com/celery/celery) (24.2k stars, 4.6k forks on 28/07/2024)\\r\\n- [samuelcolvin/arq (github.com)](https://github.com/samuelcolvin/arq) (2k stars, 170 forks on 28/07/2024) - *The author is the creator of [Pydantic](https://github.com/pydantic/pydantic), an important module integrated with FastAPI.*\\r\\n- [dmontagu/fastapi-utils (github.com)](https://github.com/dmontagu/fastapi-utils) (1.8k stars, 163 forks on 28/07/2024)\\r\\n- [aio-libs/aiojobs (github.com)](https://github.com/aio-libs/aiojobs) (821 stars, 66 forks on 28/07/2024)\\r\\n- [madkote/fastapi-plugins (github.com)](https://github.com/madkote/fastapi-plugins) (355 stars, 19 forks on 28/07/2024)\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nI hope this post was useful. If you need a project to run a demo on your environment, here is my\xa0[Git repository](https://github.com/pxuanbach/fastapi-essential-modules).\\r\\n\\r\\nHave a great weekend!\\r\\n\\r\\n## References\\r\\n\\r\\n- https://apscheduler.readthedocs.io/en/3.x/userguide.html\\r\\n- [how can i run scheduling tasks using fastapi\'s \xb7 tiangolo/fastapi \xb7 Discussion #9143 (github.com)](https://github.com/tiangolo/fastapi/discussions/9143)"},{"id":"asynchronous-request-batching-design-pattern-in-nodejs","metadata":{"permalink":"/blog/asynchronous-request-batching-design-pattern-in-nodejs","source":"@site/blog/07_asynchronous_request_batching/index.md","title":"Asynchronous Request Batching Design Pattern in Node.js","description":"Hello, This is the first post when I migrated from Wordpress to Docusaurus.","date":"2024-07-18T10:00:00.000Z","tags":[{"label":"nodejs","permalink":"/blog/tags/nodejs"},{"label":"batching","permalink":"/blog/tags/batching"}],"readingTime":6.98,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"asynchronous-request-batching-design-pattern-in-nodejs","title":"Asynchronous Request Batching Design Pattern in Node.js","authors":["pxuanbach"],"tags":["nodejs","batching"],"date":"2024-07-18T10:00","image":"/img/07_asynchronous_request_batching/featured.png"},"unlisted":false,"prevItem":{"title":"Essential modules for developing applications with FastAPI (P4 - Job Scheduler)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p4-job-scheduler"},"nextItem":{"title":"Destructuring in Python","permalink":"/blog/destructuring-in-python"}},"content":"Hello, This is the first post when I migrated from Wordpress to Docusaurus.\\r\\n\\r\\nRecently, I have been researching some design patterns in Node.js to apply to my team\'s project. Are you a fan of Node.js?\\r\\n\\r\\nI have discovered some interesting patterns that can be applied to my project. Today, I will introduce the **Asynchronous Request Batching Design Pattern**. I\'m really excited to share it with you.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Define the Problem\\r\\n\\r\\nIn reality, a large number of systems are expected to face issues with high throughput and large workloads. And the projects I have been assigned to are no exception \ud83d\ude01. So\u2026 what is the solution? At that time, I imagined a lot of solutions like caching, scaling, partitioning,\u2026\\r\\n\\r\\nCaching seems quite efficient. However, it comes with the challenge of invalidating cached data. Is there a similar approach to caching that doesn\'t involve worrying about data invalidation? Is there any simpler way?\\r\\n\\r\\nThe Asynchronous Request Batching Design Pattern appears. This design pattern is really appealing to me at the moment.\\r\\n\\r\\n## Introduction to Asynchronous Request Batching\\r\\n\\r\\nLet\'s say I export an API and the Client makes multiple requests to that API at the same time. The server receives those requests and processes them concurrently. Please look at the image below.\\r\\n\\r\\n<div class=\\"text--center\\">\\r\\n  ![Normal Async Request](./normal-async-request.png)\\r\\n</div>\\r\\n\\r\\nIn the example above, both Client 1 and Client 2 make requests to the server. Each request is considered an async operation. As the number of requests increases, the number of asynchronous operations that need to be executed also grows.\\r\\n\\r\\nNow, let\'s talk about batching.\\r\\n\\r\\n<div class=\\"text--center\\">\\r\\n    ![Async Request Batching](./async-request-batching.png)\\r\\n</div>\\r\\n\\r\\nStill using the example with Client 1 and Client 2, but this time, their requests are batched together and processed in just one async operation. By doing this, when the operation completes, both clients are notified, even though the\xa0async operation is actually executed only once. \\r\\n\\r\\nThis approach offers a remarkably simple yet powerful way to optimize the load of an application. It avoids the complexity of caching mechanisms, which often require robust memory management and invalidation strategies.\\r\\n\\r\\n## Normal API server\\r\\n\\r\\nLet\'s consider an API server that manages the sales of an e-commerce company. Its task is to retrieve quantity information for hundreds of thousands of products from the database.\\r\\n\\r\\nThe schema used for this example is a simple table with three fields.\\r\\n\\r\\n```sql showLineNumbers\\r\\nCREATE TABLE sales (\\r\\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\\r\\n  product varchar(255), \\r\\n  quantity bigint\\r\\n)\\r\\n```\\r\\n\\r\\nThe data processing operation is straightforward. It involves retrieving all records with the corresponding product and calculating their total quantity. The algorithm is intentionally slow as we want to highlight the effect of batching and caching later on. The routine would look as follows (file\xa0`totalQuantity.js`):\\r\\n\\r\\n```jsx showLineNumbers title=\\"./totalQuantity.js\\"\\r\\nimport db from \'./database.js\';\\r\\n\\r\\nexport async function totalQuantity(product) {\\r\\n  const now = Date.now();\\r\\n  let sql = `SELECT product, quantity FROM sales WHERE product=?`;\\r\\n  let total = 0;\\r\\n  return new Promise(function (resolve, reject) {\\r\\n    db.all(sql, [product], (err, rows) => {\\r\\n      if (err) return reject();\\r\\n      rows.forEach((row) => {\\r\\n        total += row.quantity;\\r\\n      })\\r\\n      console.log(`totalQuantity() took: ${Date.now() - now}ms`);\\r\\n      resolve(total);\\r\\n    })\\r\\n  })\\r\\n}\\r\\n```\\r\\n\\r\\nNow, I will expose the `totalQuantity` API through a simple [Express.js](https://expressjs.com/) server (the `server.js` file):\\r\\n\\r\\n\\r\\n```jsx showLineNumbers title=\\"./server.js\\"\\r\\nimport express from \'express\';\\r\\nimport { totalQuantity } from \'./totalQuantity.js\';\\r\\n\\r\\nconst app = express();\\r\\nconst PORT = 8000;\\r\\n\\r\\napp.get(\'/\', (req, res) => {\\r\\n  res.send(\'Hello World!\');\\r\\n})\\r\\n\\r\\napp.get(\'/total-quantity\', async (req, res) => {\\r\\n  const { product } = req.query;\\r\\n  const total = await totalQuantity(product);\\r\\n  res.status(200).json({\\r\\n    product,\\r\\n    total\\r\\n  })\\r\\n})\\r\\n\\r\\napp.listen(PORT, () => {\\r\\n  console.log(`Example app listening on PORT ${PORT}`)\\r\\n})\\r\\n```\\r\\n\\r\\nBefore starting the server, we will generate some sample data. Leveraging SQLite\'s lightweight and efficient nature, I\'ve chosen it as the database for this example. Furthermore, I have prepared a script to populate the sales table with 200,000 rows (the Github repository can be found in the conclusion section).\\r\\n\\r\\n```bash\\r\\nnpm run populate\\r\\n```\\r\\n\\r\\nAfter seeding data:\\r\\n\\r\\n![After Seeding Data](./after-seeding-data.png)\\r\\n\\r\\nLet\'s start the server now.\\r\\n\\r\\n```bash\\r\\nnpm start\\r\\n```\\r\\n\\r\\n## Test scenario\\r\\n\\r\\nTo clearly illustrate the difference between a server without batching and one with a batching pattern, we will create a scenario with more than one request. So, we will use a script named `loadTest.js`, which sends 20 requests at intervals of 20ms. The request sending interval should be lower than the `totalQuantity` function\'s processing duration.\\r\\n\\r\\nTo run the test, just execute the following command:\\r\\n\\r\\n```bash\\r\\nnode loadTest.js\\r\\n```\\r\\n\\r\\nNote the total time taken for the test.\\r\\n\\r\\n![Test Normal API Server](./test-normal-api-server.png)\\r\\n\\r\\nThe results of 3 test runs are 994ms, 954ms, 957ms ~ avg **968ms**.\\r\\n\\r\\nLet\'s move on to the exciting part of today\'s discussion: optimizing asynchronous request processing using the batching pattern.\\r\\n\\r\\n## Async request batching server\\r\\n\\r\\nNow, we need to introduce an additional processing layer on top of the `totalQuantity` function. This is where we implement the mechanism of the batching pattern.\\r\\n\\r\\nNow, imagine working with caching without worrying about cache invalidation. That makes it much easier to understand.\\r\\n\\r\\nWe will use a memory space to store the promises that need to be processed (it could be an array, map, dict, etc.). And we will differentiate them based on the input of the request. New requests with the same input will reference the promises stored in the memory space. When the promises complete, they will return results to all the requests referencing them, and then we will remove them from the memory space.\\r\\n\\r\\nNow it\'s time to code.\\r\\n\\r\\n```jsx showLineNumbers {3,6-8,11-14,16} title=\\"./totalQuantityBatch.js\\"\\r\\nimport { totalQuantity as totalQuantityRaw } from \\"./totalQuantity.js\\";\\r\\n\\r\\nconst requestsQueue = new Map();\\r\\n\\r\\nexport function totalQuantity(product) {\\r\\n  if (requestsQueue.has(product)) {\\r\\n    console.log(\\"Batching...\\")\\r\\n    return requestsQueue.get(product);\\r\\n  }\\r\\n\\r\\n  const promise = totalQuantityRaw(product);\\r\\n  requestsQueue.set(product, promise);\\r\\n  promise.finally(() => {\\r\\n    requestsQueue.delete(product)\\r\\n  })\\r\\n  return promise;\\r\\n}\\r\\n```\\r\\n\\r\\nThe `totalQuantity` function of `totalQuantityBatch` module can be considered a proxy for `totalQuantityRaw` function of `totalQuantity` module, let\'s see how it works:\\r\\n\\r\\n1. In **line 3**, I define the variable `requestsQueue` as a Map to serve as temporary memory.\\r\\n2. From **line 6-8**, we check if a request with the input \u201cproduct\u201d already exists in the temporary memory. If it does, we return the stored promises.\\r\\n3. From **line 11-14**, If the input \u201cproduct\u201d does not exist, we start executing the `totalQuantityRaw` function and store it in the temporary memory. One nice thing is that we can leverage `.finally` to attach a callback that removes the promise from the temporary memory.\\r\\n4. In **line 16**, return running promise.\\r\\n\\r\\nAfter completing the logic in the `totalQuantityBatch` module, we need to update the `server.js` to incorporate the new logic.\\r\\n\\r\\n```jsx showLineNumbers title=\\"./server.js\\"\\r\\nimport express from \'express\';\\r\\n// import { totalQuantity } from \'./totalQuantity.js\';\\r\\nimport { totalQuantity } from \'./totalQuantityBatch.js\';\\r\\n...\\r\\n```\\r\\n\\r\\nRestart the server and let\'s see how the results change.\\r\\n\\r\\n<div class=\\"text--center\\">\\r\\n    ![Restart Server](./restart-server.png)\\r\\n</div>\\r\\n\\r\\nLet\u2019s run the `loadTest.js` script:\\r\\n\\r\\n![Test Async Request Batching](./test-async-request-batching.png)\\r\\n\\r\\nThe results of 3 test runs are 624ms, 648ms, 592ms ~ avg **621ms**.\\r\\n\\r\\nWe can clearly see that the processing time for all requests has been significantly reduced. To observe even greater efficiency, you can increase the number of records in the database to extend the processing time of the `totalQuantity` function.\\r\\n\\r\\n## Notes\\r\\n\\r\\nWhen implementing in a real-world project, we will use more advanced techniques to ensure the application operates reliably and smoothly.\\r\\n\\r\\n- We will need a more optimized temporary memory space. A large number of requests to the server with different inputs later could cause the memory to expand significantly. You might consider using **LRU (Least Recently Used)** or **FIFO (First In, First Out)** methods for data management.\\r\\n- You can also apply caching to enhance the effectiveness of this technique and standardize the stored data for easier sharing. Of course, you will need to address the issue of cache invalidation.\\r\\n- When the application is distributed across multiple processes and instances, storing data in memory in different locations can lead to inconsistent results and become redundant. The solution is to use a shared storage. Common caching solutions include **Redis** ([Redis - The Real-time Data Platform](https://redis.io/)) and **Memcached** ([memcached - a distributed memory object caching system](https://memcached.org/)).\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nI hope this post was useful. If you need a project to run a demo on your environment, here is my\xa0[Git repository](https://github.com/pxuanbach/nodejs-design-pattern).\\r\\n\\r\\nSee you again!\\r\\n\\r\\n## References\\r\\n\\r\\n- [Asynchronous Request Batching & Caching in Node.js | by Maharshi Shah | GoGroup Tech Blog](https://blog.gogroup.co/asynchronous-request-batching-caching-in-node-js-b724c8a92562)\\r\\n- [Node.js Design Patterns Third Edition by Mario Casciaro and Luciano Mammino (nodejsdesignpatterns.com)](https://www.nodejsdesignpatterns.com/)"},{"id":"destructuring-in-python","metadata":{"permalink":"/blog/destructuring-in-python","source":"@site/blog/06_destructuring_in_python/index.md","title":"Destructuring in Python","description":"Hello, are you learning Python? Does your job involve using Python? If so, you will find this article quite interesting \ud83d\ude09.","date":"2024-06-08T10:00:00.000Z","tags":[{"label":"python","permalink":"/blog/tags/python"}],"readingTime":6.42,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"destructuring-in-python","title":"Destructuring in Python","authors":["pxuanbach"],"tags":["python"],"date":"2024-06-08T10:00","image":"/img/06_destructuring_in_python/featured.png"},"unlisted":false,"prevItem":{"title":"Asynchronous Request Batching Design Pattern in Node.js","permalink":"/blog/asynchronous-request-batching-design-pattern-in-nodejs"},"nextItem":{"title":"Essential modules for developing applications with FastAPI (P3 - Caching)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching"}},"content":"Hello, are you learning Python? Does your job involve using Python? If so, you will find this article quite interesting \ud83d\ude09.\\r\\n\\r\\nThis article is largely inspired by the destructuring assignment in JavaScript. While working with JavaScript, I became greatly fascinated with the destructuring assignment syntax in JavaScript, and I needed it in my Python code.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## What is Destructuring in Python?\\r\\n\\r\\nDestructuring syntax is an extremely useful feature in Python that breaks down values from lists, tuples, or dictionary attributes into individual variables. It helps us write clean and readable code.\\r\\n\\r\\n![Destructuring Dictionary Example](./destructuring-dict-example.png)\\r\\n\\r\\nNow, let\u2019s explore it!\\r\\n\\r\\n## Destructuring with Lists and Tuples\\r\\n\\r\\n### Standard concept\\r\\n\\r\\nWe can easily unpack Lists or Tuples using the following syntax:\\r\\n\\r\\n```python showLineNumbers\\r\\n# list\\r\\nfirst, second, third = [3, 6, 8]  \\r\\nprint(first)    # 3\\r\\nprint(second)   # 6\\r\\nprint(third)    # 8\\r\\n\\r\\n# tuple\\r\\none, two, three = (1, 2, 3)\\r\\nprint(one)      # 1\\r\\nprint(two)      # 2\\r\\nprint(three)    # 3\\r\\n```\\r\\n\\r\\nThese are just the standard concepts for destructuring lists or tuples. At this point, we should be concerned with the order and number of elements of a list or a tuple. \\r\\n\\r\\n- Using the wrong order will lead to incorrect data flow in the system. This can become a major issue for your system, where high data consistency is essential.\\r\\n- If the number of variables to be unpacked is not equal to the length of the object. It will raise a `ValueError` exception. For example:\\r\\n\\r\\n```python showLineNumbers\\r\\ntry:\\r\\n    first, second, third, four = [3, 6, 8]  \\r\\nexcept ValueError as e:\\r\\n    print(\\"ValueError -\\", str(e))\\r\\n# ValueError - not enough values to unpack (expected 4, got 3)\\r\\n```\\r\\n\\r\\n### Ignoring values\\r\\n\\r\\nIn practice, there are many cases where we just want to unpack some values in a list or a tuple. How can we do that? Luckily, we have some syntax to make this more convenient. \\r\\n\\r\\nWith an `_` character In place of a variable name. We can skip the unused element and move on to the next element in the list or tuple.\\r\\n\\r\\nCode example:\\r\\n\\r\\n```python showLineNumbers\\r\\none, _, three, _, _ = [1, 2, 3, 4, 5]\\r\\nprint(one, three)   # 1 3\\r\\n```\\r\\n\\r\\n### Assign the remaining values\\r\\n\\r\\nIn some cases, we still want to use the remaining values. The `*` operator will help us do it. In Python, we can use the\xa0`*`\xa0operator to collect leftover values when performing a destructuring assignment.\\r\\n\\r\\n```python showLineNumbers\\r\\na, b, *re = [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"]\\r\\nprint(a)    # a\\r\\nprint(b)    # b\\r\\nprint(re)   # [\'c\', \'d\', \'e\']\\r\\n\\r\\n*start, end = (\\"dog\\", \\"cat\\", \\"frog\\", \\"crab\\")\\r\\nprint(start)    # [\'dog\', \'cat\', \'frog\']\\r\\nprint(end)      # crab\\r\\n```\\r\\n\\r\\nWe can use the `_` character and the `*` operator together to ignore the remaining values.\\r\\n\\r\\n```python showLineNumbers\\r\\na, *_ = [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"]\\r\\nprint(a)    # a\\r\\n\\r\\nstart, *_, end = (\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\")\\r\\nprint(start)    # a\\r\\nprint(end)      # e\\r\\n```\\r\\n\\r\\n## Destructuring in loops\\r\\n\\r\\nWe are familiar with the syntax of `for` loops. We can access each element in a list directly instead of using an index like in some other languages. This makes our code more Pythonic.\\r\\n\\r\\n```python showLineNumbers\\r\\nusers = [\\r\\n    ( 1, \\"Bach\\", \\"HCM\\" ),\\r\\n    ( 2, \\"Nam\\", \\"HN\\" ),\\r\\n    ( 3, \\"Trung\\", \\"NT\\" )\\r\\n]\\r\\n\\r\\nfor user in users:\\r\\n    print(user)\\r\\n# (1, \'Bach\', \'HCM\')\\r\\n# (2, \'Nam\', \'HN\')\\r\\n# (3, \'Trung\', \'NT\')\\r\\n```\\r\\n\\r\\nWith destructuring syntax, we can access individual attributes inside an element in for loops. We can write clearer and more readable code. For example:\\r\\n\\r\\n```python showLineNumbers\\r\\nusers = [\\r\\n    ( 1, \\"Bach\\", \\"HCM\\" ),\\r\\n    ( 2, \\"Nam\\", \\"HN\\" ),\\r\\n    ( 3, \\"Trung\\", \\"PR-TC\\" )\\r\\n]\\r\\n\\r\\nfor id, name, city in users:\\r\\n    print(\\"Id:\\", id, \\"- Name:\\", name, \\"- City:\\", city)\\r\\n# Id: 1 - Name: Bach - City: HCM\\r\\n# Id: 2 - Name: Nam - City: HN\\r\\n# Id: 3 - Name: Trung - City: PR-TC\\r\\n```\\r\\n\\r\\nOr you can even get the index of the element with the `enumerate` object.\\r\\n\\r\\n> The enumerate object yields pairs containing a count (from start, which defaults to zero) and a value yielded by the iterable argument.\\r\\n\u2014 [Python Docs](https://docs.python.org/2/library/functions.html?highlight=enumerate#enumerate) \u2014\\r\\n> \\r\\n\\r\\n```python showLineNumbers\\r\\nusers = [\\r\\n    ( 1, \\"Bach\\", \\"HCM\\" ),\\r\\n    ( 2, \\"Nam\\", \\"HN\\" ),\\r\\n    ( 3, \\"Trung\\", \\"PR-TC\\" )\\r\\n]\\r\\n\\r\\nfor index, (id, name, city) in enumerate(users):\\r\\n    print(\\"Index:\\", index, \\"- Id:\\", id, \\"- Name:\\", name, \\"- City:\\", city)\\r\\n# Index: 0 - Id: 1 - Name: Bach - City: HCM\\r\\n# Index: 1 - Id: 2 - Name: Nam - City: HN\\r\\n# Index: 2 - Id: 3 - Name: Trung - City: PR-TC\\r\\n```\\r\\n\\r\\nFurthermore, we can combine the use of the ignore values syntax and collect the remaining values.\\r\\n\\r\\n```python showLineNumbers\\r\\nusers = [\\r\\n    [ 1, \\"Bach\\", \\"HCM\\", \\"Python\\" ],\\r\\n    [ 2, \\"Nam\\", \\"HN\\", \\"JavaScript\\" ],\\r\\n    [ 3, \\"Trung\\", \\"PR-TC\\", \\"TypeScript\\" ]\\r\\n]\\r\\n\\r\\nfor id, _, *values in users:\\r\\n    print(\\"Id:\\", id, \\"- Value:\\", values)\\r\\n# Id: 1 - Value: [\'HCM\', \'Python\']\\r\\n# Id: 2 - Value: [\'HN\', \'JavaScript\']\\r\\n# Id: 3 - Value: [\'PR-TC\', \'TypeScript\']\\r\\n```\\r\\n\\r\\n## Destructuring dictionaries\\r\\n\\r\\nIn my work, I often encounter situations where I need to handle objects/dictionaries with many key-value pairs. \\r\\n\\r\\n### Standard concept\\r\\n\\r\\nLet\'s evaluate the example below.\\r\\n\\r\\n```python showLineNumbers\\r\\ncustomer = {\\r\\n    \\"first_name\\": \\"John\\",\\r\\n    \\"last_name\\": \\"Cena\\",\\r\\n    \\"age\\": 23,\\r\\n    \\"email\\": \\"johncena@gmail.com\\"\\r\\n}\\r\\n\\r\\none, two, three, four = customer\\r\\nprint(f\\"One \'{one}\', two \'{two}\', three \'{three}\', four \'{four}\'\\")\\r\\n# One \'first_name\', two \'last_name\', three \'age\', four \'email\'\\r\\n\\r\\none, two, three, four = customer.values()\\r\\nprint(f\\"One \'{one}\', two \'{two}\', three \'{three}\', four \'{four}\'\\")\\r\\n# One \'John\', two \'Cena\', three \'23\', four \'johncena@gmail.com\'\\r\\n```\\r\\n\\r\\nIn this example, when we try to get the variables `one`, `two`, `three`, and `four`, these variables will receive the corresponding values of the keys from the `customer` dictionary. Or we can get the list of values of the keys in that order by using the `.values()` method of the dictionary.\\r\\n\\r\\nInstead of using the above approach, we can directly get the values from the dictionary using their keys. \\r\\n\\r\\n```python showLineNumbers\\r\\nprint(f\\"Customer email {customer[\'email\']}, age {customer[\'age\']}\\")\\r\\n# Customer email johncena@gmail.com, age 23\\r\\n```\\r\\n\\r\\nIf you access an unknown key, it will throw a `KeyError` exception.\\r\\n\\r\\n### Advanced techniques\\r\\n\\r\\nThe above approaches are completely fine, and you can get the job done quickly without much effort.\\r\\n\\r\\nBut maintaining or reading that code is a nightmare. Imagine you have a dictionary with hundreds of keys (or even more), each part where you access a key of the dictionary and perform logic with it. After a few weeks or months, you get a task related to that code. That\'s where the nightmare begins.\\r\\n\\r\\nTo solve that problem, we should group the declarations of the variables we need to use together. And I found a way to do that while keeping our code clean and readable.\\r\\n\\r\\nWe can use\xa0[`operator`](https://docs.python.org/3/library/operator.html)\xa0module from the standard library as follows:\\r\\n\\r\\n```python showLineNumbers\\r\\nfrom operator import itemgetter\\r\\n\\r\\ncurrent_user = {\\r\\n    \\"id\\": 1,\\r\\n    \\"username\\": \\"pxuanbach\\",\\r\\n    \\"email\\": \\"pxuanbach@gmail.com\\",\\r\\n    \\"phone\\": \\"832819201\\",\\r\\n    \\"full_name\\": \\"Bach Pham\\",\\r\\n    \\"gender\\": \\"Male\\",\\r\\n    \\"website\\": \\"immersedincode.io.vn\\"\\r\\n}\\r\\n\\r\\nid, email, gender, username = itemgetter(\\r\\n    \'id\', \'email\', \'gender\', \'username\'\\r\\n)(current_user)\\r\\n\\r\\nprint(\\"Id:\\", id, \\"- Email:\\", email, \\"- Gender:\\", gender, \\"- Username:\\", username)\\r\\n# Id: 1 - Email: pxuanbach@gmail.com - Gender: Male - Username: pxuanbach\\r\\n```\\r\\n\\r\\nIn the example above, the value of each variable will correspond to the order of keys in the `itemgetter` function. Additionally, If you access an unknown key, the function will throw a `KeyError` exception.\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nSo we\'ve covered destructuring lists, tuples, for loops, and dictionaries. I hope this article is helpful to you.\\r\\n\\r\\nIf you need a project to run a demo on your environment, here is my\xa0[Git repository](https://github.com/pxuanbach/Destructuring-in-Python).\\r\\n\\r\\n## References\\r\\n\\r\\n- [Destructuring in Python (teclado.com)](https://blog.teclado.com/destructuring-in-python/)\\r\\n- [Destructuring dicts and objects in Python (stackoverflow.com)](https://stackoverflow.com/a/63600600/22865115)"},{"id":"essential-modules-for-developing-applications-with-fastapi-p3-caching","metadata":{"permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching","source":"@site/blog/05_essential-modules-for-developing-applications-with-fastapi-p3/index.md","title":"Essential modules for developing applications with FastAPI (P3 - Caching)","description":"Hello, we meet again!","date":"2024-04-20T10:00:00.000Z","tags":[{"label":"caching","permalink":"/blog/tags/caching"},{"label":"fastapi","permalink":"/blog/tags/fastapi"},{"label":"redis","permalink":"/blog/tags/redis"},{"label":"python","permalink":"/blog/tags/python"},{"label":"essential-modules-fastapi","permalink":"/blog/tags/essential-modules-fastapi"}],"readingTime":7.81,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"essential-modules-for-developing-applications-with-fastapi-p3-caching","title":"Essential modules for developing applications with FastAPI (P3 - Caching)","authors":["pxuanbach"],"tags":["caching","fastapi","redis","python","essential-modules-fastapi"],"date":"2024-04-20T10:00","image":"/img/02_essential-modules-for-developing-applications-with-fastapi-p1/featured.png"},"unlisted":false,"prevItem":{"title":"Destructuring in Python","permalink":"/blog/destructuring-in-python"},"nextItem":{"title":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","permalink":"/blog/dockerizing-medusajs-for-optimized-deployment"}},"content":"Hello, we meet again!\\r\\n\\r\\nIn the previous post, I introduced two quite important modules in a project, which are Migration and Logging. In this article, I will introduce the Caching module. \\r\\n\\r\\nI hope you enjoy it!\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Framework/Library version\\r\\n\\r\\nThis project uses [Python](https://www.python.org/) 3.10 as the environment and [Poetry](https://python-poetry.org/) as the package manager.\\r\\n\\r\\nThe code and examples in this post will use frameworks/libraries with the following versions.\\r\\n\\r\\n```toml showLineNumbers title=\\"./pyproject.toml\\"\\r\\n[tool.poetry.dependencies]\\r\\npython = \\"^3.10\\"\\r\\nuvicorn = {extras = [\\"standard\\"], version = \\"^0.24.0.post1\\"}\\r\\nfastapi = \\"^0.109.1\\"\\r\\npython-multipart = \\"^0.0.7\\"\\r\\nemail-validator = \\"^2.1.0.post1\\"\\r\\npasslib = {extras = [\\"bcrypt\\"], version = \\"^1.7.4\\"}\\r\\ntenacity = \\"^8.2.3\\"\\r\\npydantic = \\">2.0\\"\\r\\nemails = \\"^0.6\\"\\r\\ngunicorn = \\"^21.2.0\\"\\r\\njinja2 = \\"^3.1.2\\"\\r\\nalembic = \\"^1.12.1\\"\\r\\npython-jose = {extras = [\\"cryptography\\"], version = \\"^3.3.0\\"}\\r\\nhttpx = \\"^0.25.1\\"\\r\\npsycopg = {extras = [\\"binary\\"], version = \\"^3.1.13\\"}\\r\\n\\r\\nsqlmodel = \\"^0.0.16\\"\\r\\n\\r\\n# Pin bcrypt until passlib supports the latest\\r\\nbcrypt = \\"4.0.1\\"\\r\\npydantic-settings = \\"^2.2.1\\"\\r\\nsentry-sdk = {extras = [\\"fastapi\\"], version = \\"^1.40.6\\"}\\r\\npsycopg2 = \\"^2.9.9\\"\\r\\nasyncpg = \\"^0.29.0\\"\\r\\nredis = {extras = [\\"hiredis\\"], version = \\"^5.0.3\\"}\\r\\norjson = \\"^3.10.0\\"\\r\\n```\\r\\n\\r\\n## Caching\\r\\n\\r\\nWe will explore integrating a caching module into FastAPI. This caching module will automatically store the results of previous API requests, thereby improving response times and reducing server load. There are many other places where you can cache data, such as In-memory, Redis, DynamoDB,\u2026 This section will implement a simple Redis caching solution for APIs.\\r\\n\\r\\nFirst, we should install the [Redis](https://github.com/redis/redis-py) package. \\r\\n\\r\\n```bash\\r\\npoetry add redis   # pip install redis\\r\\n```\\r\\n\\r\\n### Prepare an adapter for Redis\\r\\n\\r\\nWhen working with Redis, we must manage connecting and disconnecting to Redis as necessary. Additionally, certain features require additional logic to integrate with the system seamlessly.\\r\\n\\r\\nSo, I define some functions that I want to use such as\u2026\\r\\n\\r\\n- Connect.\\r\\n- Disconnect.\\r\\n- Add a key to Redis.\\r\\n- Check the key exists in Redis.\\r\\n\\r\\nLet\u2019s build it.\\r\\n\\r\\n```python showLineNumbers title=\\"./app/core/redis.py\\"\\r\\nfrom typing import Any, Dict, Tuple\\r\\nimport redis.asyncio as aioredis\\r\\nimport logging\\r\\nfrom app.utils import ORJsonCoder\\r\\n\\r\\nclass RedisClient:\\r\\n    async def connect(self, redis_url: str):\\r\\n        self.pool = aioredis.ConnectionPool().from_url(redis_url)\\r\\n        self.redis = aioredis.Redis.from_pool(self.pool)\\r\\n        if await self.redis.ping():\\r\\n            logging.info(\\"Redis connected\\")\\r\\n            return True\\r\\n        logging.warning(\\"Cannot connect to Redis\\")\\r\\n        return False\\r\\n    \\r\\n    async def add_to_cache(self, key: str, value: Dict, expire: int) -> bool:\\r\\n        response_data = None\\r\\n        try:\\r\\n            response_data = ORJsonCoder().encode(value)\\r\\n        except TypeError:\\r\\n            message = f\\"Object of type {type(value)} is not JSON-serializable\\"\\r\\n            logging.error(message)\\r\\n            return False\\r\\n        cached = await self.redis.set(name=key, value=response_data, ex=expire)\\r\\n        if cached:\\r\\n            logging.info(f\\"{key} added to cache\\")\\r\\n        else:  # pragma: no cover\\r\\n            logging.warning(f\\"Failed to cache key {key}\\")\\r\\n        return cached\\r\\n    \\r\\n    async def check_cache(self, key: str) -> Tuple[int, str]:\\r\\n        pipe = self.redis.pipeline()\\r\\n        ttl, in_cache = await pipe.ttl(key).get(key).execute()\\r\\n        if in_cache:\\r\\n            logging.info(f\\"Key {key} found in cache\\")\\r\\n        return (ttl, in_cache)\\r\\n\\r\\n    async def disconnect(self):\\r\\n        if await self.redis.ping():\\r\\n            await self.redis.aclose()\\r\\n            logging.info(\\"Redis disconnected\\")\\r\\n        return None\\r\\n\\r\\nredis_client = RedisClient()\\r\\n```\\r\\n\\r\\nIn this class, you might wonder about the `ORJsonCoder` module. Where is it? What can it do?\\r\\n\\r\\nThis module provides methods to perform JSON encoding and decoding using the [orjson](https://github.com/ijl/orjson) library, aimed at increasing speed and performance compared to standard JSON libraries in Python.\\r\\n\\r\\n```python showLineNumbers title=\\"./app/utils/orjson_coder.py\\"\\r\\nfrom typing import Any, Union\\r\\nfrom fastapi.encoders import jsonable_encoder\\r\\nimport orjson\\r\\n\\r\\nclass ORJsonCoder:\\r\\n    def encode(cls, value: Any) -> bytes:\\r\\n        return orjson.dumps(\\r\\n            value,\\r\\n            default=jsonable_encoder,\\r\\n            option=orjson.OPT_NON_STR_KEYS | orjson.OPT_SERIALIZE_NUMPY,\\r\\n        )\\r\\n\\r\\n    def decode(cls, value: Union[bytes | str]) -> Any:\\r\\n        return orjson.loads(value)\\r\\n```\\r\\n\\r\\n### Implement logic for caching\\r\\n\\r\\nNext, we will create a module to provide caching features based on `RedisClient`. To clarify what we need, let\'s explore the concept of Redis a bit.\\r\\n\\r\\n#### What is Redis?\\r\\n\\r\\n> Redis (**RE**mote\xa0**DI**ctionary\xa0**S**erver) is an open source, in-memory, NoSQL\xa0key/value store that is used primarily as an application cache or quick-response database.\\r\\n[*\u2014 By IBM \u2014*](https://www.ibm.com/topics/redis)\\r\\n\\r\\n\\r\\nSo, to effectively use Redis as a cache, we need an efficient strategy for creating keys and values. Fortunately, Redis now supports various [data types](https://redis.io/docs/latest/develop/data-types/) such as strings, hashes, lists, sets, sorted sets, and JSON\u2026\\r\\n\\r\\nAnother thing to keep in mind is what to cache and how long to keep it in cache.\\r\\n\\r\\n#### What to cache?\\r\\n\\r\\n> We don\'t want to cache many keys that change continuously.\\r\\n> We don\'t want to cache many keys that are requested very rarely.\\r\\n> We want to cache keys that are requested often and change at a reasonable rate. For an example of key not changing at a reasonable rate, think of a global counter that is continuously\xa0[INCR](https://redis.io/docs/latest/commands/incr/)emented.\\r\\n> [\u2014 By Redis Docs \u2014](https://redis.io/docs/latest/develop/use/client-side-caching/#what-to-cache)\\r\\n\\r\\n#### How long to keep it?\\r\\n\\r\\nThis is a question that you need to answer for yourself; everything depends on the logic of your system. \\r\\n\\r\\nSome systems use Redis to implement a whitelist strategy for their authorization tokens, so the time to keep that key in the cache memory could be the token\'s lifespan. \\r\\n\\r\\nThey also implement data caching for high-traffic APIs with large response data, where the data returned from these APIs has minimal changes. The time to keep the key in the cache memory could be either the time it takes for a change to occur or just enough time to ensure users aren\'t stuck with outdated data for too long.\\r\\n\\r\\nSuppose you have complex computational tasks that are time-consuming or require significant computational resources. In that case, you can use cron jobs (schedule jobs) to precompute the results and store them in the cache to optimize user experience and system resource utilization.\\r\\n\\r\\nNext, let\'s take a quick look at this code snippet.\\r\\n\\r\\n```python showLineNumbers title=\\"./app/core/cache.py\\"\\r\\nfrom typing import Any\\r\\nfrom urllib.parse import urlencode\\r\\nfrom fastapi import Request\\r\\nfrom fastapi.datastructures import QueryParams\\r\\n\\r\\nfrom app.core.redis import redis_client\\r\\nfrom app.utils import ORJsonCoder\\r\\n\\r\\ndef query_params_builder(params: QueryParams) -> str:\\r\\n    sorted_query_params = sorted(params.items())\\r\\n    return urlencode(sorted_query_params, doseq=True)\\r\\n\\r\\ndef req_key_builder(req: Request, **kwargs):\\r\\n    return \\":\\".join([\\r\\n        req.method.lower(), \\r\\n        req.url.path, \\r\\n        query_params_builder(req.query_params)\\r\\n    ])\\r\\n\\r\\nasync def add(req: Request, data: Any, expire: int = 60):   \\r\\n    cached = await redis_client.add_to_cache(req_key_builder(req), data, expire)\\r\\n    if not cached:\\r\\n        return False\\r\\n    return True\\r\\n\\r\\nasync def check_exist(req: Request) -> str:\\r\\n    key = req_key_builder(req)\\r\\n    ttl, in_cache = await redis_client.check_cache(key)\\r\\n    return in_cache\\r\\n\\r\\ndef load_cache_data(data: str):\\r\\n    return ORJsonCoder().decode(data)\\r\\n```\\r\\n\\r\\n- **query_params_builder**: The order of sent query parameters may be inconsistent. Therefore, we need a function to ensure consistency for the query parameters.\\r\\n- **req_key_builder**: This function is used to create a unique key based on the incoming request. The value of the key will look like `get:/api/v1/users:limit=20&skip=0`.\\r\\n- **add**: As the name suggests, add a key/value pair to Redis with the corresponding parameters.\\r\\n- **check_exist**: Used to check whether the data exists in the cache or not.\\r\\n- **load_cache_data**: Simply used to decode the data retrieved from the cache.\\r\\n\\r\\n### Integration with FastAPI\\r\\n\\r\\nAfter all the preparations are complete, the remaining task is to integrate it into the FastAPI application seamlessly.\\r\\n\\r\\nFirst, connect to Redis.\\r\\n\\r\\n```python showLineNumbers {10,13,18} title=\\"./app/main.py\\"\\r\\nfrom fastapi import FastAPI\\r\\nfrom contextlib import asynccontextmanager\\r\\n\\r\\nfrom app.core.config import settings\\r\\nfrom app.core.redis import redis_client\\r\\n\\r\\n@asynccontextmanager\\r\\nasync def lifespan(app: FastAPI):\\r\\n    # start up\\r\\n    await redis_client.connect(str(settings.REDIS_URL))\\r\\n    yield\\r\\n    # shut down\\r\\n    await redis_client.disconnect()\\r\\n\\r\\napp = FastAPI(\\r\\n    title=settings.PROJECT_NAME,\\r\\n    openapi_url=f\\"{settings.API_STR}{settings.API_VERSION_STR}/openapi.json\\",\\r\\n    lifespan=lifespan\\r\\n)\\r\\n```\\r\\n\\r\\nSecond, let\'s do a simple example. I have an API that returns user information as follows.\\r\\n\\r\\n```python showLineNumbers {7,8} title=\\"./app/api/user.py\\"\\r\\n@router.get(\\"\\", response_model=List[User])\\r\\nasync def get_pagination_cache(\\r\\n    skip: int = Query(0),\\r\\n    limit: int = Query(20),\\r\\n    session: AsyncSession = Depends(get_async_session)\\r\\n) -> Any:\\r\\n    data = await user.get_pagination(session, skip, limit)\\r\\n    return data\\r\\n```\\r\\n\\r\\nNow I will implement caching logic for this API. In FastAPI, you can use [Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/) to cache asynchronously, which reduces system load and request latency. \\r\\n\\r\\n```python showLineNumbers {4,9-11,13} title=\\"./app/api/user.py\\"\\r\\n@router.get(\\"\\", response_model=List[User])\\r\\nasync def get_pagination_cache(\\r\\n    request: Request,\\r\\n    bg_tasks: BackgroundTasks,\\r\\n    skip: int = Query(0),\\r\\n    limit: int = Query(20),\\r\\n    session: AsyncSession = Depends(get_async_session)\\r\\n) -> Any:\\r\\n    in_cache = await cache.check_exist(req=request)\\r\\n    if in_cache:\\r\\n        return cache.load_cache_data(in_cache)\\r\\n    data = await user.get_pagination(session, skip, limit)\\r\\n    bg_tasks.add_task(cache.add, req=request, data=data, expire=60)\\r\\n    return data\\r\\n```\\r\\n\\r\\nWhen a user calls this API, it will check the corresponding key in Redis. If this key exists, it will return the value of the key without querying the database. If the key does not exist, the system will query the database to retrieve the data as usual. Once the data is obtained, we return it and simultaneously store it in the cache with an expiration time of 60 seconds.\\r\\n\\r\\nI will insert approximately 20,000 records and use the `/users` API to retrieve that information. The larger the data, the more noticeable the difference in response time will be.\\r\\n\\r\\nLet\'s see what changes now. In the first usage, the API response time falls around 223 ms.\\r\\n\\r\\n![Test API Nocache](./test-api-nocache.png)\\r\\n\\r\\nIn the second usage, the response time falls around 136 ms.\\r\\n\\r\\n![Test API Cached](./test-api-cached.png)\\r\\n\\r\\nIf you have [Redis Insight](https://redis.io/insight/) on your machine, you can connect to Redis and view the values stored within it.\\r\\n\\r\\n![Redis Insight](./redis-insight.png)\\r\\n\\r\\nAdditionally, you will need to pay attention to response headers such as `Cache-Control`, `ETag`, `Vary`,\u2026\\r\\n\\r\\nSome packages that you may be interested in:\\r\\n\\r\\n- [long2ice/fastapi-cache (github.com)](https://github.com/long2ice/fastapi-cache) (1.1k stars, 132 forks on 18/04/2024)\\r\\n- [madkote/fastapi-plugins (github.com)](https://github.com/madkote/fastapi-plugins) (334 stars, 19 forks on 18/04/2024)\\r\\n- [comeuplater/fastapi_cache (github.com)](https://github.com/comeuplater/fastapi_cache) (208 stars, 16 forks on 18/04/2024)\\r\\n- [a-luna/fastapi-redis-cache (github.com)](https://github.com/a-luna/fastapi-redis-cache) (143 stars, 23 forks on 18/04/2024)\\r\\n- [mailgun/expiringdict (github.com)](https://github.com/mailgun/expiringdict) (335 stars, 75 forks on 18/04/2024)\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nWell, that\u2019s it. I have discussed some approaches to implementing caching for APIs with FastAPI as well as some related aspects. \\r\\n\\r\\nI hope this post was useful. If you need a project to run a demo on your environment, here is my\xa0[Git repository](https://github.com/pxuanbach/fastapi-essential-modules).\\r\\n\\r\\n## References\\r\\n\\r\\n- [What is Redis Explained? | IBM](https://www.ibm.com/topics/redis)\\r\\n- [Client-side caching in Redis | Docs (redis.io)](https://redis.io/docs/latest/develop/use/client-side-caching/)\\r\\n- [Understand Redis data types | Docs (redis.io)](https://redis.io/docs/latest/develop/data-types/)"},{"id":"dockerizing-medusajs-for-optimized-deployment","metadata":{"permalink":"/blog/dockerizing-medusajs-for-optimized-deployment","source":"@site/blog/04_dockerizing-medusajs-for-optimized-deployment/index.md","title":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","description":"Today, I say \\"Hi Medusa!\u201d.","date":"2024-04-14T10:00:00.000Z","tags":[{"label":"deployment","permalink":"/blog/tags/deployment"},{"label":"docker","permalink":"/blog/tags/docker"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"medusajs","permalink":"/blog/tags/medusajs"},{"label":"nodejs","permalink":"/blog/tags/nodejs"}],"readingTime":7.555,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"dockerizing-medusajs-for-optimized-deployment","title":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","authors":["pxuanbach"],"tags":["deployment","docker","open-source","medusajs","nodejs"],"date":"2024-04-14T10:00","image":"/img/04_dockerizing-medusajs-for-optimized-deployment/featured.png"},"unlisted":false,"prevItem":{"title":"Essential modules for developing applications with FastAPI (P3 - Caching)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p3-caching"},"nextItem":{"title":"Essential modules for developing applications with FastAPI (P2 - Logging)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging"}},"content":"Today, I say \\"Hi Medusa!\u201d.\\r\\n\\r\\nRecently, I received a request from my director. It involves developing an e-commerce product for a retail business client group. They require a customized solution for self-deployment in their environment. \\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nThe requirement is for an e-commerce source code that I and teammates can customize, and add features to meet the needs of each customer. After researching through various open-source platforms such as WooCommerce, Vendure, Saleor,... we decided to choose MedusaJS.\\r\\n\\r\\n## Introduction\\r\\n\\r\\n### What is MedusaJS?\\r\\n\\r\\nMedusaJS is a set of commerce modules and tools that allow you to build rich, reliable, and performant commerce applications without reinventing core commerce logic. The modules can be customized and used to build advanced e-commerce stores, marketplaces, or any product that needs foundational commerce primitives. All modules are open-source and freely available on npm.\\r\\n\\r\\nLearn more about\xa0[Medusa\u2019s architecture](https://docs.medusajs.com/development/fundamentals/architecture-overview)\xa0and\xa0[commerce modules](https://docs.medusajs.com/modules/overview)\xa0in the Docs.\\r\\n\\r\\n### Why containerize Medusa applications?\\r\\n\\r\\nThe benefits of dockerizing an application, specifically Medusa, are manifold:\\r\\n\\r\\n1. Set up development environments swiftly: With Docker Compose, you can set up your development environment with just a few commands. For instance, cloning source code, creating a `.env` file, and running `docker-compose up -d`.\\r\\n2. Deploy easily in the cloud or on-premises: Once you build your Docker images, you can run them consistently on any server environment. It also helps you automate your application deployment and version management. \\r\\n3. Ensure scalability and flexibility: Dockerized applications are easier to scale and expand. By using Docker Swarm or Kubernetes, you can automatically scale the number of application containers to meet user demand without the need to reconfigure infrastructure.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\nSome packages we need to install before proceeding further in this post.\\r\\n\\r\\n### Docker & Docker Compose\\r\\n\\r\\nVisit the official link below and download the version that corresponds to your environment.\\r\\n\\r\\n[Docker Desktop: The #1 Containerization Tool for Developers | Docker](https://www.docker.com/products/docker-desktop/)\\r\\n\\r\\n### Node or NVM\\r\\n\\r\\nVisit the official link below and download the version that corresponds to your environment.\\r\\n\\r\\n[Node.js \u2014 Download Node.js\xae (nodejs.org)](https://nodejs.org/en/download)\\r\\n\\r\\nI suggest you install version 18 or greater because:\\r\\n\\r\\n- Backend Admin needs Node v16+\\r\\n- Storefront needs Node v18+\\r\\n\\r\\n**Or** install [nvm](https://github.com/nvm-sh/nvm?tab=readme-ov-file#installing-and-updating) instead to install any version of NodeJS\\r\\n\\r\\n```bash\\r\\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash\\r\\n\\r\\n# Or\\r\\nwget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash\\r\\n```\\r\\n\\r\\nIf your environment is Windows, you may like this one\\r\\n\\r\\n[coreybutler/nvm-windows: A node.js version management utility for Windows. Ironically written in Go. (github.com)](https://github.com/coreybutler/nvm-windows?tab=readme-ov-file)\\r\\n\\r\\n### Git\\r\\n\\r\\nVisit the official link below and download the version that corresponds to your environment.\\r\\n\\r\\n[Git - Downloads (git-scm.com)](https://git-scm.com/downloads)\\r\\n\\r\\n### Medusa CLI\\r\\n\\r\\nTo install the Medusa backend, you need Medusa\'s CLI tool.\\r\\n\\r\\n```bash\\r\\nnpm install -g @medusajs/medusa-cli\\r\\n```\\r\\n\\r\\nOnce done, check the version using this command: \\r\\n\\r\\n```bash\\r\\nmedusa --version\\r\\n```\\r\\n\\r\\n![Medusa CLI Version](./medusa-cli-version.png)\\r\\n\\r\\n### Create Medusa Backend\\r\\n\\r\\nWe can easily create the backend with the following command:\\r\\n\\r\\n```bash\\r\\nnpx create-medusa-app --skip-db\\r\\n```\\r\\n\\r\\nI use the `\u2014skip-db` option because I will have a different setup later.\\r\\n\\r\\nNow wait a moment\u2026\\r\\n\\r\\n![Create Medusa App](./create-medusa-app.png)\\r\\n\\r\\n### Create Medusa Storefront\\r\\n\\r\\nCreate a new Next.js project using the\xa0[Medusa starter Storefront](https://github.com/medusajs/nextjs-starter-medusa):\\r\\n\\r\\n```bash\\r\\nnpx create-next-app -e https://github.com/medusajs/nextjs-starter-medusa storefront\\r\\n```\\r\\n\\r\\n## Containerize Medusa application\\r\\n\\r\\n### Backend\\r\\n\\r\\nFirst, create the `develop.sh` file with this content and place it in the `/backend` folder.\\r\\n\\r\\n```bash showLineNumbers title=\\"./backend/develop.sh\\"\\r\\n#!/bin/bash\\r\\n\\r\\nmedusa migrations run\\r\\n\\r\\nmedusa $1\\r\\n```\\r\\n\\r\\nNext, create a `Dockerfile` with this content in the same directory as in the previous step.\\r\\n\\r\\n```dockerfile showLineNumbers title=\\"./backend/Dockerfile\\"\\r\\n### Dependencies ###\\r\\nFROM node:20-alpine as deps\\r\\nWORKDIR /app/\\r\\n\\r\\n# Copy backend package package.json and yarn.lock from /backend\\r\\nCOPY ./package*.json .\\r\\nCOPY ./yarn.lock? .\\r\\n\\r\\n# Install deps\\r\\nRUN yarn install --frozen-lockfile\\r\\n\\r\\n### Build ###\\r\\nFROM node:20-alpine as builder\\r\\nWORKDIR /app/\\r\\n\\r\\n# Copy cached node_modules from deps\\r\\nCOPY --from=deps /app/node_modules /app/node_modules\\r\\n\\r\\n# Install python and medusa-cli\\r\\nRUN apk update\\r\\nRUN apk add python3\\r\\nRUN yarn global add @medusajs/medusa-cli@latest\\r\\n\\r\\n# Copy app source code\\r\\nCOPY . /app\\r\\n\\r\\n# Build the app\\r\\nRUN yarn build\\r\\n\\r\\n# Start the image with a shell script\\r\\nENTRYPOINT [\\"/bin/sh\\", \\"./develop.sh\\", \\"start\\"]\\r\\n```\\r\\n\\r\\nThe result:\\r\\n\\r\\n![Container Medusa Application Backend Result](./containerize-medusa-app-backend-result.png)\\r\\n\\r\\n### Admin UI\\r\\n\\r\\nAccording to the announcement, the admin UI source code has been moved into the core repository since version 1.8. So, we won\'t have the source code to build the Docker image anymore. \\r\\n\\r\\nWait, hold on\u2026\\r\\n\\r\\nWhen you run the command `medusa develop`, the admin UI will build static javascript and HTML files into the `/backend/build` directory, so we need to copy it and serve it with nginx.\\r\\n\\r\\n*A small note is that if you deploy it in production, you can configure it not to build the admin UI. Splitting it into two separate Docker images is the approach I\'m currently taking. Check [this one](https://docs.medusajs.com/admin/configuration#in-development---disabled-serve).*\\r\\n\\r\\nFirst, create the `admin.nginx.conf` file with this content and place it in the `/backend` folder.\\r\\n\\r\\n```plaintext showLineNumbers title=\\"./backend/admin.nginx.conf\\"\\r\\nserver {\\r\\n    listen 80 default_server;\\r\\n    server_name localhost;\\r\\n    charset utf-8;\\r\\n    root /usr/share/nginx/html;\\r\\n\\r\\n    location / {\\r\\n        try_files $uri $uri/ /index.html =404;\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nNext, create a `Dockerfile` with this content in the same directory as in the previous step.\\r\\n\\r\\n```dockerfile showLineNumbers title=\\"./backend/Dockerfile.admin\\"\\r\\n### Build ###\\r\\nFROM node:20-alpine as builder\\r\\nWORKDIR /app\\r\\n\\r\\nENV NODE_ENV=production\\r\\nENV NODE_OPTIONS=--openssl-legacy-provider\\r\\n\\r\\n# Copy medusa package.json and yarn.lock from /backend\\r\\nCOPY ./package*.json .\\r\\nCOPY ./yarn.lock? .\\r\\n\\r\\n# Install deps\\r\\nRUN yarn install --frozen-lockfile\\r\\n\\r\\n# Copy app source code\\r\\nCOPY . /app\\r\\n\\r\\nRUN yarn build:admin:prod\\r\\n\\r\\n### Runner ###\\r\\nFROM nginx:1.16.0-alpine as runner\\r\\n\\r\\n# Copy nginx.conf from /backend\\r\\nCOPY admin.nginx.conf /etc/nginx/conf.d/default.conf\\r\\n\\r\\nRUN apk add --no-cache bash\\r\\n\\r\\nEXPOSE 80 \\r\\n\\r\\n# Copy static files from Build stage\\r\\nCOPY --from=builder /app/build /usr/share/nginx/html\\r\\n\\r\\n# Image entrypoint\\r\\nENTRYPOINT [\\"nginx\\", \\"-g\\", \\"daemon off;\\"]\\r\\n```\\r\\n\\r\\nThe result:\\r\\n\\r\\n![Container Medusa Application Admin UI Result](./containerize-medusa-app-adminui-result.png)\\r\\n\\r\\n### Storefront\\r\\n\\r\\nSimilar to the components above, we will have the following `Dockerfile`.\\r\\n\\r\\n```dockerfile showLineNumbers title=\\"./storefront/Dockerfile\\"\\r\\n### Dependencies ###\\r\\nFROM node:20-alpine as deps\\r\\nWORKDIR /app\\r\\n\\r\\n# Copy storefront package.json and yarn.lock from /storefront\\r\\nCOPY ./package*.json .\\r\\nCOPY ./yarn.lock? .\\r\\n\\r\\n# Install deps and launch patch-package\\r\\nRUN yarn install --frozen-lockfile\\r\\n\\r\\n### Build ###\\r\\nFROM node:20-alpine as builder\\r\\nWORKDIR /app\\r\\n\\r\\n# Copy cached root and package node_modules from deps\\r\\nCOPY --from=deps /app/node_modules /app/node_modules\\r\\n\\r\\n# Copy app source code\\r\\nCOPY . /app\\r\\n\\r\\n# Build the app\\r\\nRUN yarn build\\r\\n\\r\\n# Run the builded app\\r\\nENTRYPOINT [ \\"yarn\\", \\"start\\" ]\\r\\n```\\r\\n\\r\\n## Manage containers\\r\\n\\r\\nWith the above sections, we are ready to build and deploy the Docker image. Next, I will introduce how I manage the containers and configure them for deployment using Docker Compose.\\r\\n\\r\\n### Manage environment variables\\r\\n\\r\\nFirstly, we should provide some environment variables for the app. In `/backend`, create a `.env` file and paste this content:\\r\\n\\r\\n```bash showLineNumbers\\r\\nJWT_SECRET=something\\r\\nCOOKIE_SECRET=something\\r\\n\\r\\nPOSTGRES_USER=postgres\\r\\nPOSTGRES_PASSWORD=CHANGE_ME\\r\\nPOSTGRES_DB=medusa\\r\\n\\r\\nDATABASE_TYPE=\\"postgres\\"\\r\\nREDIS_URL=redis://redis\\r\\nDATABASE_URL=postgres://postgres:CHANGE_ME@postgres:5432/medusa\\r\\nSTORE_CORS=/http://.+/\\r\\nADMIN_CORS=/http://.+/\\r\\nAUTH_CORS=/http://.+/\\r\\n```\\r\\n\\r\\n- Check out the documentation https://docs.medusajs.com/development/backend/configurations#environment-variables.\\r\\n\\r\\nNext, in `/storefront`, make a copy of `.env.template` to `.env.local`. Then change the value of `NEXT_PUBLIC_MEDUSA_BACKEND_URL` variable to `http://host.docker.internal:9000`, if you use Windows.\\r\\n\\r\\n### Docker Compose configuration\\r\\n\\r\\nLet\u2019s create a `docker-compose.yml` file at the same level as the `/backend` and `/storefront` directories.\\r\\n\\r\\nI declare configurations for the database and Redis as follows:\\r\\n\\r\\n```yaml showLineNumbers title=\\"./docker-compose.yml\\"\\r\\nversion: \\"3.8\\"\\r\\nservices:\\r\\n  postgres:\\r\\n    image: postgres:12\\r\\n    ports:\\r\\n      - \\"54322:5432\\"\\r\\n    env_file: ./backend/.env\\r\\n    volumes:\\r\\n      - app-db-data:/var/lib/postgresql/data:cached\\r\\n\\r\\n  redis:\\r\\n    image: redis\\r\\n    expose:\\r\\n      - 6379\\r\\n\\r\\nvolumes:\\r\\n  app-db-data:\\r\\n```\\r\\n\\r\\nThese are the services that must be ready before the application container is initialized and connected to them.\\r\\n\\r\\nNow I will define the backend and admin UI services.\\r\\n\\r\\n```yaml showLineNumbers title=\\"./docker-compose.yml\\"\\r\\n...\\r\\n  backend:\\r\\n    build:\\r\\n      context: ./backend\\r\\n      dockerfile: Dockerfile\\r\\n    image: pxuanbach/medusa-backend\\r\\n    depends_on:\\r\\n      - postgres\\r\\n      - redis\\r\\n    env_file: ./backend/.env\\r\\n    ports:\\r\\n      - \\"9000:9000\\"\\r\\n\\r\\n  admin:\\r\\n    build:\\r\\n      context: ./backend\\r\\n      dockerfile: Dockerfile.admin\\r\\n    image: pxuanbach/medusa-backend-admin\\r\\n    depends_on:\\r\\n      - postgres\\r\\n      - redis\\r\\n    ports:\\r\\n      - \\"7001:80\\"\\r\\n```\\r\\n\\r\\nThe last service is `storefront`, it needs to be initialized after the backend service. So, here is its configuration:\\r\\n\\r\\n```yaml showLineNumbers title=\\"./docker-compose.yml\\"\\r\\n...\\r\\n  storefront:\\r\\n    build:\\r\\n      context: ./storefront\\r\\n      dockerfile: Dockerfile.prod\\r\\n    image: pxuanbach/medusa-storefront\\r\\n    depends_on:\\r\\n      - backend\\r\\n    ports:\\r\\n      - \\"8000:8000\\"\\r\\n```\\r\\n\\r\\n### Launch all services\\r\\n\\r\\nAfter configuring the `docker-compose.yml` file, we will run it to see how it operates.\\r\\n\\r\\n```bash\\r\\ndocker-compose up -d --build postgres redis backend admin\\r\\n\\r\\n# waiting...\\r\\n# init seed data (Optional)\\r\\ndocker-compose exec backend yarn seed\\r\\n\\r\\n# wait...\\r\\ndocker-compose up -d --build storefront\\r\\n```\\r\\n\\r\\nIn the above command, I run `backend` before `storefront`. Because the storefront needs to fetch the region from the backend to build successfully.\\r\\n\\r\\nOnce done, check the results:\\r\\n\\r\\n- Admin Panel: http://localhost:7001\\r\\n\\r\\n    ![Medusa Admin UI](./medusa-adminui.png)\\r\\n\\r\\n- Backend: http://localhost:9000/store/products\\r\\n\\r\\n    ![Medusa Backend API](./medusa-backend-api.png)\\r\\n\\r\\n- Storefront: http://localhost:8000\\r\\n\\r\\n    ![Medusa Storefront UI](./medusa-storefront-ui.png)\\r\\n\\r\\n*Notes:* \\r\\n\\r\\n- *If you cannot connect to the Admin Panel or Storefront via Docker container, try changing the default host of each to `0.0.0.0`.*\\r\\n- *Storefront needs to fetch the regions from the Backend. So, you should initialize your Backend data first.*\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nI hope this post was useful. If you need a project to run a demo on your environment, here is my\xa0[Git repository](https://github.com/pxuanbach/medusa-ecommerce). \\r\\n\\r\\nIn this repository, I have separated into 2 compose configurations. The configuration of this post is equivalent to `prod` configuration in the repos. Additionally, I created a script called `start-up.sh` in the demo repository. It will help us to start all services automatically.\\r\\n\\r\\n## References\\r\\n\\r\\n- [feat: update backend and storefront dockerfiles by mickeiik \xb7 Pull Request #13 \xb7 medusajs/docker-medusa (github.com)](https://github.com/medusajs/docker-medusa/pull/13)\\r\\n- [Install Medusa Backend | Medusa (medusajs.com)](https://docs.medusajs.com/development/backend/install)"},{"id":"essential-modules-for-developing-applications-with-fastapi-p2-logging","metadata":{"permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging","source":"@site/blog/03_essential-modules-for-developing-applications-with-fastapi-p2/index.md","title":"Essential modules for developing applications with FastAPI (P2 - Logging)","description":"Back to FastAPI again.","date":"2024-04-01T10:00:00.000Z","tags":[{"label":"logging","permalink":"/blog/tags/logging"},{"label":"fastapi","permalink":"/blog/tags/fastapi"},{"label":"python","permalink":"/blog/tags/python"},{"label":"essential-modules-fastapi","permalink":"/blog/tags/essential-modules-fastapi"}],"readingTime":7.675,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"essential-modules-for-developing-applications-with-fastapi-p2-logging","title":"Essential modules for developing applications with FastAPI (P2 - Logging)","authors":["pxuanbach"],"tags":["logging","fastapi","python","essential-modules-fastapi"],"date":"2024-04-01T10:00","image":"/img/02_essential-modules-for-developing-applications-with-fastapi-p1/featured.png"},"unlisted":false,"prevItem":{"title":"Dockerize MedusaJS Components: Optimize and Deploy Your Application","permalink":"/blog/dockerizing-medusajs-for-optimized-deployment"},"nextItem":{"title":"Essential modules for developing applications with FastAPI (P1 - Migration)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration"}},"content":"Back to FastAPI again.\\r\\n\\r\\nThis post comes with Logging module that you will want to have in your project.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Framework/Library version\\r\\n\\r\\nThis project uses [Python](https://www.python.org/) 3.10 as the environment and [Poetry](https://python-poetry.org/) as the package manager.\\r\\n\\r\\nThe code and examples in this post will use frameworks/libraries with the following versions.\\r\\n\\r\\n```toml showLineNumbers title=\\"./pyproject.toml\\"\\r\\n[tool.poetry.dependencies]\\r\\npython = \\"^3.10\\"\\r\\nuvicorn = {extras = [\\"standard\\"], version = \\"^0.24.0.post1\\"}\\r\\nfastapi = \\"^0.109.1\\"\\r\\npython-multipart = \\"^0.0.7\\"\\r\\nemail-validator = \\"^2.1.0.post1\\"\\r\\npasslib = {extras = [\\"bcrypt\\"], version = \\"^1.7.4\\"}\\r\\ntenacity = \\"^8.2.3\\"\\r\\npydantic = \\">2.0\\"\\r\\nemails = \\"^0.6\\"\\r\\ngunicorn = \\"^21.2.0\\"\\r\\njinja2 = \\"^3.1.2\\"\\r\\nalembic = \\"^1.12.1\\"\\r\\npython-jose = {extras = [\\"cryptography\\"], version = \\"^3.3.0\\"}\\r\\nhttpx = \\"^0.25.1\\"\\r\\npsycopg = {extras = [\\"binary\\"], version = \\"^3.1.13\\"}\\r\\n\\r\\nsqlmodel = \\"^0.0.16\\"\\r\\n\\r\\n# Pin bcrypt until passlib supports the latest\\r\\nbcrypt = \\"4.0.1\\"\\r\\npydantic-settings = \\"^2.2.1\\"\\r\\nsentry-sdk = {extras = [\\"fastapi\\"], version = \\"^1.40.6\\"}\\r\\npsycopg2 = \\"^2.9.9\\"\\r\\nasyncpg = \\"^0.29.0\\"\\r\\n```\\r\\n\\r\\n## Logging\\r\\n\\r\\nThis module plays an important role in recording information, warnings, and errors during execution.\\r\\n\\r\\nLog messages can provide information about system activities, input and output data, important events, and any issues that arise to aid in locating and fixing errors.\\r\\n\\r\\nLet\'s dive deeper into this module.\\r\\n\\r\\n### Log Level\\r\\n\\r\\nIn Python, log level refers to the importance level of a log message and is used to specify the type of that message.\\r\\n\\r\\n![Log level](./log-level.png)\\r\\n\\r\\n1. **DEBUG**: Log level for debug messages, typically used to record detailed information for debugging and development purposes.\\r\\n2. **INFO**: Log level for informational messages, often used to record normal operations in the application.\\r\\n3. **WARNING**: Log level for warning messages, usually used to record unexpected or potential situations that do not affect the application\'s operation.\\r\\n4. **ERROR**: Log level for error messages, typically used to record situations that cause errors during application execution.\\r\\n5. **CRITICAL**: Log level for critical error messages, commonly used to log fatal errors that may prevent the application from continuing.\\r\\n\\r\\nIn my experience, I often use DEBUG log level in the development environment. When the application is moved to production, I will hide the DEBUG level. Typically, only logs from INFO level or higher are allowed to be printed.\\r\\n\\r\\nTo get more knowledge about this part, you can refer to this https://stackoverflow.com/questions/2031163/when-to-use-the-different-log-levels.\\r\\n\\r\\n### Log Format\\r\\n\\r\\nBy default, the log format looks very simple. The problem is that this simplicity makes it lack information and become difficult to read.\\r\\n\\r\\n![Log before format](./log-before-format.png)\\r\\n\\r\\nLet\'s say you put a log inside a function and it\'s used multiple times. The special thing is that when the system is operating, you can\'t always sit next to the screen to check whether that function records errors or not? And at what point does the error occur? Ha ha\u2026 a classic example.\\r\\n\\r\\nTo solve that problem, log formatting is the key solution. It helps you and your system get through the darkest days.\\r\\n\\r\\nFor example, I have different formats for files and consoles.\\r\\n\\r\\n```python showLineNumbers {9-10} title=\\"./app/core/logger.py\\"\\r\\nconsole_msg_format = \\"%(asctime)s %(levelname)s: %(message)s\\"\\r\\n\\r\\n# Create the root logger.\\r\\nlogger = logging.getLogger()\\r\\nlogger.setLevel(logging.DEBUG)\\r\\n\\r\\n# Set up logging to the console.\\r\\nstream_handler = logging.StreamHandler()\\r\\nstream_formatter = logging.Formatter(console_msg_format)\\r\\nstream_handler.setFormatter(stream_formatter)\\r\\n\\r\\nlogger.addHandler(stream_handler)\\r\\n```\\r\\n\\r\\nThe logs will look like\\r\\n\\r\\n![Log after format](./log-after-format.png)\\r\\n\\r\\nEven though the log format has been changed, it still doesn\'t seem easy to follow. Next, what we\'re going to do to improve it is add color to the log. At this point, I offer a simple solution to make the log better.\\r\\n\\r\\n```python showLineNumbers {27-28} title=\\"./app/core/logger.py\\"\\r\\nclass ColoredFormatter(logging.Formatter):\\r\\n    COLOR_CODES = {\\r\\n        \'DEBUG\': \'\\\\033[94m\',  # blue\\r\\n        \'INFO\': \'\\\\033[92m\',   # green\\r\\n        \'WARNING\': \'\\\\033[93m\',  # yellow\\r\\n        \'ERROR\': \'\\\\033[91m\',  # red\\r\\n        \'CRITICAL\': \'\\\\033[41m\\\\033[97m\'  # red background color and white text\\r\\n    }\\r\\n    RESET_CODE = \'\\\\033[0m\'  # called to return to standard terminal text color\\r\\n\\r\\n    def format(self, record):\\r\\n        # Get the color corresponding to the log level\\r\\n        color = self.COLOR_CODES.get(record.levelname, \'\')\\r\\n        # Add color to log messages and reset color at the end\\r\\n        formatted_message = f\\"{color}{super().format(record)}{self.RESET_CODE}\\"\\r\\n        return formatted_message\\r\\n        \\r\\n        \\r\\nconsole_msg_format = \\"%(asctime)s %(levelname)s: %(message)s\\"\\r\\n\\r\\n# Create the root logger.\\r\\nlogger = logging.getLogger()\\r\\nlogger.setLevel(logging.DEBUG)\\r\\n\\r\\n# Set up logging to the console.\\r\\nstream_handler = logging.StreamHandler()\\r\\nstream_formatter = ColoredFormatter(console_msg_format)\\r\\nstream_handler.setFormatter(stream_formatter)\\r\\n\\r\\nlogger.addHandler(stream_handler)\\r\\n```\\r\\n\\r\\nThe result ^_^:\\r\\n\\r\\n![Log after color format](./log-after-color-format.png)\\r\\n\\r\\n### Logging to Multiple Sources\\r\\n\\r\\nWe have customized the log format for printing to the console in the above sections. If we only print to the console, when we restart the application, the logs on the console will be reset... We have lost all the old logs.\\r\\n\\r\\nSo, we need to store logs to multiple sources. As far as I know, we can store locations like files, databases, or online log services.\\r\\n\\r\\nYou can integrate with other tools like [Elasticsearch, Logstash, Kibana (ELK Stack)](https://www.elastic.co/elastic-stack).\\r\\n\\r\\n![ELK Stack](./elk-stack.png)\\r\\n\\r\\nOr [Promtail, Loki and Grafana](https://grafana.com/docs/loki/latest/) for simpler setup.\\r\\n\\r\\n![Promtail, Loki and Grafana](./promtail-grafana-loki.png)\\r\\n\\r\\nIn this post, we will discuss how to log into a file and some considerations to remember regarding this process.\\r\\n\\r\\nWith just a few lines of code, you can log into a specific file anywhere you want.\\r\\n\\r\\n```python showLineNumbers {8} title=\\"./app/core/logger.py\\"\\r\\n# Create the root logger.\\r\\nlogger = logging.getLogger()\\r\\nlogger.setLevel(logging.DEBUG)\\r\\n\\r\\nlogging.basicConfig(level=logging.DEBUG)\\r\\n\\r\\n# Set up logging to the file.\\r\\nfile_handler = logging.FileHandler(\'app.log\') # <--- File location\\r\\nfile_handler.setLevel(logging.DEBUG)\\r\\nformatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\\r\\nfile_handler.setFormatter(formatter)\\r\\n\\r\\nlogger.addHandler(file_handler)\\r\\n```\\r\\n\\r\\nLet\u2019s see the results. \\r\\n\\r\\n![Logging to Multiple Sources Result](./logging-to-multiple-sources-result.png)\\r\\n\\r\\nAs you can see, `app.log` automatically creates and contains all the logs we print. However, that\'s not enough. We cannot simply dump all the system logs into a single file and monitor the logs from there. It is terrible! (\uff03\xb0\u0414\xb0)\\r\\n\\r\\nTo avoid that, what we need is a log rotation. It helps us manage file size, the maximum number of backup files if the file size exceeds the threshold.\\r\\n\\r\\n```python showLineNumbers {34-36,40} title=\\"./app/core/logger.py\\"\\r\\n# Define default logfile format.\\r\\nfile_name_format = \\"{year:04d}{month:02d}{day:02d}.log\\"\\r\\n\\r\\n# Define the default logging message formats.\\r\\nfile_msg_format = \\"%(asctime)s %(levelname)-8s: %(message)s\\"\\r\\nconsole_msg_format = \\"%(levelname)s: %(message)s\\"\\r\\n\\r\\n# Define the log rotation criteria.\\r\\nmax_bytes = 1024**2   # ~ 1MB\\r\\nbackup_count = 100\\r\\n\\r\\n# Create the root logger.\\r\\nlogger = logging.getLogger()\\r\\nlogger.setLevel(logging.DEBUG)\\r\\n\\r\\n# Validate the given directory.\\r\\ndir=\\"log\\"\\r\\ndir = os.path.normpath(dir)\\r\\n\\r\\n# Create a folder for the logfiles.\\r\\nif not os.path.exists(dir):\\r\\n    os.makedirs(dir)\\r\\n\\r\\n# Construct the name of the logfile.\\r\\nt = datetime.datetime.now()\\r\\nfile_name = file_name_format.format(\\r\\n    year=t.year,\\r\\n    month=t.month,\\r\\n    day=t.day,\\r\\n)\\r\\nfile_name = os.path.join(dir, file_name)\\r\\n\\r\\n# Set up logging to the logfile.\\r\\nfile_handler = logging.handlers.RotatingFileHandler(\\r\\n    filename=file_name, maxBytes=max_bytes, backupCount=backup_count\\r\\n)\\r\\nfile_handler.setLevel(logging.DEBUG)\\r\\nfile_formatter = logging.Formatter(file_msg_format)\\r\\nfile_handler.setFormatter(file_formatter)\\r\\nlogger.addHandler(file_handler)\\r\\n```\\r\\n\\r\\nIf you\'re not familiar with how this module works, the `RotatingFileHandler` class has the following description:\\r\\n\\r\\n> Rollover occurs whenever the current log file is nearly maxBytes in length. If backupCount is >= 1, the system will successively create new files with the same pathname as the base file, but with extensions \\".1\\", \\".2\\" etc. appended to it. For example, with a backupCount of 5 and a base file name of \\"app.log\\", you would get \\"app.log\\", \\"app.log.1\\", \\"app.log.2\\", ... through to \\"app.log.5\\". The file being written to is always \\"app.log\\" - when it gets filled up, it is closed and renamed to \\"app.log.1\\", and if files \\"app.log.1\\", \\"app.log.2\\" etc. exist, then they are renamed to \\"app.log.2\\", \\"app.log.3\\" etc. respectively.\\r\\n\\r\\n> If maxBytes is zero, rollover never occurs.\\r\\n\\r\\nIn the example, I set the `maxBytes` value to 1MB, so if the size of the log file reaches 1MB, it will be moved to a backup file (`20240405.log.1`, `20240405.log.2`), and new logs will continue to be written to the current file (`20240405.log`).\\r\\n\\r\\n![Logging to Multiple Sources Rotation Example](./logging-to-multiple-sources-rotation-example.png)\\r\\n\\r\\n### Performance\\r\\n\\r\\nLogging in the system may seem lightweight, but in reality, it still consumes time and a certain amount of system resources, especially when performing concurrent logging from multiple threads or processes.\\r\\n\\r\\nSpecifying log levels, optimizing log structure, and configuring rotation also significantly reduce the burden on the system. Additionally, we can consider asynchronous logging and reduce the number of logging operations to reduce the load on the storage system, especially when handling large log volumes.\\r\\n\\r\\nIn FastAPI, you can use [Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/) to log asynchronously, which reduces system load and request latency.\\r\\n\\r\\n```python showLineNumbers {9,11}\\r\\nfrom typing import Any\\r\\nfrom fastapi import BackgroundTasks\\r\\nimport logging\\r\\n\\r\\n@self.router.get(\\r\\n    \\"/something\\"\\r\\n)\\r\\nasync def handle_something(\\r\\n    bg_task: BackgroundTasks\\r\\n) -> Any:\\r\\n    bg_task.add_task(logging.info, f\\"This is log\\")\\r\\n    return { \\"something\\": True }\\r\\n```\\r\\n\\r\\n### Integration with FastAPI\\r\\n\\r\\nIn my experience, you should wrap up all configurations into a function and call it when initializing the FastAPI application. In the new FastAPI version, [Lifespan Events](https://fastapi.tiangolo.com/advanced/events/#lifespan) will help you do that.\\r\\n\\r\\nFor instance:\\r\\n\\r\\n```python showLineNumbers {8,16} title=\\"./app/main.py\\"\\r\\nfrom contextlib import asynccontextmanager\\r\\nfrom app.core.logger import setup as setup_logging\\r\\nfrom app.core.config import settings\\r\\n\\r\\n@asynccontextmanager\\r\\nasync def lifespan(app: FastAPI):\\r\\n    # start up\\r\\n    setup_logging(minLevel=logging.DEBUG)\\r\\n    yield\\r\\n    # shut down\\r\\n    pass\\r\\n\\r\\napp = FastAPI(\\r\\n    title=settings.PROJECT_NAME,\\r\\n    openapi_url=f\\"{settings.API_STR}{settings.API_VERSION_STR}/openapi.json\\",\\r\\n    lifespan=lifespan\\r\\n)\\r\\n```\\r\\n\\r\\n## To sum up\\r\\n\\r\\nWe discussed an essential module in developing applications with FastAPI: Logging. By carefully and effectively using this module, we can build and maintain powerful and flexible FastAPI applications. Hopefully, this article has given you the overview and knowledge needed to use this important module in your projects.\\r\\n\\r\\nIf you need a project to run a demo on your environment, here are my\xa0[Git repository](https://github.com/pxuanbach/fastapi-essential-modules).\\r\\n\\r\\nSee you again!\\r\\n\\r\\n## References\\r\\n\\r\\n- [borntyping/python-colorlog: A colored formatter for the python logging module (github.com)](https://github.com/borntyping/python-colorlog)\\r\\n- [acschaefer/duallog: Python package to enable simultaneous logging to console and logfile (github.com)](https://github.com/acschaefer/duallog/tree/master)"},{"id":"essential-modules-for-developing-applications-with-fastapi-p1-migration","metadata":{"permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration","source":"@site/blog/02_essential-modules-for-developing-applications-with-fastapi-p1/index.md","title":"Essential modules for developing applications with FastAPI (P1 - Migration)","description":"Today, I say \u201cHi FastAPI\u201d.","date":"2024-03-31T10:00:00.000Z","tags":[{"label":"alembic","permalink":"/blog/tags/alembic"},{"label":"migration","permalink":"/blog/tags/migration"},{"label":"fastapi","permalink":"/blog/tags/fastapi"},{"label":"python","permalink":"/blog/tags/python"},{"label":"essential-modules-fastapi","permalink":"/blog/tags/essential-modules-fastapi"}],"readingTime":4.78,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"essential-modules-for-developing-applications-with-fastapi-p1-migration","title":"Essential modules for developing applications with FastAPI (P1 - Migration)","authors":["pxuanbach"],"tags":["alembic","migration","fastapi","python","essential-modules-fastapi"],"date":"2024-03-31T10:00","image":"/img/02_essential-modules-for-developing-applications-with-fastapi-p1/featured.png"},"unlisted":false,"prevItem":{"title":"Essential modules for developing applications with FastAPI (P2 - Logging)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p2-logging"},"nextItem":{"title":"Zero-downtime Deployments with Docker Compose & Nginx","permalink":"/blog/zero-downtime-deployment-with-docker-compose-nginx"}},"content":"Today, I say \u201cHi FastAPI\u201d.\\r\\n\\r\\nRecently, [FastAPI](https://fastapi.tiangolo.com/) has become increasingly popular, partly driven by the current trend of AI innovation. In my opinion, being modern, robust, fast, and micro is what makes me fall in love with this framework.\\r\\n\\r\\nThis post comes with some modules that you will want to have in your project.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Framework/Library version\\r\\n\\r\\nThis project uses [Python](https://www.python.org/) 3.10 as the environment and [Poetry](https://python-poetry.org/) as the package manager.\\r\\n\\r\\nThe code and examples in this post will use frameworks/libraries with the following versions.\\r\\n\\r\\n```toml showLineNumbers title=\\"./pyproject.toml\\"\\r\\n[tool.poetry.dependencies]\\r\\npython = \\"^3.10\\"\\r\\nuvicorn = {extras = [\\"standard\\"], version = \\"^0.24.0.post1\\"}\\r\\nfastapi = \\"^0.109.1\\"\\r\\npython-multipart = \\"^0.0.7\\"\\r\\nemail-validator = \\"^2.1.0.post1\\"\\r\\npasslib = {extras = [\\"bcrypt\\"], version = \\"^1.7.4\\"}\\r\\ntenacity = \\"^8.2.3\\"\\r\\npydantic = \\">2.0\\"\\r\\nemails = \\"^0.6\\"\\r\\ngunicorn = \\"^21.2.0\\"\\r\\njinja2 = \\"^3.1.2\\"\\r\\nalembic = \\"^1.12.1\\"\\r\\npython-jose = {extras = [\\"cryptography\\"], version = \\"^3.3.0\\"}\\r\\nhttpx = \\"^0.25.1\\"\\r\\npsycopg = {extras = [\\"binary\\"], version = \\"^3.1.13\\"}\\r\\n\\r\\nsqlmodel = \\"^0.0.16\\"\\r\\n\\r\\n# Pin bcrypt until passlib supports the latest\\r\\nbcrypt = \\"4.0.1\\"\\r\\npydantic-settings = \\"^2.2.1\\"\\r\\nsentry-sdk = {extras = [\\"fastapi\\"], version = \\"^1.40.6\\"}\\r\\npsycopg2 = \\"^2.9.9\\"\\r\\nasyncpg = \\"^0.29.0\\"\\r\\n```\\r\\n\\r\\n## Migration\\r\\n\\r\\nIf you work with SQL databases, these are important components you need to set up. What I want to talk about is Alembic.\\r\\n\\r\\n[Alembic](https://alembic.sqlalchemy.org/)\xa0is a lightweight database migration tool for usage with the\xa0[SQLAlchemy](https://www.sqlalchemy.org/)\xa0Database Toolkit for Python.\\r\\n\\r\\nTo add or initialize a module, you can use the following commands:\\r\\n\\r\\n```bash\\r\\npoetry add alembic  # pip install alembic\\r\\n\\r\\nalembic init migrations\\r\\n```\\r\\n\\r\\n*Note that \u201cmigrations\u201d is the name of the directory where you want to store your migration versions.*\\r\\n\\r\\n### Directory structure\\r\\n\\r\\n```plaintext \\r\\nProject/\\r\\n\u251c\u2500\u2500 app/\\r\\n\u2502   \u2514\u2500\u2500 ...\\r\\n\u251c\u2500\u2500 migrations/\\r\\n\u2502   \u251c\u2500\u2500 versions/\\r\\n\u2502   \u2502   \u2514\u2500\u2500 ...\\r\\n\u2502   \u251c\u2500\u2500 README\\r\\n\u2502   \u251c\u2500\u2500 env.py\\r\\n\u2502   \u2514\u2500\u2500 script.py.mako\\r\\n\u251c\u2500\u2500 alembic.ini\\r\\n\u251c\u2500\u2500 pyproject.toml\\r\\n\u251c\u2500\u2500 poetry.lock\\r\\n\u251c\u2500\u2500 README.md\\r\\n\u2514\u2500\u2500 ...\\r\\n```\\r\\n\\r\\n### alembic.ini\\r\\n\\r\\nI will use its default configuration. Or go to this link https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file to get more details.\\r\\n\\r\\n```toml showLineNumbers title=\\"./alembic.ini\\"\\r\\n# A generic, single database configuration.\\r\\n\\r\\n[alembic]\\r\\n# path to migration scripts\\r\\nscript_location = migrations\\r\\n\\r\\n# template used to generate migration files\\r\\n# file_template = %%(rev)s_%%(slug)s\\r\\n\\r\\n# sys.path path, will be prepended to sys.path if present.\\r\\n# defaults to the current working directory.\\r\\nprepend_sys_path = .\\r\\n...\\r\\n```\\r\\n\\r\\n### env.py\\r\\n\\r\\nIn this version of FastAPI, [SQLModel](https://sqlmodel.tiangolo.com/) appears as an integrated version of SQLAlchemy and Pydantic and is recommended for use in the official documentation.\\r\\n\\r\\nSo instead of using `DeclarativeMeta`, we will use `SQLModel.metadata` in Alembic\u2019s env.\\r\\n\\r\\nAfter completing the steps above, the `env.py`  will look like this.\\r\\n\\r\\n```python showLineNumbers {6-7,22} title=\\"./migrations/env.py\\"\\r\\nfrom logging.config import fileConfig\\r\\nfrom sqlalchemy import engine_from_config\\r\\nfrom sqlalchemy import pool\\r\\nfrom alembic import context\\r\\n\\r\\nfrom app.core.db import SQLModel       # <--- CHANGE HERE\\r\\nfrom app.core.config import settings   # <--- CHANGE HERE\\r\\n\\r\\n# this is the Alembic Config object, which provides\\r\\n# access to the values within the .ini file in use.\\r\\nconfig = context.config\\r\\n\\r\\n# Interpret the config file for Python logging.\\r\\n# This line sets up loggers basically.\\r\\nif config.config_file_name is not None:\\r\\n    fileConfig(config.config_file_name)\\r\\n\\r\\n# add your model\'s MetaData object here\\r\\n# for \'autogenerate\' support\\r\\n# from myapp import mymodel\\r\\n# target_metadata = mymodel.Base.metadata\\r\\ntarget_metadata = SQLModel.metadata    # <--- CHANGE HERE\\r\\n\\r\\n# other values from the config, defined by the needs of env.py,\\r\\n# can be acquired:\\r\\n# my_important_option = config.get_main_option(\\"my_important_option\\")\\r\\n# ... etc.\\r\\nconfig.set_section_option(\\"alembic\\", \\"sqlalchemy.url\\", str(settings.SQLALCHEMY_DATABASE_URI))  # <--- CHANGE HERE\\r\\n\\r\\ndef run_migrations_offline() -> None:\\r\\n...\\r\\n```\\r\\n\\r\\n*One note is that to achieve the Alembic `\u2014autogenerate` option, you must import the entity representing your table where you import SQLModel into env. For example `app/core/db.py`:*\\r\\n\\r\\n```python showLineNumbers {22,24-26} title=\\"./app/core/db.py\\"\\r\\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\\r\\nfrom sqlalchemy.ext.declarative import declarative_base\\r\\nfrom sqlalchemy.orm import sessionmaker\\r\\nfrom sqlmodel import SQLModel, create_engine\\r\\n\\r\\nfrom app.core.config import settings\\r\\n\\r\\nasync_engine = create_async_engine(settings.SQLALCHEMY_DATABASE_URI_ASYNC)\\r\\nasync_session_maker = sessionmaker(\\r\\n    async_engine,\\r\\n    class_=AsyncSession,\\r\\n    expire_on_commit=False,\\r\\n    autocommit=False,\\r\\n    autoflush=False,\\r\\n)\\r\\n\\r\\n# We still have a second old style sync SQLAlchemy engine for shell and alembic\\r\\nengine = create_engine(settings.SQLALCHEMY_DATABASE_URI, future=True)\\r\\nSessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False)\\r\\n\\r\\nBase = declarative_base()\\r\\nSQLModel.metadata = Base.metadata\\r\\n\\r\\n# import all models to automatically create migration through Alembic\\r\\nfrom app.modules.users.model import UserInDb\\r\\n...\\r\\n```\\r\\n\\r\\n*Alternatively, if you organize your database models within a directory, you can leverage the `__init__.py` file to gather all the models you want Alembic to create migrations automatically. For instance `app/models/__init__.py`:*\\r\\n\\r\\n```python showLineNumbers {3,4} title=\\"./app/models/__init__.py\\"\\r\\nfrom app.core.db import SQLModel\\r\\n\\r\\nfrom .user import UserInDb\\r\\nfrom .item import ItemInDb\\r\\n...\\r\\n```\\r\\n\\r\\nAnd `migrations/env.py`:\\r\\n\\r\\n```python showLineNumbers {6} title=\\"./migrations/env.py\\"\\r\\nfrom logging.config import fileConfig\\r\\nfrom sqlalchemy import engine_from_config\\r\\nfrom sqlalchemy import pool\\r\\nfrom alembic import context\\r\\n\\r\\nfrom app.models import SQLModel       # <--- CHANGE HERE\\r\\n...\\r\\n```\\r\\n\\r\\n### script.py.mako\\r\\n\\r\\nTo make things go smoothly, don\'t forget to import the `sqlmodel` into this file.\\r\\n\\r\\n```python showLineNumbers {12} title=\\"./migrations/script.py.mako\\"\\r\\n\\"\\"\\"${message}\\r\\n\\r\\nRevision ID: ${up_revision}\\r\\nRevises: ${down_revision | comma,n}\\r\\nCreate Date: ${create_date}\\r\\n\\r\\n\\"\\"\\"\\r\\nfrom typing import Sequence, Union\\r\\n\\r\\nfrom alembic import op\\r\\nimport sqlalchemy as sa\\r\\nimport sqlmodel             # <--- CHANGE HERE\\r\\n${imports if imports else \\"\\"}\\r\\n...\\r\\n```\\r\\n\\r\\n### How to use?\\r\\n\\r\\n![How to use migration](./how-to-use-migration.png)\\r\\n\\r\\n- To create a new revision:\\r\\n    ```bash\\r\\n    alembic revision --autogenerate -m \\"message\\"\\r\\n    ```\\r\\n\\r\\n- To migrate:\\r\\n    ```bash\\r\\n    alembic upgrade head\\r\\n    ```\\r\\n\\r\\n- To undo the latest migration:\\r\\n    ```bash\\r\\n    alembic downgrade -1\\r\\n    ```\\r\\n\\r\\n- To roll back to a specific revision:\\r\\n    ```bash\\r\\n    alembic downgrade <<revision_id>>\\r\\n    ```\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nWe discussed an essential module in developing applications with FastAPI: Migration. By carefully and effectively using this module, we can build and maintain powerful and flexible FastAPI applications. Hopefully, this article has given you the overview and knowledge needed to use this important module in your projects.\\r\\n\\r\\nIf you need a project to run a demo on your environment, here are my\xa0[Git repository](https://github.com/pxuanbach/fastapi-essential-modules).\\r\\n\\r\\nSee you again!\\r\\n\\r\\n## References\\r\\n\\r\\n- [How to get Alembic to recognise SQLModel database model?](https://stackoverflow.com/questions/68932099/how-to-get-alembic-to-recognise-sqlmodel-database-model)\\r\\n- [Tutorial \u2014 Alembic 1.13.2 documentation (sqlalchemy.org)](https://alembic.sqlalchemy.org/en/latest/tutorial.html)"},{"id":"zero-downtime-deployment-with-docker-compose-nginx","metadata":{"permalink":"/blog/zero-downtime-deployment-with-docker-compose-nginx","source":"@site/blog/01_zero-downtime-deployment-with-docker-compose-nginx/index.md","title":"Zero-downtime Deployments with Docker Compose & Nginx","description":"Hey, welcome to my blog!","date":"2024-03-24T10:00:00.000Z","tags":[{"label":"automation","permalink":"/blog/tags/automation"},{"label":"deployment","permalink":"/blog/tags/deployment"},{"label":"docker","permalink":"/blog/tags/docker"},{"label":"nginx","permalink":"/blog/tags/nginx"},{"label":"shell script","permalink":"/blog/tags/shell-script"}],"readingTime":4.435,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"zero-downtime-deployment-with-docker-compose-nginx","title":"Zero-downtime Deployments with Docker Compose & Nginx","authors":["pxuanbach"],"tags":["automation","deployment","docker","nginx","shell script"],"date":"2024-03-24T10:00","image":"/img/01_zero-downtime-deployment-with-docker-compose-nginx/featured.png"},"unlisted":false,"prevItem":{"title":"Essential modules for developing applications with FastAPI (P1 - Migration)","permalink":"/blog/essential-modules-for-developing-applications-with-fastapi-p1-migration"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome"}},"content":"Hey, welcome to my blog!\\r\\n\\r\\n## Introduction\\r\\n\\r\\nA few months ago, I worked on a project that utilized Docker and Nginx to deploy the product on Digital Ocean\u2019s VPS. Everything at that time was quite primitive, I had to set up everything from scratch. From containerizing the application to creating a CI/CD pipeline to build, manage, and deploy different Docker image versions.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nDocker is a great tool and I love using it in my workflow. I define the Docker services in the configuration file, then pull, up and down the containers to make sure they are up to date. But we have a problem: the time between when I down the container and when I up it. It took\xa0**2 minutes**\xa0of downtime in total. That\u2019s unacceptable for a product deployed for end-users.\\r\\n\\r\\n![Featured Image](./blue-green-deployment-with-nginx.png)\\r\\n\\r\\nSo I implemented a Zero downtime deployment strategy for that project. The BLUE-GREEN strategy is a basic deployment process, but it\u2019s great when simplicity gets the job done. \\r\\n\\r\\nNow, let\u2019s talk about some stuff.\\r\\n\\r\\n## Configuration before applying\\r\\n\\r\\n### Docker Compose\\r\\n\\r\\nI have a configuration file like this:\\r\\n\\r\\n```yaml showLineNumbers title=\\"./docker-compose.yml\\"\\r\\nservices:\\r\\n  api:\\r\\n    image: pxuanbach/simple-app\\r\\n    ports:\\r\\n      - \'8000:8000\'\\r\\n    restart: on-failure\\r\\n```\\r\\n\\r\\n### Nginx\\r\\n\\r\\nThe `nginx.conf` configuration will look like this:\\r\\n\\r\\n```plaintext {5} showLineNumbers title=\\"./nginx.conf\\"\\r\\nserver {\\r\\n    listen 80 default_server;\\r\\n    server_name api.app.com;\\r\\n    location / {\\r\\n        proxy_pass http://localhost:8000;\\r\\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\r\\n        proxy_set_header Host $host;\\r\\n        proxy_http_version 1.1;\\r\\n        proxy_set_header Upgrade $http_upgrade;\\r\\n        proxy_set_header Connection \\"upgrade\\";\\r\\n        client_max_body_size 64M;\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n### Deployment process\\r\\n\\r\\nIt was very easy, and I just followed the steps\u2026\\r\\n\\r\\n```bash {5} showLineNumbers\\r\\ndocker compose pull\\r\\n\\r\\ndocker compose down\\r\\n\\r\\n# ---DOWNTIME HERE---\\r\\n\\r\\ndocker compose up\\r\\n```\\r\\n\\r\\n![Old Deployment Flow](./old-deployment-flow.png)\\r\\n\\r\\nNow let\'s move on to the BLUE-GREEN strategy.\\r\\n\\r\\n## New configuration\\r\\n\\r\\n### Docker Compose\\r\\n\\r\\nTo apply the BLUE-GREEN strategy, I need to update this configuration file a bit. I use the\xa0**Anchors and aliases**\xa0features to have a blue and green service with the same configuration. I only change the port number for the green service.\\r\\n\\r\\n```yaml showLineNumbers title=\\"./docker-compose.yml\\"\\r\\nservices:\\r\\n  api_blue: &api\\r\\n    image: pxuanbach/simple-app\\r\\n    ports:\\r\\n      - \'8000:8000\'\\r\\n    restart: on-failure\\r\\n\\r\\n  api_green: \\r\\n    <<: *api\\r\\n    ports:\\r\\n      - \\"8001:8000\\"\\r\\n```\\r\\n\\r\\n### Nginx\\r\\n\\r\\nCreate a copy of the nginx configuration corresponding to the service name and port. For example\xa0`api_green.conf`:\\r\\n\\r\\n```plaintext showLineNumbers title=\\"./api_green.conf\\"\\r\\nserver {\\r\\n    listen 80 default_server;\\r\\n    server_name api.app.com;\\r\\n    location / {\\r\\n        proxy_pass http://localhost:8001;\\r\\n        ...\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n### Zero Downtime Deployment\\r\\n\\r\\nTo achieve the goal, I must use the Bash/Shell script. This script will make use of the Docker command line as well as the Nginx. Its goal is to implement the BLUE-GREEN strategy by identifying which service, BLUE or GREEN, is currently active and then standing up the inactive environment in parallel. To avoid downtime, I will update the Nginx configuration before stopping the old container.\\r\\n\\r\\n```bash showLineNumbers title=\\"./pull.run-service.sh\\"\\r\\n#!/bin/bash\\r\\n\\r\\n# Step 1\\r\\nBLUE_SERVICE=\\"api_blue\\"\\r\\nBLUE_SERVICE_PORT=8000\\r\\nGREEN_SERVICE=\\"api_green\\"\\r\\nGREEN_SERVICE_PORT=8001\\r\\n\\r\\nTIMEOUT=60  # Timeout in seconds\\r\\nSLEEP_INTERVAL=5  # Time to sleep between retries in seconds\\r\\nMAX_RETRIES=$((TIMEOUT / SLEEP_INTERVAL))\\r\\n\\r\\n# Step 2\\r\\nif docker ps --format \\"{{.Names}}\\" | grep -q \\"$BLUE_SERVICE\\"; then\\r\\n  ACTIVE_SERVICE=$BLUE_SERVICE\\r\\n  INACTIVE_SERVICE=$GREEN_SERVICE\\r\\nelif docker ps --format \\"{{.Names}}\\" | grep -q \\"$GREEN_SERVICE\\"; then\\r\\n  ACTIVE_SERVICE=$GREEN_SERVICE\\r\\n  INACTIVE_SERVICE=$BLUE_SERVICE\\r\\nelse\\r\\n  ACTIVE_SERVICE=\\"\\"\\r\\n  INACTIVE_SERVICE=$BLUE_SERVICE\\r\\nfi\\r\\n\\r\\necho \\"Starting $INACTIVE_SERVICE container\\"\\r\\n\\r\\ndocker compose pull $INACTIVE_SERVICE\\r\\n\\r\\ndocker compose up -d $INACTIVE_SERVICE\\r\\n\\r\\n# Step 3\\r\\n# Wait for the new environment to become healthy\\r\\necho \\"Waiting for $INACTIVE_SERVICE to become healthy...\\"\\r\\nsleep 10\\r\\n\\r\\ni=0\\r\\nwhile [ \\"$i\\" -le $MAX_RETRIES ]; do\\r\\n  HEALTH_CHECK_URL=\\"http://localhost:8000/health\\"\\r\\n  if [ \\"$INACTIVE_SERVICE\\" = \\"$BLUE_SERVICE\\" ]; then\\r\\n    HEALTH_CHECK_URL=\\"http://localhost:$BLUE_SERVICE_PORT/health\\"\\r\\n  else\\r\\n    HEALTH_CHECK_URL=\\"http://localhost:$GREEN_SERVICE_PORT/health\\"\\r\\n  fi\\r\\n\\r\\n  response=$(curl -s -o /dev/null -w \\"%{http_code}\\" $HEALTH_CHECK_URL)\\r\\n  # Check the HTTP status code\\r\\n  if [ $response -eq 200 ]; then\\r\\n      echo \\"$INACTIVE_SERVICE is healthy\\"\\r\\n      break\\r\\n  else\\r\\n      echo \\"Health check failed. API returned HTTP status code: $response\\"\\r\\n  fi\\r\\n  i=$(( i + 1 ))\\r\\n  sleep \\"$SLEEP_INTERVAL\\"\\r\\ndone\\r\\n\\r\\n# Step 4\\r\\n# update Nginx config\\r\\necho \\"Update Nginx config to $INACTIVE_SERVICE\\"\\r\\ncp ./$INACTIVE_SERVICE.conf /your/config/path/api.conf\\r\\n# restart nginx\\r\\nnginx -s reload;\\r\\n\\r\\nsleep 5\\r\\n\\r\\n# Step 5\\r\\n# remove OLD CONTAINER\\r\\necho \\"Remove OLD CONTAINER: $ACTIVE_SERVICE\\"\\r\\ndocker compose rm -fsv $ACTIVE_SERVICE\\r\\n\\r\\n# remove unused images\\r\\n(docker images -q --filter \'dangling=true\' -q | xargs docker rmi) || true\\r\\n```\\r\\n\\r\\nLet\u2019s walk through the script step by step:\\r\\n\\r\\n1. I define the name and port of the blue and green services. And the maximum retry time to check the status of the container. The value depends on your container initialization time.\\r\\n2. Execute the docker command to find the inactive service and start it.\\r\\n3. Check the status of the newly initialized container.\\r\\n4. Update Nginx configuration and reload it. Using\xa0`nginx -s reload`\xa0to reload Nginx\xa0**usually does not cause downtime**. This is because the command only tells Nginx to reload its configuration, it does not restart the entire process.\\r\\n5. Clean up some unused stuff (old docker image, old container).\\r\\n\\r\\nIn some cases the command\xa0`docker compose rm -fsv`\xa0may not work. Easily change it to:\\r\\n\\r\\n```bash showLineNumbers\\r\\ndocker compose stop $ACTIVE_SERVICE\\r\\ndocker compose rm -f $ACTIVE_SERVICE\\r\\n```\\r\\n\\r\\nTo deploy the new version, simply run the created script.\\r\\n\\r\\n```bash showLineNumbers\\r\\n./pull.run-service.sh\\r\\n```\\r\\n\\r\\n# Conclusion\\r\\n\\r\\nAs you can see, we can automate the deployment process with just 1 Bash script. The primary objective is to redirect the proxy to the newest container and then remove the old one.\\r\\n\\r\\nIf you need a project to run a demo on your environment, here are my [Git repository](https://github.com/pxuanbach/demo-blue-green-deployment).\\r\\n\\r\\n# References\\r\\n\\r\\n- [Zero-Downtime Deployments with Docker Compose \u2013 Max Countryman](https://www.maxcountryman.com/articles/zero-downtime-deployments-with-docker-compose)\\r\\n- [docker compose rm | Docker Docs](https://docs.docker.com/reference/cli/docker/compose/rm/)"},{"id":"welcome","metadata":{"permalink":"/blog/welcome","source":"@site/blog/00_welcome/index.md","title":"Welcome","description":"Hello world!","date":"2024-03-15T10:00:00.000Z","tags":[{"label":"hello world","permalink":"/blog/tags/hello-world"}],"readingTime":0.42,"hasTruncateMarker":true,"authors":[{"name":"Bach Pham","title":"Software Engineer","url":"https://github.com/pxuanbach","imageURL":"https://avatars.githubusercontent.com/u/55500268?v=4","key":"pxuanbach"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["pxuanbach"],"tags":["hello world"],"image":"/img/00_welcome/featured.png","date":"2024-03-15T10:00"},"unlisted":false,"prevItem":{"title":"Zero-downtime Deployments with Docker Compose & Nginx","permalink":"/blog/zero-downtime-deployment-with-docker-compose-nginx"}},"content":"Hello world!\\r\\n\\r\\nLike many other software engineers, I have a passion for writing code and exploring new technologies and techniques in the industry. \x3c!--truncate--\x3e That\'s why I created this blog to share the knowledge and experience I\'ve gained throughout my work and studies.\\r\\n\\r\\n## What will this blog be about?\\r\\n\\r\\nCurrently, my primary focus is on back-end development. I also manage the product deployment process and front-end web maintenance. \\r\\n\\r\\nTherefore, the main content of this blog will revolve around back-end and DevOps topics \ud83d\ude05."}]}}')}}]);